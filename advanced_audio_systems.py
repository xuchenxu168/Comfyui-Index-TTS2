#!/usr/bin/env python3
"""
IndexTTS2 é«˜çº§éŸ³é¢‘ç³»ç»Ÿ
Advanced Audio Systems for IndexTTS2 - Phase 2 Improvements
"""

import torch
import torch.nn.functional as F
import numpy as np
import hashlib
import time
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict, OrderedDict
import threading
import json
import os

class SpeakerEmbeddingCache:
    """æ™ºèƒ½è¯´è¯äººåµŒå…¥ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self, cache_size: int = 200, similarity_threshold: float = 0.95,
                 enable_multi_sample_fusion: bool = True, adaptive_cache_strategy=None):
        self.cache_size = cache_size
        self.similarity_threshold = similarity_threshold
        self.enable_multi_sample_fusion = enable_multi_sample_fusion
        self.adaptive_cache_strategy = adaptive_cache_strategy
        
        # ç¼“å­˜å­˜å‚¨
        self.cache = OrderedDict()  # LRUç¼“å­˜
        self.access_count = defaultdict(int)
        self.creation_time = {}
        self.sample_groups = defaultdict(list)  # ç›¸ä¼¼æ ·æœ¬åˆ†ç»„
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'cache_hits': 0,
            'cache_misses': 0,
            'total_requests': 0,
            'fusion_operations': 0,
            'similarity_matches': 0
        }
        
        # çº¿ç¨‹å®‰å…¨
        self._lock = threading.RLock()
        
    def _compute_audio_hash(self, audio: torch.Tensor, metadata: Dict = None) -> str:
        """è®¡ç®—éŸ³é¢‘å“ˆå¸Œå€¼"""
        # ä½¿ç”¨éŸ³é¢‘çš„ç»Ÿè®¡ç‰¹å¾å’Œå…ƒæ•°æ®è®¡ç®—å“ˆå¸Œ
        features = []
        
        # éŸ³é¢‘ç»Ÿè®¡ç‰¹å¾
        features.extend([
            audio.mean().item(),
            audio.std().item(), 
            audio.max().item(),
            audio.min().item(),
            audio.shape[-1]  # é•¿åº¦
        ])
        
        # é¢‘åŸŸç‰¹å¾
        if audio.shape[-1] > 1024:
            fft = torch.fft.fft(audio.flatten()[:1024])
            magnitude = torch.abs(fft)
            features.extend([
                magnitude.mean().item(),
                magnitude.std().item(),
                torch.argmax(magnitude).item()  # ä¸»é¢‘ä½ç½®
            ])
        
        # å…ƒæ•°æ®
        if metadata:
            features.append(str(metadata.get('sample_rate', 22050)))
            features.append(str(metadata.get('speaker_id', 'unknown')))
        
        # ç”Ÿæˆå“ˆå¸Œ
        hash_input = json.dumps(features, sort_keys=True).encode()
        return hashlib.sha256(hash_input).hexdigest()[:16]
    
    def _compute_similarity(self, emb1: torch.Tensor, emb2: torch.Tensor) -> float:
        """è®¡ç®—åµŒå…¥ç›¸ä¼¼åº¦"""
        try:
            # ç¡®ä¿ä¸¤ä¸ªåµŒå…¥å…·æœ‰ç›¸åŒçš„å½¢çŠ¶
            emb1_flat = emb1.flatten()
            emb2_flat = emb2.flatten()

            # æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é…
            if emb1_flat.shape != emb2_flat.shape:
                # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨è¾ƒå°çš„ç»´åº¦è¿›è¡Œæ¯”è¾ƒ
                min_size = min(emb1_flat.shape[0], emb2_flat.shape[0])
                emb1_flat = emb1_flat[:min_size]
                emb2_flat = emb2_flat[:min_size]

                # å¦‚æœç»´åº¦å·®å¼‚å¤ªå¤§ï¼Œè®¤ä¸ºä¸ç›¸ä¼¼
                size_ratio = max(emb1.numel(), emb2.numel()) / min(emb1.numel(), emb2.numel())
                if size_ratio > 2.0:  # å¦‚æœå¤§å°å·®å¼‚è¶…è¿‡2å€ï¼Œè®¤ä¸ºä¸ç›¸ä¼¼
                    return 0.0

            # ä½™å¼¦ç›¸ä¼¼åº¦
            cos_sim = F.cosine_similarity(emb1_flat, emb2_flat, dim=0)

            # L2è·ç¦»ç›¸ä¼¼åº¦
            l2_dist = torch.norm(emb1_flat - emb2_flat)
            l2_sim = 1.0 / (1.0 + l2_dist.item())

            # åŠ æƒç»„åˆ
            similarity = 0.7 * cos_sim.item() + 0.3 * l2_sim
            return similarity

        except Exception as e:
            # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œè¿”å›0ç›¸ä¼¼åº¦
            print(f"[SpeakerEmbeddingCache] ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _find_similar_embeddings(self, target_embedding: torch.Tensor,
                                audio_hash: str) -> List[Tuple[str, torch.Tensor, float]]:
        """æŸ¥æ‰¾ç›¸ä¼¼çš„åµŒå…¥"""
        similar_embeddings = []
        target_shape = target_embedding.shape
        target_size = target_embedding.numel()

        for cached_hash, cached_data in self.cache.items():
            if cached_hash == audio_hash:
                continue

            cached_embedding = cached_data['embedding']
            cached_shape = cached_data.get('embedding_shape', cached_embedding.shape)
            cached_size = cached_data.get('embedding_size', cached_embedding.numel())

            # åªæ¯”è¾ƒå½¢çŠ¶å’Œå¤§å°ç›¸ä¼¼çš„åµŒå…¥
            if cached_shape == target_shape or cached_size == target_size:
                similarity = self._compute_similarity(target_embedding, cached_embedding)

                if similarity > self.similarity_threshold:
                    similar_embeddings.append((cached_hash, cached_embedding, similarity))
            else:
                # è·³è¿‡ç»´åº¦ä¸åŒ¹é…çš„åµŒå…¥
                continue

        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similar_embeddings.sort(key=lambda x: x[2], reverse=True)
        return similar_embeddings[:5]  # æœ€å¤šè¿”å›5ä¸ªç›¸ä¼¼åµŒå…¥
    
    def _fuse_embeddings(self, embeddings: List[Tuple[torch.Tensor, float]]) -> torch.Tensor:
        """å¤šæ ·æœ¬åµŒå…¥èåˆ"""
        if len(embeddings) == 1:
            return embeddings[0][0]

        try:
            # æ£€æŸ¥æ‰€æœ‰åµŒå…¥çš„å½¢çŠ¶æ˜¯å¦ä¸€è‡´
            reference_shape = embeddings[0][0].shape
            compatible_embeddings = []

            for embedding, weight in embeddings:
                if embedding.shape == reference_shape:
                    compatible_embeddings.append((embedding, weight))
                else:
                    # å¦‚æœå½¢çŠ¶ä¸åŒ¹é…ï¼Œå°è¯•è°ƒæ•´æˆ–è·³è¿‡
                    if embedding.numel() == embeddings[0][0].numel():
                        # å¦‚æœå…ƒç´ æ•°é‡ç›¸åŒï¼Œé‡å¡‘å½¢çŠ¶
                        reshaped_embedding = embedding.reshape(reference_shape)
                        compatible_embeddings.append((reshaped_embedding, weight))
                    else:
                        # å½¢çŠ¶å’Œå¤§å°éƒ½ä¸åŒ¹é…ï¼Œè·³è¿‡è¿™ä¸ªåµŒå…¥
                        print(f"[SpeakerEmbeddingCache] è·³è¿‡ä¸å…¼å®¹çš„åµŒå…¥: {embedding.shape} vs {reference_shape}")
                        continue

            # å¦‚æœæ²¡æœ‰å…¼å®¹çš„åµŒå…¥ï¼Œè¿”å›ç¬¬ä¸€ä¸ª
            if len(compatible_embeddings) <= 1:
                return embeddings[0][0]

            # åŠ æƒå¹³å‡èåˆ
            total_weight = sum(weight for _, weight in compatible_embeddings)
            fused_embedding = torch.zeros_like(compatible_embeddings[0][0])

            for embedding, weight in compatible_embeddings:
                fused_embedding += embedding * (weight / total_weight)

            return fused_embedding

        except Exception as e:
            print(f"[SpeakerEmbeddingCache] åµŒå…¥èåˆå¤±è´¥: {e}")
            # è¿”å›ç¬¬ä¸€ä¸ªåµŒå…¥ä½œä¸ºå¤‡ç”¨
            return embeddings[0][0]
    
    def get_or_compute_embedding(self, audio: torch.Tensor, 
                               extractor_func,
                               metadata: Dict = None,
                               force_recompute: bool = False) -> torch.Tensor:
        """è·å–æˆ–è®¡ç®—è¯´è¯äººåµŒå…¥"""
        with self._lock:
            start_time = time.time()
            self.stats['total_requests'] += 1

            # è®¡ç®—éŸ³é¢‘å“ˆå¸Œ
            audio_hash = self._compute_audio_hash(audio, metadata)

            # æ£€æŸ¥ç¼“å­˜
            if not force_recompute and audio_hash in self.cache:
                # ç¼“å­˜å‘½ä¸­
                self.stats['cache_hits'] += 1
                self.access_count[audio_hash] += 1

                # ç§»åŠ¨åˆ°æœ«å°¾ï¼ˆLRUï¼‰
                cached_data = self.cache.pop(audio_hash)
                self.cache[audio_hash] = cached_data

                # è®°å½•æ€§èƒ½æ•°æ®
                response_time = time.time() - start_time
                if self.adaptive_cache_strategy:
                    cache_performance = {
                        'hit_rate': self.stats['cache_hits'] / self.stats['total_requests'],
                        'response_time': response_time,
                        'cache_size': len(self.cache)
                    }
                    self.adaptive_cache_strategy.analyze_usage_patterns(
                        speaker_id=audio_hash[:8],  # ç®€åŒ–çš„è¯´è¯äººID
                        session_duration=response_time,
                        cache_performance=cache_performance
                    )

                return cached_data['embedding']
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œè®¡ç®—æ–°åµŒå…¥
            self.stats['cache_misses'] += 1
            embedding = extractor_func(audio)
            
            # å¤šæ ·æœ¬èåˆï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.enable_multi_sample_fusion:
                similar_embeddings = self._find_similar_embeddings(embedding, audio_hash)
                
                if similar_embeddings:
                    self.stats['similarity_matches'] += len(similar_embeddings)
                    
                    # å‡†å¤‡èåˆæ•°æ®
                    fusion_data = [(embedding, 1.0)]  # å½“å‰åµŒå…¥æƒé‡ä¸º1.0
                    
                    for _, similar_emb, similarity in similar_embeddings:
                        # ç›¸ä¼¼åº¦è¶Šé«˜ï¼Œæƒé‡è¶Šå¤§
                        weight = similarity ** 2  # å¹³æ–¹å¢å¼ºå·®å¼‚
                        fusion_data.append((similar_emb, weight))
                    
                    # æ‰§è¡Œèåˆ
                    embedding = self._fuse_embeddings(fusion_data)
                    self.stats['fusion_operations'] += 1
            
            # ç¼“å­˜ç®¡ç†
            if len(self.cache) >= self.cache_size:
                # ç§»é™¤æœ€å°‘ä½¿ç”¨çš„åµŒå…¥
                lru_hash = next(iter(self.cache))
                del self.cache[lru_hash]
                del self.access_count[lru_hash]
                del self.creation_time[lru_hash]
            
            # æ·»åŠ åˆ°ç¼“å­˜
            self.cache[audio_hash] = {
                'embedding': embedding,
                'embedding_shape': embedding.shape,
                'embedding_size': embedding.numel(),
                'metadata': metadata or {},
                'audio_shape': audio.shape,
                'creation_time': time.time()
            }
            self.access_count[audio_hash] = 1
            self.creation_time[audio_hash] = time.time()
            
            return embedding
    
    def get_cache_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            hit_rate = (self.stats['cache_hits'] / max(self.stats['total_requests'], 1)) * 100
            
            return {
                'cache_size': len(self.cache),
                'max_cache_size': self.cache_size,
                'hit_rate': hit_rate,
                'total_requests': self.stats['total_requests'],
                'cache_hits': self.stats['cache_hits'],
                'cache_misses': self.stats['cache_misses'],
                'fusion_operations': self.stats['fusion_operations'],
                'similarity_matches': self.stats['similarity_matches'],
                'average_access_count': np.mean(list(self.access_count.values())) if self.access_count else 0
            }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self._lock:
            self.cache.clear()
            self.access_count.clear()
            self.creation_time.clear()
            self.sample_groups.clear()
    
    def save_cache_to_disk(self, filepath: str):
        """ä¿å­˜ç¼“å­˜åˆ°ç£ç›˜"""
        with self._lock:
            cache_data = {
                'cache': {k: {
                    'embedding': v['embedding'].cpu().numpy().tolist(),
                    'metadata': v['metadata'],
                    'audio_shape': v['audio_shape'],
                    'creation_time': v['creation_time']
                } for k, v in self.cache.items()},
                'access_count': dict(self.access_count),
                'stats': self.stats
            }
            
            with open(filepath, 'w') as f:
                json.dump(cache_data, f, indent=2)
    
    def load_cache_from_disk(self, filepath: str):
        """ä»ç£ç›˜åŠ è½½ç¼“å­˜"""
        if not os.path.exists(filepath):
            return
            
        with self._lock:
            try:
                with open(filepath, 'r') as f:
                    cache_data = json.load(f)
                
                # æ¢å¤ç¼“å­˜
                self.cache.clear()
                for k, v in cache_data.get('cache', {}).items():
                    self.cache[k] = {
                        'embedding': torch.tensor(v['embedding']),
                        'metadata': v['metadata'],
                        'audio_shape': v['audio_shape'],
                        'creation_time': v['creation_time']
                    }
                
                # æ¢å¤ç»Ÿè®¡ä¿¡æ¯
                self.access_count.update(cache_data.get('access_count', {}))
                self.stats.update(cache_data.get('stats', {}))
                
            except Exception as e:
                print(f"[SpeakerEmbeddingCache] åŠ è½½ç¼“å­˜å¤±è´¥: {e}")

class VoiceConsistencyController:
    """å£°éŸ³ä¸€è‡´æ€§æ§åˆ¶å™¨"""
    
    def __init__(self, consistency_threshold: float = 0.8, adaptation_rate: float = 0.1):
        self.consistency_threshold = consistency_threshold
        self.adaptation_rate = adaptation_rate
        
        # è¯´è¯äººæ¡£æ¡ˆ
        self.speaker_profiles = {}
        self.consistency_history = defaultdict(list)
        
        # å…¨å±€ä¸€è‡´æ€§ç»Ÿè®¡
        self.global_stats = {
            'total_checks': 0,
            'consistency_violations': 0,
            'corrections_applied': 0,
            'average_consistency': 0.0
        }
        
        # çº¿ç¨‹å®‰å…¨
        self._lock = threading.RLock()
    
    def register_speaker(self, speaker_id: str, reference_embedding: torch.Tensor, 
                        metadata: Dict = None):
        """æ³¨å†Œè¯´è¯äººå‚è€ƒåµŒå…¥"""
        with self._lock:
            self.speaker_profiles[speaker_id] = {
                'reference_embedding': reference_embedding.clone(),
                'embedding_history': [reference_embedding.clone()],
                'consistency_scores': [],
                'metadata': metadata or {},
                'registration_time': time.time(),
                'update_count': 0
            }
    
    def compute_consistency_score(self, current_embedding: torch.Tensor, 
                                speaker_id: str) -> float:
        """è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°"""
        with self._lock:
            if speaker_id not in self.speaker_profiles:
                return 1.0
            
            profile = self.speaker_profiles[speaker_id]
            reference = profile['reference_embedding']
            
            # å¤šç»´åº¦ä¸€è‡´æ€§è¯„ä¼°
            cos_sim = F.cosine_similarity(
                current_embedding.flatten(), 
                reference.flatten(), 
                dim=0
            ).item()
            
            # L2è·ç¦»ä¸€è‡´æ€§
            l2_dist = torch.norm(current_embedding.flatten() - reference.flatten()).item()
            l2_consistency = 1.0 / (1.0 + l2_dist)
            
            # å†å²ä¸€è‡´æ€§ï¼ˆä¸æœ€è¿‘å‡ ä¸ªåµŒå…¥çš„å¹³å‡ç›¸ä¼¼åº¦ï¼‰
            if len(profile['embedding_history']) > 1:
                recent_embeddings = profile['embedding_history'][-3:]  # æœ€è¿‘3ä¸ª
                hist_similarities = []
                
                for hist_emb in recent_embeddings:
                    hist_sim = F.cosine_similarity(
                        current_embedding.flatten(),
                        hist_emb.flatten(),
                        dim=0
                    ).item()
                    hist_similarities.append(hist_sim)
                
                hist_consistency = np.mean(hist_similarities)
            else:
                hist_consistency = cos_sim
            
            # åŠ æƒç»„åˆ
            consistency_score = (
                0.5 * cos_sim + 
                0.3 * l2_consistency + 
                0.2 * hist_consistency
            )
            
            return consistency_score
    
    def apply_consistency_constraint(self, current_embedding: torch.Tensor,
                                   speaker_id: str,
                                   constraint_strength: float = None) -> torch.Tensor:
        """åº”ç”¨ä¸€è‡´æ€§çº¦æŸ"""
        with self._lock:
            self.global_stats['total_checks'] += 1
            
            if speaker_id not in self.speaker_profiles:
                return current_embedding
            
            consistency_score = self.compute_consistency_score(current_embedding, speaker_id)
            profile = self.speaker_profiles[speaker_id]
            
            # è®°å½•ä¸€è‡´æ€§å†å²
            profile['consistency_scores'].append(consistency_score)
            if len(profile['consistency_scores']) > 20:
                profile['consistency_scores'].pop(0)
            
            # æ›´æ–°å…¨å±€ç»Ÿè®¡
            self.global_stats['average_consistency'] = (
                (self.global_stats['average_consistency'] * (self.global_stats['total_checks'] - 1) + 
                 consistency_score) / self.global_stats['total_checks']
            )
            
            if consistency_score < self.consistency_threshold:
                # ä¸€è‡´æ€§ä¸è¶³ï¼Œåº”ç”¨çº¦æŸ
                self.global_stats['consistency_violations'] += 1
                self.global_stats['corrections_applied'] += 1
                
                # è‡ªé€‚åº”çº¦æŸå¼ºåº¦
                if constraint_strength is None:
                    # æ ¹æ®ä¸€è‡´æ€§åˆ†æ•°åŠ¨æ€è°ƒæ•´çº¦æŸå¼ºåº¦
                    constraint_strength = (self.consistency_threshold - consistency_score) * 0.5
                    constraint_strength = min(constraint_strength, 0.5)  # æœ€å¤§çº¦æŸå¼ºåº¦
                
                reference = profile['reference_embedding']
                
                # åŠ æƒå¹³å‡çº¦æŸ
                constrained_embedding = (
                    current_embedding * (1 - constraint_strength) + 
                    reference * constraint_strength
                )
                
                return constrained_embedding
            
            return current_embedding
    
    def update_speaker_profile(self, speaker_id: str, 
                             new_embedding: torch.Tensor,
                             consistency_score: float):
        """æ›´æ–°è¯´è¯äººæ¡£æ¡ˆ"""
        with self._lock:
            if speaker_id not in self.speaker_profiles:
                return
            
            profile = self.speaker_profiles[speaker_id]
            
            # æ›´æ–°åµŒå…¥å†å²
            profile['embedding_history'].append(new_embedding.clone())
            if len(profile['embedding_history']) > 10:
                profile['embedding_history'].pop(0)
            
            # è‡ªé€‚åº”æ›´æ–°å‚è€ƒåµŒå…¥
            if consistency_score > 0.9:  # é«˜è´¨é‡åµŒå…¥
                old_ref = profile['reference_embedding']
                profile['reference_embedding'] = (
                    old_ref * (1 - self.adaptation_rate) + 
                    new_embedding * self.adaptation_rate
                )
                profile['update_count'] += 1
    
    def get_consistency_stats(self) -> Dict:
        """è·å–ä¸€è‡´æ€§ç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            speaker_stats = {}
            
            for speaker_id, profile in self.speaker_profiles.items():
                if profile['consistency_scores']:
                    speaker_stats[speaker_id] = {
                        'average_consistency': np.mean(profile['consistency_scores']),
                        'min_consistency': np.min(profile['consistency_scores']),
                        'max_consistency': np.max(profile['consistency_scores']),
                        'consistency_trend': np.mean(profile['consistency_scores'][-5:]) if len(profile['consistency_scores']) >= 5 else 0,
                        'update_count': profile['update_count'],
                        'history_length': len(profile['embedding_history'])
                    }
            
            violation_rate = (self.global_stats['consistency_violations'] / 
                            max(self.global_stats['total_checks'], 1)) * 100
            
            return {
                'global_stats': self.global_stats,
                'violation_rate': violation_rate,
                'speaker_count': len(self.speaker_profiles),
                'speaker_stats': speaker_stats
            }

class AdaptiveQualityMonitor:
    """è‡ªé€‚åº”è´¨é‡ç›‘æ§å™¨"""

    def __init__(self, quality_thresholds: Dict = None, enable_auto_improvement: bool = False):
        self.quality_thresholds = quality_thresholds or {
            'snr': 20.0,  # dB
            'thd': 0.05,  # 5%
            'spectral_flatness': 0.5,
            'dynamic_range': 40.0,  # dB
            'peak_level': -3.0  # dB
        }

        # è‡ªåŠ¨æ”¹è¿›åŠŸèƒ½
        self.enable_auto_improvement = enable_auto_improvement
        
        # è´¨é‡å†å²
        self.quality_history = []
        self.quality_trends = defaultdict(list)
        
        # è‡ªé€‚åº”é˜ˆå€¼
        self.adaptive_thresholds = self.quality_thresholds.copy()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_assessments': 0,
            'quality_violations': 0,
            'auto_corrections': 0,
            'threshold_adaptations': 0,
            'improvements_applied': 0
        }
        
        # çº¿ç¨‹å®‰å…¨
        self._lock = threading.RLock()
    
    def compute_snr(self, audio: torch.Tensor) -> float:
        """è®¡ç®—ä¿¡å™ªæ¯”"""
        # ä¿¡å·åŠŸç‡ï¼ˆä½¿ç”¨RMSï¼‰
        signal_power = torch.mean(audio ** 2)
        
        # å™ªå£°ä¼°è®¡ï¼ˆä½¿ç”¨æœ€ä½10%çš„èƒ½é‡ï¼‰
        sorted_power = torch.sort(audio ** 2)[0]
        noise_power = torch.mean(sorted_power[:len(sorted_power)//10])
        
        if noise_power > 0:
            snr = 10 * torch.log10(signal_power / noise_power)
            return snr.item()
        
        return float('inf')
    
    def compute_thd(self, audio: torch.Tensor, sample_rate: int = 22050) -> float:
        """è®¡ç®—æ€»è°æ³¢å¤±çœŸ"""
        if audio.shape[-1] < 2048:
            return 0.0
        
        # FFTåˆ†æ
        fft = torch.fft.fft(audio.flatten())
        magnitude = torch.abs(fft[:len(fft)//2])
        
        # æ‰¾åˆ°åŸºé¢‘
        freqs = torch.fft.fftfreq(len(audio.flatten()), 1/sample_rate)[:len(fft)//2]
        fundamental_idx = torch.argmax(magnitude[1:]) + 1  # è·³è¿‡DCåˆ†é‡
        
        # è®¡ç®—è°æ³¢å¤±çœŸ
        fundamental_power = magnitude[fundamental_idx] ** 2
        harmonic_power = 0
        
        for harmonic in range(2, 6):  # 2-5æ¬¡è°æ³¢
            harmonic_freq = freqs[fundamental_idx] * harmonic
            harmonic_idx = torch.argmin(torch.abs(freqs - harmonic_freq))
            
            if harmonic_idx < len(magnitude):
                harmonic_power += magnitude[harmonic_idx] ** 2
        
        if fundamental_power > 0:
            thd = torch.sqrt(harmonic_power / fundamental_power)
            return thd.item()
        
        return 0.0
    
    def compute_spectral_flatness(self, audio: torch.Tensor) -> float:
        """è®¡ç®—é¢‘è°±å¹³å¦åº¦"""
        if audio.shape[-1] < 1024:
            return 0.5
        
        fft = torch.fft.fft(audio.flatten())
        magnitude = torch.abs(fft[:len(fft)//2])
        magnitude = magnitude[1:]  # å»é™¤DCåˆ†é‡
        
        # å‡ ä½•å¹³å‡ / ç®—æœ¯å¹³å‡
        geometric_mean = torch.exp(torch.mean(torch.log(magnitude + 1e-8)))
        arithmetic_mean = torch.mean(magnitude)
        
        if arithmetic_mean > 0:
            return (geometric_mean / arithmetic_mean).item()
        
        return 0.0
    
    def compute_dynamic_range(self, audio: torch.Tensor) -> float:
        """è®¡ç®—åŠ¨æ€èŒƒå›´"""
        max_level = torch.max(torch.abs(audio))
        
        # å™ªå£°åº•é™ï¼ˆæœ€ä½1%çš„èƒ½é‡ï¼‰
        sorted_abs = torch.sort(torch.abs(audio.flatten()))[0]
        noise_floor = torch.mean(sorted_abs[:len(sorted_abs)//100])
        
        if noise_floor > 0 and max_level > 0:
            dynamic_range = 20 * torch.log10(max_level / noise_floor)
            return dynamic_range.item()
        
        return 0.0
    
    def compute_peak_level(self, audio: torch.Tensor) -> float:
        """è®¡ç®—å³°å€¼ç”µå¹³"""
        peak = torch.max(torch.abs(audio))
        if peak > 0:
            peak_db = 20 * torch.log10(peak)
            return peak_db.item()
        
        return -float('inf')
    
    def assess_quality(self, audio: torch.Tensor, sample_rate: int = 22050) -> Dict[str, float]:
        """ç»¼åˆè´¨é‡è¯„ä¼°"""
        with self._lock:
            self.stats['total_assessments'] += 1
            
            # è®¡ç®—å„é¡¹æŒ‡æ ‡
            metrics = {
                'snr': self.compute_snr(audio),
                'thd': self.compute_thd(audio, sample_rate),
                'spectral_flatness': self.compute_spectral_flatness(audio),
                'dynamic_range': self.compute_dynamic_range(audio),
                'peak_level': self.compute_peak_level(audio)
            }
            
            # è´¨é‡è¯„åˆ†
            quality_scores = {}
            violations = 0
            
            for metric, value in metrics.items():
                threshold = self.adaptive_thresholds[metric]
                
                if metric in ['snr', 'dynamic_range']:
                    # è¶Šé«˜è¶Šå¥½
                    score = min(value / threshold, 1.0) if threshold > 0 else 1.0
                    if value < threshold:
                        violations += 1
                elif metric in ['thd']:
                    # è¶Šä½è¶Šå¥½
                    score = max(1.0 - value / threshold, 0.0) if threshold > 0 else 1.0
                    if value > threshold:
                        violations += 1
                elif metric == 'peak_level':
                    # æ¥è¿‘é˜ˆå€¼æœ€å¥½
                    score = max(1.0 - abs(value - threshold) / 10.0, 0.0)
                    if value > -1.0:  # è¿‡è½½æ£€æŸ¥
                        violations += 1
                else:
                    # spectral_flatnessç­‰ï¼Œæ¥è¿‘é˜ˆå€¼æœ€å¥½
                    score = max(1.0 - abs(value - threshold) / threshold, 0.0) if threshold > 0 else 1.0
                
                quality_scores[metric] = score
            
            # ç»¼åˆè´¨é‡åˆ†æ•°
            overall_quality = np.mean(list(quality_scores.values()))
            
            # è®°å½•å†å²
            quality_record = {
                'timestamp': time.time(),
                'metrics': metrics,
                'scores': quality_scores,
                'overall_quality': overall_quality,
                'violations': violations
            }
            
            self.quality_history.append(quality_record)
            if len(self.quality_history) > 100:
                self.quality_history.pop(0)
            
            # æ›´æ–°è¶‹åŠ¿
            for metric, value in metrics.items():
                self.quality_trends[metric].append(value)
                if len(self.quality_trends[metric]) > 50:
                    self.quality_trends[metric].pop(0)
            
            # ç»Ÿè®¡è¿è§„
            if violations > 0:
                self.stats['quality_violations'] += 1
            
            # è‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´
            self._adapt_thresholds()

            result = {
                'metrics': metrics,
                'scores': quality_scores,
                'overall_quality': overall_quality,
                'violations': violations,
                'adaptive_thresholds': self.adaptive_thresholds.copy(),
                'improved_audio': None,
                'improvement_applied': False
            }

            # è‡ªåŠ¨è´¨é‡æ”¹è¿› - å·²ç¦ç”¨
            # if self.enable_auto_improvement and violations > 0:
            #     try:
            #         improved_audio = self._auto_improve_audio(audio, metrics, sample_rate)
            #         if improved_audio is not None:
            #             result['improved_audio'] = improved_audio
            #             result['improvement_applied'] = True
            #             self.stats['auto_corrections'] += 1
            #     except Exception as e:
            #         print(f"[AdaptiveQualityMonitor] è‡ªåŠ¨æ”¹è¿›å¤±è´¥: {e}")
            print("[AdaptiveQualityMonitor] â„¹ï¸ è‡ªåŠ¨æ”¹è¿›åŠŸèƒ½å·²ç¦ç”¨ï¼Œä½¿ç”¨åŸå§‹éŸ³é¢‘")

            return result

    def _auto_improve_audio(self, audio: torch.Tensor, metrics: Dict, sample_rate: int) -> torch.Tensor:
        """è‡ªåŠ¨éŸ³é¢‘è´¨é‡æ”¹è¿›"""
        improved_audio = audio.clone()
        improvements_applied = []

        # 1. å¤„ç†è¿‡é«˜çš„å³°å€¼ç”µå¹³ (å‰Šæ³¢/è¿‡è½½)
        if metrics['peak_level'] > -1.0:  # å³°å€¼è¿‡é«˜
            # è®¡ç®—å½“å‰å³°å€¼
            current_peak = torch.max(torch.abs(improved_audio)).item()

            # åªæœ‰åœ¨çœŸæ­£è¿‡è½½æ—¶æ‰è¿›è¡Œé™å¹…
            if current_peak > 0.95:  # æ¥è¿‘å‰Šæ³¢
                # æ¸©å’Œçš„è½¯é™å¹…ï¼Œä¿æŒæ›´å¤šåŠ¨æ€
                target_peak = 0.9
                reduction_factor = target_peak / current_peak
                improved_audio = improved_audio * reduction_factor
                improvements_applied.append(f"å³°å€¼é™å¹…({reduction_factor:.2f})")
            elif current_peak > 0.8:  # è½»å¾®è¿‡è½½
                # éå¸¸æ¸©å’Œçš„å¤„ç†
                improved_audio = torch.tanh(improved_audio * 0.95)
                improvements_applied.append("è½¯é™å¹…")

        # 2. å¤„ç†é«˜THD (æ€»è°æ³¢å¤±çœŸ)
        if metrics['thd'] > 0.1:  # THD > 10%
            # åº”ç”¨ä½é€šæ»¤æ³¢å‡å°‘é«˜é¢‘å¤±çœŸ
            nyquist = sample_rate // 2
            cutoff_freq = min(8000, nyquist * 0.8)  # 8kHzæˆ–80%å¥ˆå¥æ–¯ç‰¹é¢‘ç‡

            # ç®€å•çš„ä½é€šæ»¤æ³¢ (ç§»åŠ¨å¹³å‡)
            kernel_size = max(3, int(sample_rate / cutoff_freq))
            if kernel_size % 2 == 0:
                kernel_size += 1

            # åˆ›å»ºä½é€šæ»¤æ³¢æ ¸
            kernel = torch.ones(kernel_size) / kernel_size
            kernel = kernel.unsqueeze(0).unsqueeze(0)

            # åº”ç”¨å·ç§¯æ»¤æ³¢
            if improved_audio.dim() == 2:
                improved_audio = improved_audio.unsqueeze(0)
                filtered = torch.nn.functional.conv1d(
                    improved_audio, kernel, padding=kernel_size//2
                )
                improved_audio = filtered.squeeze(0)

            improvements_applied.append("ä½é€šæ»¤æ³¢")

        # 3. å¤„ç†åŠ¨æ€èŒƒå›´é—®é¢˜
        if metrics['dynamic_range'] > 100.0:  # åŠ¨æ€èŒƒå›´è¿‡å¤§
            # åº”ç”¨æ¸©å’Œçš„å‹ç¼©
            threshold = 0.7
            ratio = 3.0

            # ç®€å•çš„å‹ç¼©ç®—æ³•
            abs_audio = torch.abs(improved_audio)
            mask = abs_audio > threshold

            if mask.any():
                # å¯¹è¶…è¿‡é˜ˆå€¼çš„éƒ¨åˆ†åº”ç”¨å‹ç¼©
                compressed_part = threshold + (abs_audio[mask] - threshold) / ratio
                improved_audio[mask] = torch.sign(improved_audio[mask]) * compressed_part

            improvements_applied.append("åŠ¨æ€å‹ç¼©")

        # 4. æ ‡å‡†åŒ–éŸ³é‡
        if len(improvements_applied) > 0:
            # æ ‡å‡†åŒ–åˆ°åˆé€‚çš„éŸ³é‡ - ä¿®å¤éŸ³é‡è¿‡å°é—®é¢˜
            current_rms = torch.sqrt(torch.mean(improved_audio ** 2))

            # åŠ¨æ€é€‰æ‹©ç›®æ ‡RMSï¼Œç¡®ä¿éŸ³é¢‘æœ‰è¶³å¤Ÿçš„éŸ³é‡
            if current_rms > 0.5:  # å¦‚æœå½“å‰éŸ³é‡å¾ˆå¤§
                target_rms = 0.3  # é€‚åº¦é™ä½
            elif current_rms > 0.1:  # å¦‚æœå½“å‰éŸ³é‡é€‚ä¸­
                target_rms = max(0.2, current_rms * 0.8)  # è½»å¾®è°ƒæ•´
            else:  # å¦‚æœå½“å‰éŸ³é‡å¾ˆå°
                target_rms = 0.2  # æå‡åˆ°åˆç†æ°´å¹³

            if current_rms > 1e-6:  # é¿å…é™¤é›¶
                normalization_factor = target_rms / current_rms
                # é™åˆ¶æ ‡å‡†åŒ–èŒƒå›´ï¼Œé¿å…è¿‡åº¦æ”¾å¤§æˆ–ç¼©å°
                normalization_factor = max(0.1, min(5.0, normalization_factor))
                improved_audio = improved_audio * normalization_factor
                improvements_applied.append(f"éŸ³é‡æ ‡å‡†åŒ–(x{normalization_factor:.2f})")

        if improvements_applied:
            print(f"[AdaptiveQualityMonitor] ğŸ”§ è‡ªåŠ¨æ”¹è¿›åº”ç”¨: {', '.join(improvements_applied)}")
            return improved_audio

        return None  # æ²¡æœ‰æ”¹è¿›

    def _adapt_thresholds(self):
        """è‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´"""
        if len(self.quality_history) < 10:
            return
        
        # åˆ†ææœ€è¿‘çš„è´¨é‡è¶‹åŠ¿
        recent_records = self.quality_history[-10:]
        
        for metric in self.quality_thresholds.keys():
            recent_values = [r['metrics'][metric] for r in recent_records]
            
            # è®¡ç®—è¶‹åŠ¿
            mean_value = np.mean(recent_values)
            std_value = np.std(recent_values)
            
            # è‡ªé€‚åº”è°ƒæ•´
            current_threshold = self.adaptive_thresholds[metric]
            
            if metric in ['snr', 'dynamic_range']:
                # å¦‚æœå¹³å‡å€¼æŒç»­é«˜äºé˜ˆå€¼ï¼Œå¯ä»¥é€‚å½“æé«˜é˜ˆå€¼
                if mean_value > current_threshold * 1.2:
                    new_threshold = current_threshold * 1.05
                    self.adaptive_thresholds[metric] = min(new_threshold, current_threshold * 1.5)
                    self.stats['threshold_adaptations'] += 1
            elif metric in ['thd']:
                # å¦‚æœå¹³å‡å€¼æŒç»­ä½äºé˜ˆå€¼ï¼Œå¯ä»¥é€‚å½“é™ä½é˜ˆå€¼
                if mean_value < current_threshold * 0.8:
                    new_threshold = current_threshold * 0.95
                    self.adaptive_thresholds[metric] = max(new_threshold, current_threshold * 0.5)
                    self.stats['threshold_adaptations'] += 1
    
    def get_quality_stats(self) -> Dict:
        """è·å–è´¨é‡ç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            if not self.quality_history:
                return {'message': 'No quality assessments yet'}
            
            # è®¡ç®—è¶‹åŠ¿
            trends = {}
            for metric, values in self.quality_trends.items():
                if len(values) >= 5:
                    recent_trend = np.mean(values[-5:]) - np.mean(values[-10:-5]) if len(values) >= 10 else 0
                    trends[metric] = recent_trend
            
            # æœ€è¿‘è´¨é‡ç»Ÿè®¡
            recent_qualities = [r['overall_quality'] for r in self.quality_history[-10:]]
            
            violation_rate = (self.stats['quality_violations'] / 
                            max(self.stats['total_assessments'], 1)) * 100
            
            return {
                'stats': self.stats,
                'violation_rate': violation_rate,
                'average_quality': np.mean(recent_qualities),
                'quality_trend': np.mean(recent_qualities[-5:]) - np.mean(recent_qualities[-10:-5]) if len(recent_qualities) >= 10 else 0,
                'current_thresholds': self.adaptive_thresholds,
                'original_thresholds': self.quality_thresholds,
                'metric_trends': trends,
                'assessment_count': len(self.quality_history)
            }

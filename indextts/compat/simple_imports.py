# -*- coding: utf-8 -*-
"""
Simple Transformers Compatibility Layer
ç®€å•çš„ transformers å…¼å®¹å±‚

ç›´æ¥è§£å†³å¯¼å…¥é—®é¢˜ï¼Œé¿å…å¤æ‚çš„å¾ªç¯ä¾èµ–ã€‚
"""

import warnings

def get_transformers_components():
    """è·å–æ‰€æœ‰éœ€è¦çš„ transformers ç»„ä»¶ - æ”¯æŒ 4.35+ æ‰€æœ‰ç‰ˆæœ¬"""
    components = {}

    # æ£€æŸ¥ transformers ç‰ˆæœ¬
    try:
        import transformers
        version = transformers.__version__
        print(f"[IndexTTS2 Compat] æ£€æµ‹åˆ° transformers ç‰ˆæœ¬: {version}")

        from packaging import version as pkg_version
        current_ver = pkg_version.parse(version)
        min_supported = pkg_version.parse("4.35.0")

        if current_ver >= min_supported:
            print(f"[IndexTTS2 Compat] âœ… ç‰ˆæœ¬ {version} å—æ”¯æŒ (>= 4.35.0)")
            version_supported = True
        else:
            print(f"[IndexTTS2 Compat] âš ï¸ ç‰ˆæœ¬ {version} è¿‡æ—§ï¼Œå»ºè®®å‡çº§åˆ° 4.35.0+")
            version_supported = False

    except ImportError:
        version = "æœªå®‰è£…"
        version_supported = False
        print(f"[IndexTTS2 Compat] âŒ transformers æœªå®‰è£…")

    # 1. GPT2 ç»„ä»¶ - æ™ºèƒ½é€‰æ‹©ç­–ç•¥ï¼Œæ”¯æŒ 4.35+ æ‰€æœ‰ç‰ˆæœ¬
    try:
        # ä¼˜å…ˆå°è¯•ç³»ç»Ÿ transformersï¼ˆé€‚ç”¨äº 4.35+ æ‰€æœ‰ç‰ˆæœ¬ï¼‰
        from transformers import GPT2Config, GPT2Model, GPT2PreTrainedModel

        # æµ‹è¯•åŸºæœ¬åŠŸèƒ½æ˜¯å¦æ­£å¸¸
        test_config = GPT2Config(vocab_size=100, n_positions=512, n_embd=768, n_layer=12, n_head=12)

        components['GPT2Config'] = GPT2Config
        components['GPT2Model'] = GPT2Model
        components['GPT2PreTrainedModel'] = GPT2PreTrainedModel
        components['gpt2_source'] = 'system'
        print(f"[IndexTTS2 Compat] âœ… ä½¿ç”¨ç³»ç»Ÿ GPT2 ç»„ä»¶ (transformers {version})")

    except (ImportError, Exception) as e:
        # å¦‚æœç³»ç»Ÿç‰ˆæœ¬æœ‰é—®é¢˜ï¼Œä½¿ç”¨å†…ç½®å…¼å®¹ç‰ˆæœ¬
        try:
            from indextts.gpt.transformers_gpt2 import GPT2Config, GPT2Model, GPT2PreTrainedModel
            components['GPT2Config'] = GPT2Config
            components['GPT2Model'] = GPT2Model
            components['GPT2PreTrainedModel'] = GPT2PreTrainedModel
            components['gpt2_source'] = 'fallback'
            print(f"[IndexTTS2 Compat] ğŸ”„ ä½¿ç”¨å†…ç½® GPT2 ç»„ä»¶ (ç³»ç»Ÿç‰ˆæœ¬é—®é¢˜: {e})")
        except ImportError as fallback_error:
            print(f"[IndexTTS2 Compat] âŒ GPT2 ç»„ä»¶åŠ è½½å¤±è´¥: {fallback_error}")
            raise
    
    # 2. ç”Ÿæˆç»„ä»¶ - 4.35+ ç‰ˆæœ¬é€šå¸¸éƒ½æ”¯æŒè¿™äº›ç»„ä»¶
    try:
        from transformers import LogitsProcessorList
        from transformers.generation import GenerationConfig
        from transformers import GenerationMixin

        # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
        test_config = GenerationConfig()

        components['LogitsProcessorList'] = LogitsProcessorList
        components['GenerationConfig'] = GenerationConfig
        components['GenerationMixin'] = GenerationMixin
        components['generation_source'] = 'system'
        print(f"[IndexTTS2 Compat] âœ… ä½¿ç”¨ç³»ç»Ÿç”Ÿæˆç»„ä»¶")

    except (ImportError, Exception) as e:
        try:
            from indextts.gpt.transformers_generation_utils import LogitsProcessorList, GenerationConfig, GenerationMixin
            components['LogitsProcessorList'] = LogitsProcessorList
            components['GenerationConfig'] = GenerationConfig
            components['GenerationMixin'] = GenerationMixin
            components['generation_source'] = 'fallback'
            print(f"[IndexTTS2 Compat] ğŸ”„ ä½¿ç”¨å†…ç½®ç”Ÿæˆç»„ä»¶ (ç³»ç»Ÿç‰ˆæœ¬é—®é¢˜: {e})")
        except ImportError as fallback_error:
            print(f"[IndexTTS2 Compat] âŒ ç”Ÿæˆç»„ä»¶åŠ è½½å¤±è´¥: {fallback_error}")
            raise
    
    # 3. è¾“å‡ºç±» - 4.35+ ç‰ˆæœ¬éƒ½åº”è¯¥æ”¯æŒ
    try:
        from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions

        # æµ‹è¯•ç±»æ˜¯å¦å¯ç”¨
        test_output = CausalLMOutputWithCrossAttentions()

        components['CausalLMOutputWithCrossAttentions'] = CausalLMOutputWithCrossAttentions
        components['output_source'] = 'system'
        print(f"[IndexTTS2 Compat] âœ… ä½¿ç”¨ç³»ç»Ÿè¾“å‡ºç±»")

    except (ImportError, Exception) as e:
        # åˆ›å»ºå…¼å®¹çš„å›é€€å®ç°
        from dataclasses import dataclass
        from typing import Optional, Tuple
        import torch

        @dataclass
        class CausalLMOutputWithCrossAttentions:
            """å…¼å®¹ç‰ˆæœ¬çš„ CausalLMOutputWithCrossAttentions - æ”¯æŒæ‰€æœ‰ transformers ç‰ˆæœ¬"""
            loss: Optional[torch.FloatTensor] = None
            logits: torch.FloatTensor = None
            past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
            hidden_states: Optional[Tuple[torch.FloatTensor]] = None
            attentions: Optional[Tuple[torch.FloatTensor]] = None
            cross_attentions: Optional[Tuple[torch.FloatTensor]] = None

        components['CausalLMOutputWithCrossAttentions'] = CausalLMOutputWithCrossAttentions
        components['output_source'] = 'fallback'
        print(f"[IndexTTS2 Compat] ğŸ”„ ä½¿ç”¨å†…ç½®è¾“å‡ºç±» (ç³»ç»Ÿç‰ˆæœ¬é—®é¢˜: {e})")
    
    # 4. åˆ†è¯å™¨ - 4.35+ ç‰ˆæœ¬éƒ½æ”¯æŒ AutoTokenizer
    try:
        from transformers import AutoTokenizer

        # æµ‹è¯•åŸºæœ¬åŠŸèƒ½ï¼ˆä¸å®é™…åŠ è½½æ¨¡å‹ï¼‰
        if hasattr(AutoTokenizer, 'from_pretrained'):
            components['AutoTokenizer'] = AutoTokenizer
            components['tokenizer_source'] = 'system'
            print(f"[IndexTTS2 Compat] âœ… ä½¿ç”¨ç³»ç»Ÿåˆ†è¯å™¨")
        else:
            raise ImportError("AutoTokenizer ç¼ºå°‘ from_pretrained æ–¹æ³•")

    except (ImportError, Exception) as e:
        print(f"[IndexTTS2 Compat] âŒ AutoTokenizer ä¸å¯ç”¨: {e}")
        warnings.warn(f"AutoTokenizer not available: {e}", UserWarning)
        raise
    
    # 5. æ¨¡å‹å¹¶è¡Œå·¥å…·
    try:
        from transformers.utils.model_parallel_utils import assert_device_map, get_device_map
        components['assert_device_map'] = assert_device_map
        components['get_device_map'] = get_device_map
        components['parallel_source'] = 'system'
    except ImportError:
        # ç®€å•çš„å›é€€å®ç°
        def assert_device_map(device_map, num_blocks):
            pass
        
        def get_device_map(num_blocks, range_list):
            return {i: 0 for i in range(num_blocks)}
        
        components['assert_device_map'] = assert_device_map
        components['get_device_map'] = get_device_map
        components['parallel_source'] = 'fallback'
    
    # 6. ç‰¹å¾æå–å™¨ - å¯é€‰ç»„ä»¶ï¼ŒæŸäº›ç‰ˆæœ¬å¯èƒ½ä¸æ”¯æŒ
    try:
        from transformers import SeamlessM4TFeatureExtractor

        # æµ‹è¯•ç±»æ˜¯å¦å¯ç”¨
        if hasattr(SeamlessM4TFeatureExtractor, '__init__'):
            components['SeamlessM4TFeatureExtractor'] = SeamlessM4TFeatureExtractor
            components['extractor_source'] = 'system'
            print(f"[IndexTTS2 Compat] âœ… ä½¿ç”¨ç³»ç»Ÿç‰¹å¾æå–å™¨")
        else:
            raise ImportError("SeamlessM4TFeatureExtractor ä¸å®Œæ•´")

    except (ImportError, Exception) as e:
        print(f"[IndexTTS2 Compat] âš ï¸ SeamlessM4TFeatureExtractor ä¸å¯ç”¨: {e}")
        warnings.warn(f"SeamlessM4TFeatureExtractor not available: {e}", UserWarning)
        # ä¸è®¾ç½®è¿™ä¸ªç»„ä»¶ï¼Œè®©è°ƒç”¨è€…å¤„ç†
    
    return components

# è·å–æ‰€æœ‰ç»„ä»¶
_components = get_transformers_components()

# å¯¼å‡ºç»„ä»¶
GPT2Config = _components['GPT2Config']
GPT2Model = _components['GPT2Model']
GPT2PreTrainedModel = _components['GPT2PreTrainedModel']
LogitsProcessorList = _components['LogitsProcessorList']
GenerationConfig = _components['GenerationConfig']
GenerationMixin = _components['GenerationMixin']
CausalLMOutputWithCrossAttentions = _components['CausalLMOutputWithCrossAttentions']
AutoTokenizer = _components['AutoTokenizer']
assert_device_map = _components['assert_device_map']
get_device_map = _components['get_device_map']

# å¯é€‰ç»„ä»¶
if 'SeamlessM4TFeatureExtractor' in _components:
    SeamlessM4TFeatureExtractor = _components['SeamlessM4TFeatureExtractor']

# æ‰“å°æœ€ç»ˆåŠ è½½çŠ¶æ€æ€»ç»“
print(f"\n[IndexTTS2 Compat] ğŸ¯ ç»„ä»¶åŠ è½½å®Œæˆ:")
print(f"  â€¢ GPT2: {_components['gpt2_source']}")
print(f"  â€¢ Generation: {_components['generation_source']}")
print(f"  â€¢ Output: {_components['output_source']}")
print(f"  â€¢ Tokenizer: {_components['tokenizer_source']}")
print(f"  â€¢ Parallel: {_components['parallel_source']}")
if 'extractor_source' in _components:
    print(f"  â€¢ Extractor: {_components['extractor_source']}")
else:
    print(f"  â€¢ Extractor: ä¸å¯ç”¨")

# ç‰ˆæœ¬å…¼å®¹æ€§æ€»ç»“
try:
    import transformers
    version = transformers.__version__
    from packaging import version as pkg_version
    current_ver = pkg_version.parse(version)
    min_supported = pkg_version.parse("4.35.0")

    if current_ver >= min_supported:
        print(f"[IndexTTS2 Compat] ğŸ‰ transformers {version} å®Œå…¨æ”¯æŒï¼")
    else:
        print(f"[IndexTTS2 Compat] âš ï¸ transformers {version} ç‰ˆæœ¬è¿‡æ—§ï¼Œå»ºè®®å‡çº§åˆ° 4.35.0+")
except ImportError:
    print(f"[IndexTTS2 Compat] âŒ transformers æœªå®‰è£…")

print(f"[IndexTTS2 Compat] ğŸš€ IndexTTS2 å…¼å®¹å±‚åˆå§‹åŒ–å®Œæˆ\n")

# å¯¼å‡ºåˆ—è¡¨
__all__ = [
    'GPT2Config',
    'GPT2Model',
    'GPT2PreTrainedModel',
    'LogitsProcessorList',
    'GenerationConfig',
    'GenerationMixin',
    'CausalLMOutputWithCrossAttentions',
    'AutoTokenizer',
    'assert_device_map',
    'get_device_map',
    'SeamlessM4TFeatureExtractor'  # å¯èƒ½ä¸å­˜åœ¨
]

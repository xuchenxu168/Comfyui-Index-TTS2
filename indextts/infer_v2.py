import os
from subprocess import CalledProcessError

os.environ['HF_HUB_CACHE'] = './checkpoints/hf_cache'
import time
import librosa
import torch
import torchaudio
from torch.nn.utils.rnn import pad_sequence

import warnings

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

from omegaconf import OmegaConf

from indextts.gpt.model_v2 import UnifiedVoice
from indextts.utils.maskgct_utils import build_semantic_model, build_semantic_codec
from indextts.utils.checkpoint import load_checkpoint
from indextts.utils.front import TextNormalizer, TextTokenizer

from indextts.s2mel.modules.commons import load_checkpoint2, MyModel
from indextts.s2mel.modules.bigvgan import bigvgan
from indextts.s2mel.modules.campplus.DTDNN import CAMPPlus
from indextts.s2mel.modules.audio import mel_spectrogram

# ä½¿ç”¨ç®€åŒ–å…¼å®¹å±‚å¯¼å…¥ transformers ç»„ä»¶
from indextts.compat.simple_imports import AutoTokenizer, SeamlessM4TFeatureExtractor
from modelscope import AutoModelForCausalLM
from huggingface_hub import hf_hub_download
import safetensors
import random
import torch.nn.functional as F

class IndexTTS2:
    def __init__(
            self, cfg_path="checkpoints/config.yaml", model_dir="checkpoints", is_fp16=False, device=None,
            use_cuda_kernel=None,
    ):
        """
        Args:
            cfg_path (str): path to the config file.
            model_dir (str): path to the model directory.
            is_fp16 (bool): whether to use fp16.
            device (str): device to use (e.g., 'cuda:0', 'cpu'). If None, it will be set automatically based on the availability of CUDA or MPS.
            use_cuda_kernel (None | bool): whether to use BigVGan custom fused activation CUDA kernel, only for CUDA device.
        """
        if device is not None:
            self.device = torch.device(device)
            self.is_fp16 = False if device == "cpu" else is_fp16
            self.use_cuda_kernel = use_cuda_kernel is not None and use_cuda_kernel and device.startswith("cuda")
        elif torch.cuda.is_available():
            self.device = torch.device("cuda:0")
            self.is_fp16 = is_fp16
            self.use_cuda_kernel = use_cuda_kernel is None or use_cuda_kernel
        elif hasattr(torch, "mps") and torch.backends.mps.is_available():
            self.device = torch.device("mps")
            self.is_fp16 = False  # Use float16 on MPS is overhead than float32
            self.use_cuda_kernel = False
        else:
            self.device = torch.device("cpu")
            self.is_fp16 = False
            self.use_cuda_kernel = False
            print(">> Be patient, it may take a while to run in CPU mode.")

        self.cfg = OmegaConf.load(cfg_path)
        self.model_dir = model_dir
        self.dtype = torch.float16 if self.is_fp16 else None
        self.stop_mel_token = self.cfg.gpt.stop_mel_token

        # ========== ç³»ç»Ÿæ€§å±æ€§åˆå§‹åŒ– - é¿å…æ‰€æœ‰AttributeError ==========
        # è¿›åº¦å¼•ç”¨æ˜¾ç¤ºï¼ˆå¯é€‰ï¼‰
        self.gr_progress = None

        # ç¼“å­˜å‚è€ƒéŸ³é¢‘
        self.cache_spk_cond = None
        self.cache_s2mel_style = None
        self.cache_s2mel_prompt = None
        self.cache_spk_audio_prompt = None
        self.cache_emo_cond = None
        self.cache_emo_audio_prompt = None
        self.cache_mel = None

        # æ¨¡å‹ç›¸å…³å±æ€§
        self.semantic_model = None
        self.semantic_codec = None
        self.gpt = None
        self.s2mel = None
        self.bigvgan = None
        self.campplus_model = None
        self.extract_features = None
        self.normalizer = None
        self.tokenizer = None

        # ç»Ÿè®¡å’ŒçŸ©é˜µå±æ€§
        self.semantic_mean = None
        self.semantic_std = None
        self.emo_matrix = None
        self.spk_matrix = None
        self.emo_num = None

        # mel_fnå‡½æ•° - ä½¿ç”¨æ­£ç¡®çš„å¯¼å…¥è·¯å¾„
        try:
            from indextts.s2mel.modules.audio import mel_spectrogram
            mel_fn_args = {
                "n_fft": self.cfg.s2mel['preprocess_params']['spect_params']['n_fft'],
                "win_size": self.cfg.s2mel['preprocess_params']['spect_params']['win_length'],
                "hop_size": self.cfg.s2mel['preprocess_params']['spect_params']['hop_length'],
                "num_mels": self.cfg.s2mel['preprocess_params']['spect_params']['n_mels'],
                "sampling_rate": self.cfg.s2mel["preprocess_params"]["sr"],
                "fmin": self.cfg.s2mel['preprocess_params']['spect_params'].get('fmin', 0),
                "fmax": None if self.cfg.s2mel['preprocess_params']['spect_params'].get('fmax', "None") == "None" else 8000,
                "center": False
            }
            self.mel_fn = lambda x: mel_spectrogram(x, **mel_fn_args)
        except ImportError as e:
            print(f"[WARNING] mel_spectrogramå¯¼å…¥å¤±è´¥: {e}")
            print("[WARNING] å°†åœ¨åç»­åˆå§‹åŒ–ä¸­é‡è¯•")
            self.mel_fn = None

        # æ¨¡å‹ç‰ˆæœ¬
        self.model_version = self.cfg.version if hasattr(self.cfg, "version") else None

        print("[IndexTTS2] âœ“ æ‰€æœ‰å…³é”®å±æ€§å·²æå‰åˆå§‹åŒ–")
        # ========== å±æ€§åˆå§‹åŒ–å®Œæˆ ==========

        # æ£€æŸ¥qwen_emoæ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        qwen_emo_path = os.path.join(self.model_dir, self.cfg.qwen_emo_path)
        if os.path.exists(qwen_emo_path):
            self.qwen_emo = QwenEmotion(qwen_emo_path)
        else:
            print(f"âš ï¸  Qwen emotion model not found at: {qwen_emo_path}")
            print("âš ï¸  Emotion analysis will be disabled")
            self.qwen_emo = None

        self.gpt = UnifiedVoice(**self.cfg.gpt)
        self.gpt_path = os.path.join(self.model_dir, self.cfg.gpt_checkpoint)
        load_checkpoint(self.gpt, self.gpt_path)
        self.gpt = self.gpt.to(self.device)
        if self.is_fp16:
            self.gpt.eval().half()
        else:
            self.gpt.eval()
        print(">> GPT weights restored from:", self.gpt_path)
        # ä½¿ç”¨å…¼å®¹æ€§æ¨¡å—æ£€æŸ¥DeepSpeedå¯ç”¨æ€§
        try:
            from indextts.compat.deepspeed_compat import DEEPSPEED_AVAILABLE, check_deepspeed_availability
            use_deepspeed, _ = check_deepspeed_availability()
        except ImportError:
            # å¦‚æœå…¼å®¹æ€§æ¨¡å—ä¸å¯ç”¨ï¼Œå›é€€åˆ°åŸå§‹æ£€æŸ¥
            use_deepspeed = False
            try:
                import deepspeed
                if hasattr(deepspeed, 'init_inference'):
                    use_deepspeed = True
                    print(">> DeepSpeedå¯ç”¨ï¼Œå¯ç”¨åŠ é€Ÿæ¨ç†")
                else:
                    print(">> DeepSpeedæ¨¡å—ä¸å®Œæ•´ï¼Œä½¿ç”¨æ ‡å‡†PyTorchæ¨ç†")
            except (ImportError, OSError, CalledProcessError, FileNotFoundError, AttributeError, ModuleNotFoundError) as e:
                use_deepspeed = False
                print(f">> DeepSpeedä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†PyTorchæ¨ç†: {e}")
                if "deepspeed.utils.torch" in str(e):
                    print(">> æ£€æµ‹åˆ°DeepSpeedç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼Œå»ºè®®æ›´æ–°DeepSpeedæˆ–ä½¿ç”¨æ ‡å‡†æ¨ç†æ¨¡å¼")

        if self.is_fp16:
            self.gpt.post_init_gpt2_config(use_deepspeed=use_deepspeed, kv_cache=True, half=True)
        else:
            self.gpt.post_init_gpt2_config(use_deepspeed=use_deepspeed, kv_cache=True, half=False)

        if self.use_cuda_kernel:
            # preload the CUDA kernel for BigVGAN
            try:
                from indextts.BigVGAN.alias_free_activation.cuda import load

                anti_alias_activation_cuda = load.load()
                print(">> Preload custom CUDA kernel for BigVGAN", anti_alias_activation_cuda)
            except:
                print(">> Failed to load custom CUDA kernel for BigVGAN. Falling back to torch.")
                self.use_cuda_kernel = False

        # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰w2v-bertæ¨¡å‹æ–‡ä»¶
        from indextts.utils.model_cache_manager import get_indextts2_cache_dir
        cache_dir = get_indextts2_cache_dir()

        # åˆå§‹åŒ–local_w2v_pathå˜é‡
        local_w2v_path = None

        # æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„HuggingFaceç¼“å­˜æ ¼å¼
        hf_cache_paths = [
            # æ ‡å‡†external_modelsç¼“å­˜
            cache_dir / "w2v_bert" / "models--facebook--w2v-bert-2.0",
            cache_dir / "w2v_bert" / "facebook_w2v-bert-2.0",
            # HuggingFace Hubç¼“å­˜
            cache_dir / "huggingface" / "hub" / "models--facebook--w2v-bert-2.0",
            cache_dir / "huggingface" / "transformers" / "models--facebook--w2v-bert-2.0",
            # å…¶ä»–å¯èƒ½çš„æ ¼å¼
            cache_dir / "models--facebook--w2v-bert-2.0",
            cache_dir.parent / "w2v_bert" / "models--facebook--w2v-bert-2.0",
        ]

        # æŸ¥æ‰¾HuggingFaceç¼“å­˜ä¸­çš„snapshotsç›®å½•
        for hf_path in hf_cache_paths:
            if hf_path.exists():
                snapshots_dir = hf_path / "snapshots"
                if snapshots_dir.exists():
                    for snapshot in snapshots_dir.iterdir():
                        if snapshot.is_dir():
                            config_file = snapshot / "config.json"
                            model_file = snapshot / "model.safetensors"
                            preprocessor_file = snapshot / "preprocessor_config.json"
                            if config_file.exists() and model_file.exists():
                                local_w2v_path = snapshot
                                print(f"[IndexTTS2] å‘ç°æœ¬åœ°w2v-bertæ¨¡å‹ (HuggingFaceç¼“å­˜): {local_w2v_path}")
                                break
                    if local_w2v_path:
                        break

        # å¦‚æœHuggingFaceç¼“å­˜ä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œæ£€æŸ¥ç›´æ¥è·¯å¾„
        if not local_w2v_path:
            direct_paths = [
                cache_dir / "w2v_bert",  # æ ‡å‡†ç¼“å­˜è·¯å¾„
                cache_dir,  # ç›´æ¥åœ¨external_modelsç›®å½•
                cache_dir.parent / "w2v_bert",  # ä¸Šä¸€çº§ç›®å½•çš„w2v_bertæ–‡ä»¶å¤¹
            ]
            for path in direct_paths:
                config_file = path / "config.json"
                model_file = path / "model.safetensors"
                if config_file.exists() and model_file.exists():
                    local_w2v_path = path
                    print(f"[IndexTTS2] å‘ç°æœ¬åœ°w2v-bertæ¨¡å‹ (ç›´æ¥è·¯å¾„): {local_w2v_path}")
                    break

        # åŠ è½½SeamlessM4TFeatureExtractor
        if local_w2v_path:
            print(f"[IndexTTS2] ä½¿ç”¨æœ¬åœ°w2v-bertæ¨¡å‹: {local_w2v_path}")
            self.extract_features = SeamlessM4TFeatureExtractor.from_pretrained(
                str(local_w2v_path),
                local_files_only=True
            )
        else:
            print(f"[IndexTTS2] æœ¬åœ°æœªæ‰¾åˆ°w2v-bertæ¨¡å‹ï¼Œå°è¯•ä»è¿œç¨‹ä¸‹è½½...")
            from indextts.utils.model_cache_manager import get_hf_download_kwargs
            w2v_kwargs = get_hf_download_kwargs("facebook/w2v-bert-2.0")
            self.extract_features = SeamlessM4TFeatureExtractor.from_pretrained(
                "facebook/w2v-bert-2.0", **w2v_kwargs
            )
        self.semantic_model, self.semantic_mean, self.semantic_std = build_semantic_model(
            os.path.join(self.model_dir, self.cfg.w2v_stat))
        self.semantic_model = self.semantic_model.to(self.device)
        self.semantic_model.eval()
        self.semantic_mean = self.semantic_mean.to(self.device)
        self.semantic_std = self.semantic_std.to(self.device)

        semantic_codec = build_semantic_codec(self.cfg.semantic_codec)

        # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰MaskGCTè¯­ä¹‰ç¼–è§£ç å™¨
        local_maskgct_path = None

        # æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„HuggingFaceç¼“å­˜æ ¼å¼
        maskgct_cache_paths = [
            # æ ‡å‡†external_modelsç¼“å­˜
            cache_dir / "maskgct" / "models--amphion--MaskGCT",
            cache_dir / "maskgct" / "amphion_MaskGCT",
            # HuggingFace Hubç¼“å­˜
            cache_dir / "huggingface" / "hub" / "models--amphion--MaskGCT",
            cache_dir / "huggingface" / "transformers" / "models--amphion--MaskGCT",
            # å…¶ä»–å¯èƒ½çš„æ ¼å¼
            cache_dir / "models--amphion--MaskGCT",
            cache_dir.parent / "maskgct" / "models--amphion--MaskGCT",
        ]

        # æŸ¥æ‰¾HuggingFaceç¼“å­˜ä¸­çš„snapshotsç›®å½•
        for hf_path in maskgct_cache_paths:
            if hf_path.exists():
                snapshots_dir = hf_path / "snapshots"
                if snapshots_dir.exists():
                    for snapshot in snapshots_dir.iterdir():
                        if snapshot.is_dir():
                            semantic_codec_file = snapshot / "semantic_codec" / "model.safetensors"
                            if semantic_codec_file.exists():
                                local_maskgct_path = semantic_codec_file
                                print(f"[IndexTTS2] å‘ç°æœ¬åœ°MaskGCTè¯­ä¹‰ç¼–è§£ç å™¨ (HuggingFaceç¼“å­˜): {local_maskgct_path}")
                                break
                    if local_maskgct_path:
                        break

        # åŠ è½½MaskGCTè¯­ä¹‰ç¼–è§£ç å™¨
        if local_maskgct_path:
            print(f"[IndexTTS2] ä½¿ç”¨æœ¬åœ°MaskGCTè¯­ä¹‰ç¼–è§£ç å™¨: {local_maskgct_path}")
            semantic_code_ckpt = str(local_maskgct_path)
        else:
            print(f"[IndexTTS2] æœ¬åœ°æœªæ‰¾åˆ°MaskGCTè¯­ä¹‰ç¼–è§£ç å™¨ï¼Œå°è¯•ä»è¿œç¨‹ä¸‹è½½...")
            maskgct_kwargs = get_hf_download_kwargs("amphion/MaskGCT")
            semantic_code_ckpt = hf_hub_download(
                "amphion/MaskGCT",
                filename="semantic_codec/model.safetensors",
                **maskgct_kwargs
            )
        safetensors.torch.load_model(semantic_codec, semantic_code_ckpt)
        self.semantic_codec = semantic_codec.to(self.device)
        self.semantic_codec.eval()
        print('>> semantic_codec weights restored from: {}'.format(semantic_code_ckpt))

        s2mel_path = os.path.join(self.model_dir, self.cfg.s2mel_checkpoint)
        s2mel = MyModel(self.cfg.s2mel, use_gpt_latent=True)
        s2mel, _, _, _ = load_checkpoint2(
            s2mel,
            None,
            s2mel_path,
            load_only_params=True,
            ignore_modules=[],
            is_distributed=False,
        )
        self.s2mel = s2mel.to(self.device)
        self.s2mel.models['cfm'].estimator.setup_caches(max_batch_size=1, max_seq_length=8192)
        self.s2mel.eval()
        print(">> s2mel weights restored from:", s2mel_path)

        # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰CAMPPlusæ¨¡å‹
        local_campplus_path = None

        # æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„HuggingFaceç¼“å­˜æ ¼å¼
        campplus_cache_paths = [
            # æ ‡å‡†external_modelsç¼“å­˜
            cache_dir / "campplus" / "models--funasr--campplus",
            cache_dir / "campplus" / "funasr_campplus",
            # HuggingFace Hubç¼“å­˜
            cache_dir / "huggingface" / "hub" / "models--funasr--campplus",
            cache_dir / "huggingface" / "transformers" / "models--funasr--campplus",
            # å…¶ä»–å¯èƒ½çš„æ ¼å¼
            cache_dir / "models--funasr--campplus",
            cache_dir.parent / "campplus" / "models--funasr--campplus",
        ]

        # æŸ¥æ‰¾HuggingFaceç¼“å­˜ä¸­çš„snapshotsç›®å½•
        for hf_path in campplus_cache_paths:
            if hf_path.exists():
                snapshots_dir = hf_path / "snapshots"
                if snapshots_dir.exists():
                    for snapshot in snapshots_dir.iterdir():
                        if snapshot.is_dir():
                            campplus_file = snapshot / "campplus_cn_common.bin"
                            if campplus_file.exists():
                                local_campplus_path = campplus_file
                                print(f"[IndexTTS2] å‘ç°æœ¬åœ°CAMPPlusæ¨¡å‹ (HuggingFaceç¼“å­˜): {local_campplus_path}")
                                break
                    if local_campplus_path:
                        break

        # åŠ è½½CAMPPlusæ¨¡å‹
        if local_campplus_path:
            print(f"[IndexTTS2] ä½¿ç”¨æœ¬åœ°CAMPPlusæ¨¡å‹: {local_campplus_path}")
            campplus_ckpt_path = str(local_campplus_path)
        else:
            print(f"[IndexTTS2] æœ¬åœ°æœªæ‰¾åˆ°CAMPPlusæ¨¡å‹ï¼Œå°è¯•ä»è¿œç¨‹ä¸‹è½½...")
            campplus_kwargs = get_hf_download_kwargs("funasr/campplus")
            campplus_ckpt_path = hf_hub_download(
                "funasr/campplus",
                filename="campplus_cn_common.bin",
                **campplus_kwargs
            )
        campplus_model = CAMPPlus(feat_dim=80, embedding_size=192)
        campplus_model.load_state_dict(torch.load(campplus_ckpt_path, map_location="cpu"))
        self.campplus_model = campplus_model.to(self.device)
        self.campplus_model.eval()
        print(">> campplus_model weights restored from:", campplus_ckpt_path)

        bigvgan_name = self.cfg.vocoder.name
        # ä¸‹è½½BigVGANåˆ°ComfyUIæ¨¡å‹ç›®å½•
        from indextts.utils.model_cache_manager import get_bigvgan_download_kwargs
        bigvgan_kwargs = get_bigvgan_download_kwargs(bigvgan_name)
        
        # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰BigVGANæ¨¡å‹æ–‡ä»¶
        from indextts.utils.model_cache_manager import get_indextts2_cache_dir
        cache_dir = get_indextts2_cache_dir()

        # åˆå§‹åŒ–local_bigvgan_pathå˜é‡
        local_bigvgan_path = None

        # æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„æœ¬åœ°è·¯å¾„
        local_bigvgan_paths = [
            cache_dir / "bigvgan",  # æ ‡å‡†ç¼“å­˜è·¯å¾„
            cache_dir,  # ç›´æ¥åœ¨external_modelsç›®å½•
            cache_dir.parent / "bigvgan",  # ä¸Šä¸€çº§ç›®å½•çš„bigvganæ–‡ä»¶å¤¹
        ]
        
        # æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„HuggingFaceç¼“å­˜æ ¼å¼
        hf_cache_paths = [
            # æ ‡å‡†external_modelsç¼“å­˜
            cache_dir / "bigvgan" / "models--nvidia--bigvgan_v2_22khz_80band_256x",
            cache_dir / "bigvgan" / "nvidia_bigvgan_v2_22khz_80band_256x",
            # HuggingFace Hubç¼“å­˜
            cache_dir / "huggingface" / "hub" / "models--nvidia--bigvgan_v2_22khz_80band_256x",
            cache_dir / "huggingface" / "transformers" / "models--nvidia--bigvgan_v2_22khz_80band_256x",
            # å…¶ä»–å¯èƒ½çš„æ ¼å¼
            cache_dir / "models--nvidia--bigvgan_v2_22khz_80band_256x",
            cache_dir.parent / "bigvgan" / "models--nvidia--bigvgan_v2_22khz_80band_256x",
        ]
        
        # æŸ¥æ‰¾HuggingFaceç¼“å­˜ä¸­çš„snapshotsç›®å½•
        for hf_path in hf_cache_paths:
            if hf_path.exists():
                snapshots_dir = hf_path / "snapshots"
                if snapshots_dir.exists():
                    for snapshot in snapshots_dir.iterdir():
                        if snapshot.is_dir():
                            config_file = snapshot / "config.json"
                            model_file = snapshot / "bigvgan_generator.pt"
                            if config_file.exists() and model_file.exists():
                                local_bigvgan_path = snapshot
                                print(f"[IndexTTS2] å‘ç°æœ¬åœ°BigVGANæ¨¡å‹ (HuggingFaceç¼“å­˜): {local_bigvgan_path}")
                                break
                    if local_bigvgan_path:
                        break
        
        # å¦‚æœHuggingFaceç¼“å­˜ä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œæ£€æŸ¥ç›´æ¥è·¯å¾„
        if not local_bigvgan_path:
            for path in local_bigvgan_paths:
                config_file = path / "config.json"
                model_file = path / "bigvgan_generator.pt"
                if config_file.exists() and model_file.exists():
                    local_bigvgan_path = path
                    print(f"[IndexTTS2] å‘ç°æœ¬åœ°BigVGANæ¨¡å‹ (ç›´æ¥è·¯å¾„): {local_bigvgan_path}")
                    break
        
        # æ·»åŠ è¶…æ—¶å’Œé”™è¯¯å¤„ç†çš„BigVGANåŠ è½½
        print(f"[IndexTTS2] å¼€å§‹åŠ è½½BigVGANæ¨¡å‹: {bigvgan_name}")
        print(f"[IndexTTS2] ç¼“å­˜ç›®å½•: {bigvgan_kwargs['cache_dir']}")
        
        # æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦æ”¯æŒsignal.SIGALRM (Windowsä¸æ”¯æŒ)
        import threading
        import platform
        import signal
        
        # æ£€æŸ¥æ˜¯å¦åœ¨ä¸»çº¿ç¨‹ä¸­è¿è¡Œ
        import threading
        is_main_thread = threading.current_thread() is threading.main_thread()
        
        if platform.system() == "Windows" or not hasattr(signal, 'SIGALRM') or not is_main_thread:
            # Windowsç³»ç»Ÿã€æ²¡æœ‰SIGALRMæˆ–ä¸åœ¨ä¸»çº¿ç¨‹ä¸­ï¼Œä½¿ç”¨threadingè¶…æ—¶æœºåˆ¶
            print("[IndexTTS2] ä½¿ç”¨threadingè¶…æ—¶æœºåˆ¶ (è·¨å¹³å°å…¼å®¹)")
            
            def load_bigvgan_with_timeout():
                try:
                    # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„
                    if local_bigvgan_path:
                        print(f"[IndexTTS2] ä½¿ç”¨æœ¬åœ°BigVGANæ¨¡å‹: {local_bigvgan_path}")
                        self.bigvgan = bigvgan.BigVGAN.from_pretrained(
                            str(local_bigvgan_path),  # ä½¿ç”¨æœ¬åœ°è·¯å¾„
                            use_cuda_kernel=False,
                            cache_dir=bigvgan_kwargs["cache_dir"]
                        )
                    else:
                        print(f"[IndexTTS2] ä»HuggingFaceä¸‹è½½BigVGANæ¨¡å‹: {bigvgan_name}")
                        self.bigvgan = bigvgan.BigVGAN.from_pretrained(
                            bigvgan_name,  # ä½¿ç”¨è¿œç¨‹ID
                            use_cuda_kernel=False,
                            cache_dir=bigvgan_kwargs["cache_dir"]
                        )

                    print("[IndexTTS2] å¼€å§‹åå¤„ç†BigVGANæ¨¡å‹...")

                    # æ£€æŸ¥GPUå†…å­˜
                    if self.device.type == 'cuda':
                        torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜
                        print(f"[IndexTTS2] GPUå†…å­˜æ¸…ç†å®Œæˆ")

                    # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
                    print(f"[IndexTTS2] å°†BigVGANæ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡: {self.device}")
                    self.bigvgan = self.bigvgan.to(self.device)
                    print("[IndexTTS2] âœ“ æ¨¡å‹ç§»åŠ¨å®Œæˆ")

                    # ç§»é™¤æƒé‡å½’ä¸€åŒ–
                    print("[IndexTTS2] ç§»é™¤æƒé‡å½’ä¸€åŒ–...")
                    self.bigvgan.remove_weight_norm()
                    print("[IndexTTS2] âœ“ æƒé‡å½’ä¸€åŒ–ç§»é™¤å®Œæˆ")

                    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
                    print("[IndexTTS2] è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼...")
                    self.bigvgan.eval()
                    print("[IndexTTS2] âœ“ è¯„ä¼°æ¨¡å¼è®¾ç½®å®Œæˆ")

                    return True
                except Exception as e:
                    print(f"[ERROR] BigVGANæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    return False
            
            # ä½¿ç”¨çº¿ç¨‹å’Œè¶…æ—¶
            result = [False]
            exception = [None]
            
            def target():
                try:
                    result[0] = load_bigvgan_with_timeout()
                except Exception as e:
                    exception[0] = e
            
            thread = threading.Thread(target=target)
            thread.daemon = True
            thread.start()
            thread.join(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
            
            if thread.is_alive():
                print("[ERROR] BigVGANæ¨¡å‹åŠ è½½è¶…æ—¶ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜")
                print("[ERROR] è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å°è¯•ä½¿ç”¨ä»£ç†")
                raise TimeoutError("BigVGANæ¨¡å‹åŠ è½½è¶…æ—¶")
            
            if exception[0]:
                raise exception[0]
            
            if not result[0]:
                raise RuntimeError("BigVGANæ¨¡å‹åŠ è½½å¤±è´¥")

            print(">> bigvgan weights restored from:", local_bigvgan_path if local_bigvgan_path else bigvgan_name)
            
        else:
            # Unix/Linuxç³»ç»Ÿä¸”åœ¨ä¸»çº¿ç¨‹ä¸­ï¼Œä½¿ç”¨signalè¶…æ—¶æœºåˆ¶
            print("[IndexTTS2] ä½¿ç”¨signalè¶…æ—¶æœºåˆ¶ (Unix/Linuxä¸»çº¿ç¨‹)")
            
            import signal
            
            def timeout_handler(signum, frame):
                raise TimeoutError("BigVGANæ¨¡å‹åŠ è½½è¶…æ—¶")
            
            # è®¾ç½®è¶…æ—¶ï¼ˆ5åˆ†é’Ÿï¼‰
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(300)  # 5åˆ†é’Ÿè¶…æ—¶
            
            try:
                # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„
                if local_bigvgan_path:
                    print(f"[IndexTTS2] ä½¿ç”¨æœ¬åœ°BigVGANæ¨¡å‹: {local_bigvgan_path}")
                    self.bigvgan = bigvgan.BigVGAN.from_pretrained(
                        str(local_bigvgan_path),  # ä½¿ç”¨æœ¬åœ°è·¯å¾„
                        use_cuda_kernel=False,
                        cache_dir=bigvgan_kwargs["cache_dir"]
                    )
                else:
                    print(f"[IndexTTS2] ä»HuggingFaceä¸‹è½½BigVGANæ¨¡å‹: {bigvgan_name}")
                    self.bigvgan = bigvgan.BigVGAN.from_pretrained(
                        bigvgan_name,  # ä½¿ç”¨è¿œç¨‹ID
                        use_cuda_kernel=False,
                        cache_dir=bigvgan_kwargs["cache_dir"]
                    )

                print("[IndexTTS2] å¼€å§‹åå¤„ç†BigVGANæ¨¡å‹...")

                # æ£€æŸ¥GPUå†…å­˜
                if self.device.type == 'cuda':
                    torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜
                    print(f"[IndexTTS2] GPUå†…å­˜æ¸…ç†å®Œæˆ")

                # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
                print(f"[IndexTTS2] å°†BigVGANæ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡: {self.device}")
                self.bigvgan = self.bigvgan.to(self.device)
                print("[IndexTTS2] âœ“ æ¨¡å‹ç§»åŠ¨å®Œæˆ")

                # ç§»é™¤æƒé‡å½’ä¸€åŒ–
                print("[IndexTTS2] ç§»é™¤æƒé‡å½’ä¸€åŒ–...")
                self.bigvgan.remove_weight_norm()
                print("[IndexTTS2] âœ“ æƒé‡å½’ä¸€åŒ–ç§»é™¤å®Œæˆ")

                # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
                print("[IndexTTS2] è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼...")
                self.bigvgan.eval()
                print("[IndexTTS2] âœ“ è¯„ä¼°æ¨¡å¼è®¾ç½®å®Œæˆ")

                signal.alarm(0)  # å–æ¶ˆè¶…æ—¶

                print(">> bigvgan weights restored from:", local_bigvgan_path if local_bigvgan_path else bigvgan_name)
                
            except TimeoutError:
                signal.alarm(0)
                print("[ERROR] BigVGANæ¨¡å‹åŠ è½½è¶…æ—¶ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜")
                print("[ERROR] è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å°è¯•ä½¿ç”¨ä»£ç†")
                raise
            except Exception as e:
                signal.alarm(0)
                print(f"[ERROR] BigVGANæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                print(f"[ERROR] æ¨¡å‹åç§°: {bigvgan_name}")
                print(f"[ERROR] ç¼“å­˜ç›®å½•: {bigvgan_kwargs['cache_dir']}")
                if local_bigvgan_path:
                    print(f"[ERROR] æœ¬åœ°è·¯å¾„: {local_bigvgan_path}")
                raise

        # æ£€æŸ¥BPEæ¨¡å‹æ–‡ä»¶è·¯å¾„
        bpe_filename = self.cfg.dataset["bpe_model"]

        # æ„å»ºå¯èƒ½çš„BPEæ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨å¤šç§æ–¹æ³•ç¡®ä¿å…¼å®¹æ€§
        possible_bpe_paths = []

        # æ–¹æ³•1: ç›´æ¥ä½¿ç”¨os.path.joinï¼ˆä¿æŒåŸæœ‰å…¼å®¹æ€§ï¼‰
        possible_bpe_paths.append(os.path.join(self.model_dir, bpe_filename))
        possible_bpe_paths.append(os.path.join(self.model_dir, "bpe_model.model"))

        # æ–¹æ³•2: å¦‚æœmodel_diræŒ‡å‘checkpointsï¼Œå°è¯•ä¸Šä¸€çº§ç›®å½•
        parent_dir = os.path.dirname(self.model_dir)
        possible_bpe_paths.append(os.path.join(parent_dir, bpe_filename))

        # æ–¹æ³•3: ç›¸å¯¹äºå½“å‰è„šæœ¬çš„è·¯å¾„
        script_dir = os.path.dirname(__file__)
        possible_bpe_paths.append(os.path.join(script_dir, "..", "bpe_model.model"))

        # æ–¹æ³•4: åœ¨å½“å‰å·¥ä½œç›®å½•æŸ¥æ‰¾
        possible_bpe_paths.append(bpe_filename)
        possible_bpe_paths.append("bpe_model.model")

        self.bpe_path = None
        for path in possible_bpe_paths:
            if os.path.exists(path):
                self.bpe_path = path
                print(f"[IndexTTS2] å‘ç°BPEæ¨¡å‹æ–‡ä»¶: {self.bpe_path}")
                break

        if not self.bpe_path:
            print(f"[ERROR] æœªæ‰¾åˆ°BPEæ¨¡å‹æ–‡ä»¶ï¼Œå°è¯•çš„è·¯å¾„:")
            for path in possible_bpe_paths:
                print(f"  {path} - {'å­˜åœ¨' if os.path.exists(path) else 'ä¸å­˜åœ¨'}")
            print(f"[ERROR] å½“å‰model_dir: {self.model_dir}")
            print(f"[ERROR] é…ç½®ä¸­çš„BPEæ–‡ä»¶å: {bpe_filename}")
            raise FileNotFoundError(f"BPEæ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {bpe_filename}")

        print("[IndexTTS2] å¼€å§‹åˆ›å»ºTextNormalizer...")
        print("[IndexTTS2] ç”±äºTextNormalizerç»å¸¸å¡ä½ï¼Œç›´æ¥ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬...")

        # ç›´æ¥ä½¿ç”¨ç®€åŒ–çš„TextNormalizerï¼Œé¿å…å¡ä½é—®é¢˜
        try:
            self.normalizer = self._create_fallback_normalizer()
            print("[IndexTTS2] âœ“ ä½¿ç”¨ç®€åŒ–TextNormalizerï¼ˆè·³è¿‡å¯èƒ½å¡ä½çš„æ­£å¸¸åŠ è½½ï¼‰")
            print(">> TextNormalizer loaded")
        except Exception as e:
            print(f"[ERROR] åˆ›å»ºç®€åŒ–TextNormalizerå¤±è´¥: {e}")
            raise RuntimeError(f"TextNormalizeråˆå§‹åŒ–å¤±è´¥: {e}")

    def _create_fallback_normalizer(self):
        """åˆ›å»ºä¸€ä¸ªå¢å¼ºçš„TextNormalizerä½œä¸ºå›é€€æ–¹æ¡ˆï¼ŒåŒ…å«æ•°å­—è½¬æ¢åŠŸèƒ½"""
        class EnhancedFallbackTextNormalizer:
            def __init__(self):
                self.zh_normalizer = self._create_enhanced_normalizer()
                self.en_normalizer = self._create_simple_normalizer()

            def _create_enhanced_normalizer(self):
                class EnhancedNormalizer:
                    def __init__(self):
                        # ä¸­æ–‡æ•°å­—æ˜ å°„
                        self.digit_map = {
                            '0': 'é›¶', '1': 'ä¸€', '2': 'äºŒ', '3': 'ä¸‰', '4': 'å››',
                            '5': 'äº”', '6': 'å…­', '7': 'ä¸ƒ', '8': 'å…«', '9': 'ä¹'
                        }
                        self.unit_map = ['', 'å', 'ç™¾', 'åƒ', 'ä¸‡', 'åä¸‡', 'ç™¾ä¸‡', 'åƒä¸‡', 'äº¿']

                    def number_to_chinese(self, num_str):
                        """å°†æ•°å­—å­—ç¬¦ä¸²è½¬æ¢ä¸ºä¸­æ–‡"""
                        try:
                            num = int(num_str)
                            if num == 0:
                                return 'é›¶'

                            # æ‰©å±•çš„æ•°å­—è½¬æ¢ï¼ˆæ”¯æŒ0-99999999ï¼‰
                            if num < 10:
                                return self.digit_map[str(num)]
                            elif num < 100:
                                tens = num // 10
                                ones = num % 10
                                if tens == 1:
                                    result = 'å'
                                else:
                                    result = self.digit_map[str(tens)] + 'å'
                                if ones > 0:
                                    result += self.digit_map[str(ones)]
                                return result
                            elif num < 1000:
                                hundreds = num // 100
                                remainder = num % 100
                                result = self.digit_map[str(hundreds)] + 'ç™¾'
                                if remainder > 0:
                                    if remainder < 10:
                                        result += 'é›¶' + self.digit_map[str(remainder)]
                                    else:
                                        result += self.number_to_chinese(str(remainder))
                                return result
                            elif num < 10000:
                                thousands = num // 1000
                                remainder = num % 1000
                                result = self.digit_map[str(thousands)] + 'åƒ'
                                if remainder > 0:
                                    if remainder < 100:
                                        result += 'é›¶' + self.number_to_chinese(str(remainder))
                                    else:
                                        result += self.number_to_chinese(str(remainder))
                                return result
                            elif num < 100000:
                                # ä¸‡çº§åˆ«å¤„ç†
                                wan = num // 10000
                                remainder = num % 10000
                                if wan == 1:
                                    result = 'ä¸€ä¸‡'
                                else:
                                    result = self.number_to_chinese(str(wan)) + 'ä¸‡'
                                if remainder > 0:
                                    if remainder < 1000:
                                        result += 'é›¶' + self.number_to_chinese(str(remainder))
                                    else:
                                        result += self.number_to_chinese(str(remainder))
                                return result
                            elif num < 1000000:
                                # åä¸‡çº§åˆ«
                                wan = num // 10000
                                remainder = num % 10000
                                result = self.number_to_chinese(str(wan)) + 'ä¸‡'
                                if remainder > 0:
                                    if remainder < 1000:
                                        result += 'é›¶' + self.number_to_chinese(str(remainder))
                                    else:
                                        result += self.number_to_chinese(str(remainder))
                                return result
                            else:
                                # å¯¹äºæ›´å¤§çš„æ•°å­—ï¼Œç®€åŒ–å¤„ç†
                                if num < 100000000:  # ä¸€äº¿ä»¥ä¸‹
                                    wan = num // 10000
                                    remainder = num % 10000
                                    result = self.number_to_chinese(str(wan)) + 'ä¸‡'
                                    if remainder > 0:
                                        if remainder < 1000:
                                            result += 'é›¶' + self.number_to_chinese(str(remainder))
                                        else:
                                            result += self.number_to_chinese(str(remainder))
                                    return result
                                else:
                                    # è¶…å¤§æ•°å­—ï¼Œé€ä½è½¬æ¢
                                    return ''.join(self.digit_map.get(d, d) for d in num_str)
                        except:
                            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œé€ä½è½¬æ¢
                            return ''.join(self.digit_map.get(d, d) for d in num_str)

                    def normalize(self, text):
                        import re

                        # åŸºæœ¬çš„æ–‡æœ¬æ¸…ç†
                        text = re.sub(r'["""]', '"', text)
                        text = re.sub(r"[''']", "'", text)
                        text = re.sub(r'[â€¦]', '...', text)
                        text = re.sub(r'[â€”â€“]', '-', text)

                        # æ•°å­—è½¬æ¢ï¼ˆåŒ¹é…è¿ç»­çš„æ•°å­—ï¼‰
                        def replace_numbers(match):
                            number = match.group()
                            return self.number_to_chinese(number)

                        # è½¬æ¢è¿ç»­çš„æ•°å­—
                        text = re.sub(r'\d+', replace_numbers, text)

                        # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
                        text = re.sub(r'\s+', ' ', text)
                        text = text.strip()
                        return text

                return EnhancedNormalizer()

            def _create_simple_normalizer(self):
                class SimpleNormalizer:
                    def normalize(self, text):
                        import re
                        # è‹±æ–‡æ•°å­—ä¿æŒä¸å˜ï¼ŒåªåšåŸºæœ¬æ¸…ç†
                        text = re.sub(r'["""]', '"', text)
                        text = re.sub(r"[''']", "'", text)
                        text = re.sub(r'[â€¦]', '...', text)
                        text = re.sub(r'[â€”â€“]', '-', text)
                        text = re.sub(r'\s+', ' ', text)
                        text = text.strip()
                        return text
                return SimpleNormalizer()

            def normalize(self, text, lang="zh"):
                if lang == "zh":
                    return self.zh_normalizer.normalize(text)
                else:
                    return self.en_normalizer.normalize(text)

        return EnhancedFallbackTextNormalizer()

        print(f"[IndexTTS2] å¼€å§‹åˆ›å»ºTextTokenizerï¼ŒBPEè·¯å¾„: {self.bpe_path}")
        self.tokenizer = TextTokenizer(self.bpe_path, self.normalizer)
        print("[IndexTTS2] âœ“ TextTokenizeråˆ›å»ºå®Œæˆ")
        print(">> bpe model loaded from:", self.bpe_path)

        print(f"[IndexTTS2] å¼€å§‹åŠ è½½æƒ…æ„ŸçŸ©é˜µ: {self.cfg.emo_matrix}")
        emo_matrix = torch.load(os.path.join(self.model_dir, self.cfg.emo_matrix))
        self.emo_matrix = emo_matrix.to(self.device)
        self.emo_num = list(self.cfg.emo_num)
        print("[IndexTTS2] âœ“ æƒ…æ„ŸçŸ©é˜µåŠ è½½å®Œæˆ")

        print(f"[IndexTTS2] å¼€å§‹åŠ è½½è¯´è¯äººçŸ©é˜µ: {self.cfg.spk_matrix}")
        spk_matrix = torch.load(os.path.join(self.model_dir, self.cfg.spk_matrix))
        self.spk_matrix = spk_matrix.to(self.device)
        print("[IndexTTS2] âœ“ è¯´è¯äººçŸ©é˜µåŠ è½½å®Œæˆ")

        self.emo_matrix = torch.split(self.emo_matrix, self.emo_num)
        self.spk_matrix = torch.split(self.spk_matrix, self.emo_num)

        # åå¤‡mel_fnåˆå§‹åŒ–ï¼ˆå¦‚æœå‰é¢å¤±è´¥äº†ï¼‰
        if self.mel_fn is None:
            try:
                from indextts.s2mel.modules.audio import mel_spectrogram
                mel_fn_args = {
                    "n_fft": self.cfg.s2mel['preprocess_params']['spect_params']['n_fft'],
                    "win_size": self.cfg.s2mel['preprocess_params']['spect_params']['win_length'],
                    "hop_size": self.cfg.s2mel['preprocess_params']['spect_params']['hop_length'],
                    "num_mels": self.cfg.s2mel['preprocess_params']['spect_params']['n_mels'],
                    "sampling_rate": self.cfg.s2mel["preprocess_params"]["sr"],
                    "fmin": self.cfg.s2mel['preprocess_params']['spect_params'].get('fmin', 0),
                    "fmax": None if self.cfg.s2mel['preprocess_params']['spect_params'].get('fmax', "None") == "None" else 8000,
                    "center": False
                }
                self.mel_fn = lambda x: mel_spectrogram(x, **mel_fn_args)
                print("[IndexTTS2] âœ“ mel_fnåå¤‡åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"[ERROR] mel_fnåå¤‡åˆå§‹åŒ–ä¹Ÿå¤±è´¥: {e}")
                raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–mel_fnå‡½æ•°: {e}")

        print("[IndexTTS2] ğŸ‰ IndexTTS2åˆå§‹åŒ–å®Œæˆï¼æ‰€æœ‰å±æ€§å’Œæ¨¡å‹å·²å°±ç»ª")

    @torch.no_grad()
    def get_emb(self, input_features, attention_mask):
        vq_emb = self.semantic_model(
            input_features=input_features,
            attention_mask=attention_mask,
            output_hidden_states=True,
        )
        feat = vq_emb.hidden_states[17]  # (B, T, C)
        feat = (feat - self.semantic_mean) / self.semantic_std
        return feat

    def remove_long_silence(self, codes: torch.Tensor, silent_token=52, max_consecutive=30):
        """
        Shrink special tokens (silent_token and stop_mel_token) in codes
        codes: [B, T]
        """
        code_lens = []
        codes_list = []
        device = codes.device
        dtype = codes.dtype
        isfix = False
        for i in range(0, codes.shape[0]):
            code = codes[i]
            if not torch.any(code == self.stop_mel_token).item():
                len_ = code.size(0)
            else:
                stop_mel_idx = (code == self.stop_mel_token).nonzero(as_tuple=False)
                len_ = stop_mel_idx[0].item() if len(stop_mel_idx) > 0 else code.size(0)

            count = torch.sum(code == silent_token).item()
            if count > max_consecutive:
                # code = code.cpu().tolist()
                ncode_idx = []
                n = 0
                for k in range(len_):
                    assert code[
                               k] != self.stop_mel_token, f"stop_mel_token {self.stop_mel_token} should be shrinked here"
                    if code[k] != silent_token:
                        ncode_idx.append(k)
                        n = 0
                    elif code[k] == silent_token and n < 10:
                        ncode_idx.append(k)
                        n += 1
                    # if (k == 0 and code[k] == 52) or (code[k] == 52 and code[k-1] == 52):
                    #    n += 1
                # new code
                len_ = len(ncode_idx)
                codes_list.append(code[ncode_idx])
                isfix = True
            else:
                # shrink to len_
                codes_list.append(code[:len_])
            code_lens.append(len_)
        if isfix:
            if len(codes_list) > 1:
                codes = pad_sequence(codes_list, batch_first=True, padding_value=self.stop_mel_token)
            else:
                codes = codes_list[0].unsqueeze(0)
        else:
            # unchanged
            pass
        # clip codes to max length
        max_len = max(code_lens)
        if max_len < codes.shape[1]:
            codes = codes[:, :max_len]
        code_lens = torch.tensor(code_lens, dtype=torch.long, device=device)
        return codes, code_lens

    def insert_interval_silence(self, wavs, sampling_rate=22050, interval_silence=200):
        """
        Insert silences between sentences.
        wavs: List[torch.tensor]
        """

        if not wavs or interval_silence <= 0:
            return wavs

        # get channel_size
        channel_size = wavs[0].size(0)
        # get silence tensor
        sil_dur = int(sampling_rate * interval_silence / 1000.0)
        sil_tensor = torch.zeros(channel_size, sil_dur)

        wavs_list = []
        for i, wav in enumerate(wavs):
            wavs_list.append(wav)
            if i < len(wavs) - 1:
                wavs_list.append(sil_tensor)

        return wavs_list

    def _set_gr_progress(self, value, desc):
        if self.gr_progress is not None:
            self.gr_progress(value, desc=desc)

    # åŸå§‹æ¨ç†æ¨¡å¼
    def infer(self, spk_audio_prompt, text, output_path,
              emo_audio_prompt=None, emo_alpha=1.0,
              emo_vector=None,
              use_emo_text=False, emo_text=None, use_random=False, interval_silence=200,
              verbose=False, max_text_tokens_per_sentence=120, **generation_kwargs):
        print(">> start inference...")
        self._set_gr_progress(0, "start inference...")
        if verbose:
            print(f"origin text:{text}, spk_audio_prompt:{spk_audio_prompt},"
                  f" emo_audio_prompt:{emo_audio_prompt}, emo_alpha:{emo_alpha}, "
                  f"emo_vector:{emo_vector}, use_emo_text:{use_emo_text}, "
                  f"emo_text:{emo_text}")
        start_time = time.perf_counter()

        if use_emo_text:
            emo_audio_prompt = None
            emo_alpha = 1.0
            # assert emo_audio_prompt is None
            # assert emo_alpha == 1.0
            if emo_text is None:
                emo_text = text

            if self.qwen_emo is not None:
                emo_dict, content = self.qwen_emo.inference(emo_text)
                print(emo_dict)
                emo_vector = list(emo_dict.values())
            else:
                print("âš ï¸  Emotion model not available, using default emotion vector")
                # ä½¿ç”¨é»˜è®¤çš„æƒ…æ„Ÿå‘é‡
                emo_vector = [0.5] * 8  # å‡è®¾æœ‰8ä¸ªæƒ…æ„Ÿç»´åº¦

        if emo_vector is not None:
            emo_audio_prompt = None
            emo_alpha = 1.0
            # assert emo_audio_prompt is None
            # assert emo_alpha == 1.0

        if emo_audio_prompt is None:
            emo_audio_prompt = spk_audio_prompt
            emo_alpha = 1.0
            # assert emo_alpha == 1.0

        # å¦‚æœå‚è€ƒéŸ³é¢‘æ”¹å˜äº†ï¼Œæ‰éœ€è¦é‡æ–°ç”Ÿæˆ, æå‡é€Ÿåº¦
        if self.cache_spk_cond is None or self.cache_spk_audio_prompt != spk_audio_prompt:
            audio, sr = librosa.load(spk_audio_prompt)
            audio = torch.tensor(audio).unsqueeze(0)
            audio_22k = torchaudio.transforms.Resample(sr, 22050)(audio)
            audio_16k = torchaudio.transforms.Resample(sr, 16000)(audio)

            inputs = self.extract_features(audio_16k, sampling_rate=16000, return_tensors="pt")
            input_features = inputs["input_features"]
            attention_mask = inputs["attention_mask"]
            input_features = input_features.to(self.device)
            attention_mask = attention_mask.to(self.device)
            spk_cond_emb = self.get_emb(input_features, attention_mask)

            _, S_ref = self.semantic_codec.quantize(spk_cond_emb)
            ref_mel = self.mel_fn(audio_22k.to(spk_cond_emb.device).float())
            ref_target_lengths = torch.LongTensor([ref_mel.size(2)]).to(ref_mel.device)
            feat = torchaudio.compliance.kaldi.fbank(audio_16k.to(ref_mel.device),
                                                     num_mel_bins=80,
                                                     dither=0,
                                                     sample_frequency=16000)
            feat = feat - feat.mean(dim=0, keepdim=True)  # feat2å¦å¤–ä¸€ä¸ªæ»¤æ³¢å™¨èƒ½é‡ç»„ç‰¹å¾[922, 80]
            style = self.campplus_model(feat.unsqueeze(0))  # å‚è€ƒéŸ³é¢‘çš„å…¨å±€style2[1,192]

            prompt_condition = self.s2mel.models['length_regulator'](S_ref,
                                                                     ylens=ref_target_lengths,
                                                                     n_quantizers=3,
                                                                     f0=None)[0]

            self.cache_spk_cond = spk_cond_emb
            self.cache_s2mel_style = style
            self.cache_s2mel_prompt = prompt_condition
            self.cache_spk_audio_prompt = spk_audio_prompt
            self.cache_mel = ref_mel
        else:
            style = self.cache_s2mel_style
            prompt_condition = self.cache_s2mel_prompt
            spk_cond_emb = self.cache_spk_cond
            ref_mel = self.cache_mel

        if emo_vector is not None:
            print(f"[IndexTTS2] Processing emotion vector: {emo_vector}")
            weight_vector = torch.tensor(emo_vector).to(self.device)

            # éªŒè¯æƒ…æ„Ÿå‘é‡çš„æœ‰æ•ˆæ€§
            weight_sum = torch.sum(weight_vector)
            print(f"[IndexTTS2] Emotion vector sum: {weight_sum:.6f}")
            print(f"[IndexTTS2] Individual values: {[f'{v:.6f}' for v in emo_vector]}")

            if weight_sum <= 0.001:
                print("[IndexTTS2] Warning: emotion vector sum is near zero, using default neutral emotion")
                # è®¾ç½®é»˜è®¤çš„ä¸­æ€§æƒ…æ„Ÿ
                weight_vector = torch.zeros_like(weight_vector)
                weight_vector[7] = 0.2  # Neutral emotion
            elif weight_sum > 2.0:
                print(f"[IndexTTS2] Warning: emotion vector sum is {weight_sum:.3f}, normalizing")
                weight_vector = weight_vector / weight_sum * 1.0  # å½’ä¸€åŒ–åˆ°åˆç†èŒƒå›´

            print(f"[IndexTTS2] Final weight_vector: {weight_vector.tolist()}")

            if use_random:
                random_index = [random.randint(0, x - 1) for x in self.emo_num]
            else:
                random_index = [find_most_similar_cosine(style, tmp) for tmp in self.spk_matrix]

            # éªŒè¯ç´¢å¼•çš„æœ‰æ•ˆæ€§ï¼Œé˜²æ­¢ç´¢å¼•è¶…å‡ºèŒƒå›´
            validated_indices = []
            for i, (index, tmp, emo_dim_size) in enumerate(zip(random_index, self.emo_matrix, self.emo_num)):
                # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                if index >= tmp.shape[0]:
                    print(f"[IndexTTS2] Warning: emotion index {index} >= matrix size {tmp.shape[0]} for dimension {i}, using 0")
                    index = 0
                elif index < 0:
                    print(f"[IndexTTS2] Warning: emotion index {index} < 0 for dimension {i}, using 0")
                    index = 0
                validated_indices.append(index)

            try:
                emo_matrix = [tmp[index].unsqueeze(0) for index, tmp in zip(validated_indices, self.emo_matrix)]
                emo_matrix = torch.cat(emo_matrix, 0)
                emovec_mat = weight_vector.unsqueeze(1) * emo_matrix
                emovec_mat = torch.sum(emovec_mat, 0)
                emovec_mat = emovec_mat.unsqueeze(0)
            except Exception as e:
                print(f"[IndexTTS2] Error in emotion matrix processing: {e}")
                print(f"[IndexTTS2] weight_vector shape: {weight_vector.shape}")
                print(f"[IndexTTS2] validated_indices: {validated_indices}")
                print(f"[IndexTTS2] emo_matrix shapes: {[tmp.shape for tmp in self.emo_matrix]}")
                # åˆ›å»ºä¸€ä¸ªå®‰å…¨çš„é»˜è®¤æƒ…æ„ŸçŸ©é˜µ
                default_emovec = torch.zeros((1, self.emo_matrix[0].shape[1]), device=self.device)
                emovec_mat = default_emovec

        if self.cache_emo_cond is None or self.cache_emo_audio_prompt != emo_audio_prompt:
            emo_audio, _ = librosa.load(emo_audio_prompt, sr=16000)
            emo_inputs = self.extract_features(emo_audio, sampling_rate=16000, return_tensors="pt")
            emo_input_features = emo_inputs["input_features"]
            emo_attention_mask = emo_inputs["attention_mask"]
            emo_input_features = emo_input_features.to(self.device)
            emo_attention_mask = emo_attention_mask.to(self.device)
            emo_cond_emb = self.get_emb(emo_input_features, emo_attention_mask)

            self.cache_emo_cond = emo_cond_emb
            self.cache_emo_audio_prompt = emo_audio_prompt
        else:
            emo_cond_emb = self.cache_emo_cond

        self._set_gr_progress(0.1, "text processing...")
        text_tokens_list = self.tokenizer.tokenize(text)
        sentences = self.tokenizer.split_sentences(text_tokens_list, max_text_tokens_per_sentence)
        if verbose:
            print("text_tokens_list:", text_tokens_list)
            print("sentences count:", len(sentences))
            print("max_text_tokens_per_sentence:", max_text_tokens_per_sentence)
            print(*sentences, sep="\n")
        do_sample = generation_kwargs.pop("do_sample", True)
        top_p = generation_kwargs.pop("top_p", 0.8)
        top_k = generation_kwargs.pop("top_k", 30)
        temperature = generation_kwargs.pop("temperature", 0.8)
        autoregressive_batch_size = 1
        length_penalty = generation_kwargs.pop("length_penalty", 0.0)
        num_beams = generation_kwargs.pop("num_beams", 3)
        repetition_penalty = generation_kwargs.pop("repetition_penalty", 10.0)
        max_mel_tokens = generation_kwargs.pop("max_mel_tokens", 1500)
        sampling_rate = 22050

        wavs = []
        gpt_gen_time = 0
        gpt_forward_time = 0
        s2mel_time = 0
        bigvgan_time = 0
        progress = 0
        has_warned = False
        for sent in sentences:
            text_tokens = self.tokenizer.convert_tokens_to_ids(sent)
            text_tokens = torch.tensor(text_tokens, dtype=torch.int32, device=self.device).unsqueeze(0)
            if verbose:
                print(text_tokens)
                print(f"text_tokens shape: {text_tokens.shape}, text_tokens type: {text_tokens.dtype}")
                # debug tokenizer
                text_token_syms = self.tokenizer.convert_ids_to_tokens(text_tokens[0].tolist())
                print("text_token_syms is same as sentence tokens", text_token_syms == sent)

            m_start_time = time.perf_counter()
            with torch.no_grad():
                with torch.amp.autocast(text_tokens.device.type, enabled=self.dtype is not None, dtype=self.dtype):
                    emovec = self.gpt.merge_emovec(
                        spk_cond_emb,
                        emo_cond_emb,
                        torch.tensor([spk_cond_emb.shape[-1]], device=text_tokens.device),
                        torch.tensor([emo_cond_emb.shape[-1]], device=text_tokens.device),
                        alpha=emo_alpha
                    )

                    if emo_vector is not None:
                        # ç¡®ä¿æƒé‡å‘é‡çš„å’Œåœ¨åˆç†èŒƒå›´å†…
                        weight_sum = torch.sum(weight_vector)
                        weight_sum = torch.clamp(weight_sum, 0.0, 1.0)  # é™åˆ¶åœ¨[0,1]èŒƒå›´å†…

                        # æ··åˆæƒ…æ„Ÿå‘é‡å’ŒåŸå§‹å‘é‡
                        emovec = emovec_mat + (1 - weight_sum) * emovec
                        # emovec = emovec_mat

                    codes, speech_conditioning_latent = self.gpt.inference_speech(
                        spk_cond_emb,
                        text_tokens,
                        emo_cond_emb,
                        cond_lengths=torch.tensor([spk_cond_emb.shape[-1]], device=text_tokens.device),
                        emo_cond_lengths=torch.tensor([emo_cond_emb.shape[-1]], device=text_tokens.device),
                        emo_vec=emovec,
                        do_sample=True,
                        top_p=top_p,
                        top_k=top_k,
                        temperature=temperature,
                        num_return_sequences=autoregressive_batch_size,
                        length_penalty=length_penalty,
                        num_beams=num_beams,
                        repetition_penalty=repetition_penalty,
                        max_generate_length=max_mel_tokens,
                        **generation_kwargs
                    )

                gpt_gen_time += time.perf_counter() - m_start_time
                if not has_warned and (codes[:, -1] != self.stop_mel_token).any():
                    warnings.warn(
                        f"WARN: generation stopped due to exceeding `max_mel_tokens` ({max_mel_tokens}). "
                        f"Input text tokens: {text_tokens.shape[1]}. "
                        f"Consider reducing `max_text_tokens_per_sentence`({max_text_tokens_per_sentence}) or increasing `max_mel_tokens`.",
                        category=RuntimeWarning
                    )
                    has_warned = True

                code_lens = torch.tensor([codes.shape[-1]], device=codes.device, dtype=codes.dtype)
                #                 if verbose:
                #                     print(codes, type(codes))
                #                     print(f"codes shape: {codes.shape}, codes type: {codes.dtype}")
                #                     print(f"code len: {code_lens}")

                code_lens = []
                for code in codes:
                    if self.stop_mel_token not in code:
                        code_lens.append(len(code))
                        code_len = len(code)
                    else:
                        len_ = (code == self.stop_mel_token).nonzero(as_tuple=False)[0] + 1
                        code_len = len_ - 1
                    code_lens.append(code_len)
                codes = codes[:, :code_len]
                code_lens = torch.LongTensor(code_lens)
                code_lens = code_lens.to(self.device)
                if verbose:
                    print(codes, type(codes))
                    print(f"fix codes shape: {codes.shape}, codes type: {codes.dtype}")
                    print(f"code len: {code_lens}")

                m_start_time = time.perf_counter()
                use_speed = torch.zeros(spk_cond_emb.size(0)).to(spk_cond_emb.device).long()
                with torch.amp.autocast(text_tokens.device.type, enabled=self.dtype is not None, dtype=self.dtype):
                    latent = self.gpt(
                        speech_conditioning_latent,
                        text_tokens,
                        torch.tensor([text_tokens.shape[-1]], device=text_tokens.device),
                        codes,
                        torch.tensor([codes.shape[-1]], device=text_tokens.device),
                        emo_cond_emb,
                        cond_mel_lengths=torch.tensor([spk_cond_emb.shape[-1]], device=text_tokens.device),
                        emo_cond_mel_lengths=torch.tensor([emo_cond_emb.shape[-1]], device=text_tokens.device),
                        emo_vec=emovec,
                        use_speed=use_speed,
                    )
                    gpt_forward_time += time.perf_counter() - m_start_time

                dtype = None
                with torch.amp.autocast(text_tokens.device.type, enabled=dtype is not None, dtype=dtype):
                    m_start_time = time.perf_counter()
                    diffusion_steps = 25
                    inference_cfg_rate = 0.7
                    latent = self.s2mel.models['gpt_layer'](latent)
                    S_infer = self.semantic_codec.quantizer.vq2emb(codes.unsqueeze(1))
                    S_infer = S_infer.transpose(1, 2)
                    S_infer = S_infer + latent
                    target_lengths = (code_lens * 1.72).long()

                    cond = self.s2mel.models['length_regulator'](S_infer,
                                                                 ylens=target_lengths,
                                                                 n_quantizers=3,
                                                                 f0=None)[0]
                    cat_condition = torch.cat([prompt_condition, cond], dim=1)
                    vc_target = self.s2mel.models['cfm'].inference(cat_condition,
                                                                   torch.LongTensor([cat_condition.size(1)]).to(
                                                                       cond.device),
                                                                   ref_mel, style, None, diffusion_steps,
                                                                   inference_cfg_rate=inference_cfg_rate)
                    vc_target = vc_target[:, :, ref_mel.size(-1):]
                    s2mel_time += time.perf_counter() - m_start_time

                    m_start_time = time.perf_counter()
                    wav = self.bigvgan(vc_target.float()).squeeze().unsqueeze(0)
                    print(wav.shape)
                    bigvgan_time += time.perf_counter() - m_start_time
                    wav = wav.squeeze(1)

                wav = torch.clamp(32767 * wav, -32767.0, 32767.0)
                if verbose:
                    print(f"wav shape: {wav.shape}", "min:", wav.min(), "max:", wav.max())
                # wavs.append(wav[:, :-512])
                wavs.append(wav.cpu())  # to cpu before saving
        end_time = time.perf_counter()
        self._set_gr_progress(0.9, "save audio...")
        wavs = self.insert_interval_silence(wavs, sampling_rate=sampling_rate, interval_silence=interval_silence)
        wav = torch.cat(wavs, dim=1)
        wav_length = wav.shape[-1] / sampling_rate
        print(f">> gpt_gen_time: {gpt_gen_time:.2f} seconds")
        print(f">> gpt_forward_time: {gpt_forward_time:.2f} seconds")
        print(f">> s2mel_time: {s2mel_time:.2f} seconds")
        print(f">> bigvgan_time: {bigvgan_time:.2f} seconds")
        print(f">> Total inference time: {end_time - start_time:.2f} seconds")
        print(f">> Generated audio length: {wav_length:.2f} seconds")
        print(f">> RTF: {(end_time - start_time) / wav_length:.4f}")

        # save audio
        wav = wav.cpu()  # to cpu
        if output_path:
            # ç›´æ¥ä¿å­˜éŸ³é¢‘åˆ°æŒ‡å®šè·¯å¾„ä¸­
            if os.path.isfile(output_path):
                os.remove(output_path)
                print(">> remove old wav file:", output_path)
            if os.path.dirname(output_path) != "":
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
            torchaudio.save(output_path, wav.type(torch.int16), sampling_rate)
            print(">> wav file saved to:", output_path)
            return output_path
        else:
            # è¿”å›ä»¥ç¬¦åˆGradioçš„æ ¼å¼è¦æ±‚
            wav_data = wav.type(torch.int16)
            wav_data = wav_data.numpy().T
            return (sampling_rate, wav_data)


def find_most_similar_cosine(query_vector, matrix):
    try:
        query_vector = query_vector.float()
        matrix = matrix.float()

        # æ£€æŸ¥è¾“å…¥çš„æœ‰æ•ˆæ€§
        if matrix.shape[0] == 0:
            print("[IndexTTS2] Warning: empty matrix in find_most_similar_cosine, returning 0")
            return 0

        if torch.isnan(query_vector).any() or torch.isinf(query_vector).any():
            print("[IndexTTS2] Warning: invalid query_vector in find_most_similar_cosine, returning 0")
            return 0

        similarities = F.cosine_similarity(query_vector, matrix, dim=1)

        # æ£€æŸ¥ç›¸ä¼¼åº¦è®¡ç®—ç»“æœ
        if torch.isnan(similarities).any() or torch.isinf(similarities).any():
            print("[IndexTTS2] Warning: invalid similarities in find_most_similar_cosine, returning 0")
            return 0

        most_similar_index = torch.argmax(similarities)

        # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
        index_value = most_similar_index.item()
        if index_value >= matrix.shape[0]:
            print(f"[IndexTTS2] Warning: computed index {index_value} >= matrix size {matrix.shape[0]}, using 0")
            return 0

        return index_value

    except Exception as e:
        print(f"[IndexTTS2] Error in find_most_similar_cosine: {e}")
        return 0

class QwenEmotion:
    def __init__(self, model_dir):
        # é¦–å…ˆè®¾ç½®æ‰€æœ‰å¿…è¦çš„å±æ€§ï¼Œç¡®ä¿å³ä½¿åˆå§‹åŒ–å¤±è´¥ä¹Ÿä¸ä¼šå‡ºç°AttributeError
        self.model_dir = model_dir
        self.model = None
        self.tokenizer = None
        self.is_available = False

        # è®¾ç½®é»˜è®¤å±æ€§
        self._initialize_default_attributes()

        # æ™ºèƒ½åŠ è½½ç­–ç•¥ï¼šå…ˆæ£€æŸ¥transformersç‰ˆæœ¬å…¼å®¹æ€§
        # Smart loading strategy: check transformers version compatibility first
        print(f"[IndexTTS2] å°è¯•åŠ è½½Qwenæƒ…æ„Ÿæ¨¡å‹: {model_dir}")
        print(f"[IndexTTS2] Attempting to load Qwen emotion model: {model_dir}")

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡åˆå§‹æ¨¡å‹åŠ è½½
        should_skip_initial_load = self._should_skip_initial_model_load(model_dir)

        if should_skip_initial_load:
            print(f"[IndexTTS2] ğŸ”„ æ£€æµ‹åˆ°ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼Œç›´æ¥ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
            print(f"[IndexTTS2] ğŸ”„ Version compatibility issue detected, using fallback directly")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè€Œæ˜¯ç›´æ¥è·³åˆ°å¤‡ç”¨æ–¹æ¡ˆ
            self._handle_fallback_loading()
            return

        try:
            # ç›´æ¥å°è¯•åŠ è½½ï¼Œè®©transformersè‡ªå·±å¤„ç†å…¼å®¹æ€§
            if os.path.exists(model_dir):
                # æœ¬åœ°è·¯å¾„ï¼Œä½¿ç”¨local_files_only=True
                print(f"[IndexTTS2] ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹...")
                print(f"[IndexTTS2] Loading model from local path...")

                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_dir,
                    local_files_only=True,
                    trust_remote_code=True
                )
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_dir,
                    torch_dtype="float16",
                    device_map="auto",
                    local_files_only=True,
                    trust_remote_code=True
                )
            else:
                # è¿œç¨‹repoï¼Œæ­£å¸¸åŠ è½½
                print(f"[IndexTTS2] ä»è¿œç¨‹ä»“åº“åŠ è½½æ¨¡å‹...")
                print(f"[IndexTTS2] Loading model from remote repository...")

                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_dir,
                    trust_remote_code=True
                )
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_dir,
                    torch_dtype="float16",
                    device_map="auto",
                    trust_remote_code=True
                )

            self.is_available = True
            print(f"[IndexTTS2] âœ… Qwenæƒ…æ„Ÿæ¨¡å‹åŠ è½½æˆåŠŸï¼")
            print(f"[IndexTTS2] âœ… Qwen emotion model loaded successfully!")

        except Exception as e:
            # ä»»ä½•åŠ è½½å¤±è´¥éƒ½ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼Œä¸ç®¡å…·ä½“åŸå› 
            print(f"[IndexTTS2] âš ï¸  Qwenæƒ…æ„Ÿæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print(f"[IndexTTS2] âš ï¸  Failed to load Qwen emotion model: {e}")

            # æä¾›å…·ä½“çš„é”™è¯¯åˆ†æå’Œå»ºè®®
            self._analyze_loading_error(e)

            print(f"[IndexTTS2] ğŸ”„ å°†ä½¿ç”¨å¤‡ç”¨æƒ…æ„Ÿåˆ†ææ–¹æ³•")
            print(f"[IndexTTS2] ğŸ”„ Will use fallback emotion analysis method")

            # å°è¯•æ™ºèƒ½å¤‡ç”¨æ–¹æ¡ˆï¼šæ ¹æ®transformersç‰ˆæœ¬åŠ è½½å…¼å®¹çš„Qwenæ¨¡å‹
            print(f"[IndexTTS2] ğŸ”„ å°è¯•æ™ºèƒ½å¤‡ç”¨æ–¹æ¡ˆ...")
            print(f"[IndexTTS2] ğŸ”„ Trying intelligent fallback...")

            fallback_success = self._try_fallback_qwen_models()

            if not fallback_success:
                print(f"[IndexTTS2] ğŸ”„ æ‰€æœ‰Qwenæ¨¡å‹éƒ½æ— æ³•åŠ è½½ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…å¤‡ç”¨æ–¹æ¡ˆ")
                print(f"[IndexTTS2] ğŸ”„ All Qwen models failed to load, using keyword matching fallback")
                self.is_available = False
                self.model = None
                self.tokenizer = None

    def _initialize_default_attributes(self):
        """åˆå§‹åŒ–é»˜è®¤å±æ€§ï¼Œç¡®ä¿æ‰€æœ‰æ–¹æ³•éƒ½èƒ½æ­£å¸¸è°ƒç”¨"""
        # è®¾ç½®æƒ…æ„Ÿåˆ†æç›¸å…³çš„é»˜è®¤å±æ€§
        self.prompt = """ä½ æ˜¯ä¸€ä¸ªæƒ…æ„Ÿåˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿï¼Œå¹¶ç»™å‡º8ä¸ªç»´åº¦çš„æƒ…æ„Ÿåˆ†æ•°ï¼ˆ0-1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰ï¼š
        happyï¼ˆå¼€å¿ƒï¼‰ã€angryï¼ˆæ„¤æ€’ï¼‰ã€sadï¼ˆæ‚²ä¼¤ï¼‰ã€fearï¼ˆææƒ§ï¼‰ã€hateï¼ˆåŒæ¶ï¼‰ã€lowï¼ˆä½è½ï¼‰ã€surpriseï¼ˆæƒŠè®¶ï¼‰ã€neutralï¼ˆä¸­æ€§ï¼‰ã€‚

        è¯·ç›´æ¥è¿”å›JSONæ ¼å¼çš„ç»“æœï¼Œä¾‹å¦‚ï¼š
        {"happy": 0.8, "angry": 0.0, "sad": 0.1, "fear": 0.0, "hate": 0.0, "low": 0.0, "surprise": 0.1, "neutral": 0.0}

        æ–‡æœ¬ï¼š"""

        # è®¾ç½®å¤‡ç”¨æƒ…æ„Ÿå­—å…¸
        self.backup_dict = {
            "happy": 0, "angry": 0, "sad": 0, "fear": 0,
            "hate": 0, "low": 0, "surprise": 0, "neutral": 1.0
        }

        # è®¾ç½®åˆ†æ•°èŒƒå›´
        self.max_score = 1.2
        self.min_score = 0.0
        # è®¾ç½®è½¬æ¢å­—å…¸
        self.convert_dict = {
            "æ„¤æ€’": "angry",
            "é«˜å…´": "happy",
            "ææƒ§": "fear",
            "åæ„Ÿ": "hate",
            "æ‚²ä¼¤": "sad",
            "ä½è½": "low",
            "æƒŠè®¶": "surprise",
            "è‡ªç„¶": "neutral",
        }

    def _analyze_loading_error(self, error):
        """åˆ†æåŠ è½½é”™è¯¯å¹¶æä¾›å…·ä½“çš„è§£å†³å»ºè®®"""
        error_str = str(error).lower()

        if "qwen3" in error_str and "transformers does not recognize" in error_str:
            print(f"[IndexTTS2] ğŸ’¡ é”™è¯¯åˆ†æ: Qwen3æ¨¡å‹éœ€è¦æ›´æ–°çš„transformersç‰ˆæœ¬")
            print(f"[IndexTTS2] ğŸ’¡ Error analysis: Qwen3 model requires newer transformers version")
            print(f"[IndexTTS2] ğŸ”§ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
            print(f"[IndexTTS2] ğŸ”§ Suggested solutions:")
            print(f"[IndexTTS2]    1. å‡çº§transformers: pip install --upgrade transformers")
            print(f"[IndexTTS2]    2. æˆ–å®‰è£…å¼€å‘ç‰ˆæœ¬: pip install git+https://github.com/huggingface/transformers.git")
            print(f"[IndexTTS2]    3. å½“å‰å°†å°è¯•ä½¿ç”¨å…¼å®¹çš„å¤‡ç”¨æ¨¡å‹")
        elif "keyerror" in error_str:
            print(f"[IndexTTS2] ğŸ’¡ é”™è¯¯åˆ†æ: æ¨¡å‹æ¶æ„ä¸è¢«å½“å‰transformersç‰ˆæœ¬æ”¯æŒ")
            print(f"[IndexTTS2] ğŸ’¡ Error analysis: Model architecture not supported by current transformers version")
        elif "no module named" in error_str:
            print(f"[IndexTTS2] ğŸ’¡ é”™è¯¯åˆ†æ: ç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ…")
            print(f"[IndexTTS2] ğŸ’¡ Error analysis: Missing required dependencies")
        elif "out of memory" in error_str or "cuda out of memory" in error_str:
            print(f"[IndexTTS2] ğŸ’¡ é”™è¯¯åˆ†æ: GPUå†…å­˜ä¸è¶³")
            print(f"[IndexTTS2] ğŸ’¡ Error analysis: Insufficient GPU memory")
            print(f"[IndexTTS2] ğŸ”§ å»ºè®®: å°†å°è¯•ä½¿ç”¨æ›´å°çš„æ¨¡å‹")
        else:
            print(f"[IndexTTS2] ğŸ’¡ é”™è¯¯åˆ†æ: é€šç”¨åŠ è½½é”™è¯¯ï¼Œå°†å°è¯•å¤‡ç”¨æ–¹æ¡ˆ")
            print(f"[IndexTTS2] ğŸ’¡ Error analysis: General loading error, trying fallback options")

    def _should_skip_initial_model_load(self, model_dir):
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡åˆå§‹æ¨¡å‹åŠ è½½
        åŸºäºæ¨¡å‹è·¯å¾„å’Œtransformersç‰ˆæœ¬è¿›è¡Œæ™ºèƒ½åˆ¤æ–­
        """
        try:
            import transformers
            from packaging import version

            current_ver = version.parse(transformers.__version__)
            print(f"[IndexTTS2] æ£€æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§ - transformers: {transformers.__version__}")
            print(f"[IndexTTS2] Checking version compatibility - transformers: {transformers.__version__}")

            # æ£€æŸ¥æ¨¡å‹è·¯å¾„ä¸­æ˜¯å¦åŒ…å«å·²çŸ¥çš„ç‰ˆæœ¬æ•æ„Ÿå…³é”®è¯
            model_path_lower = model_dir.lower()

            # Qwen3ç›¸å…³æ¨¡å‹éœ€è¦transformers >= 4.51.0
            if any(keyword in model_path_lower for keyword in ['qwen3', 'qwen-3', 'qwen_3']):
                if current_ver < version.parse("4.51.0"):
                    print(f"[IndexTTS2] âš ï¸  æ£€æµ‹åˆ°Qwen3æ¨¡å‹ï¼Œä½†transformersç‰ˆæœ¬ {transformers.__version__} < 4.51.0")
                    print(f"[IndexTTS2] âš ï¸  Detected Qwen3 model, but transformers version {transformers.__version__} < 4.51.0")
                    return True

            # æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„ç‰¹å®šæ¨¡å‹åç§°
            if 'qwen0.6bemo4-merge' in model_path_lower:
                # è¿™ä¸ªæ¨¡å‹å¾ˆå¯èƒ½æ˜¯Qwen3æ¶æ„ï¼Œéœ€è¦æ›´æ–°çš„transformers
                # å¯¹äº4.49.0+ç‰ˆæœ¬ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•åŠ è½½ï¼Œä½†ä»ç„¶å‡†å¤‡å¤‡ç”¨æ–¹æ¡ˆ
                if current_ver < version.parse("4.49.0"):
                    print(f"[IndexTTS2] âš ï¸  æ£€æµ‹åˆ°qwen0.6bemo4-mergeæ¨¡å‹ï¼Œtransformersç‰ˆæœ¬ {transformers.__version__} å¯èƒ½ä¸å…¼å®¹")
                    print(f"[IndexTTS2] âš ï¸  Detected qwen0.6bemo4-merge model, transformers version {transformers.__version__} may not be compatible")
                    return True
                else:
                    print(f"[IndexTTS2] ğŸ’¡ transformersç‰ˆæœ¬ {transformers.__version__} >= 4.49.0ï¼Œå°è¯•åŠ è½½qwen0.6bemo4-mergeæ¨¡å‹")
                    print(f"[IndexTTS2] ğŸ’¡ transformers version {transformers.__version__} >= 4.49.0, attempting to load qwen0.6bemo4-merge model")

            return False

        except Exception as e:
            print(f"[IndexTTS2] âš ï¸  ç‰ˆæœ¬å…¼å®¹æ€§æ£€æŸ¥å¤±è´¥: {e}")
            print(f"[IndexTTS2] âš ï¸  Version compatibility check failed: {e}")
            return False

    def _handle_fallback_loading(self):
        """å¤„ç†å¤‡ç”¨åŠ è½½é€»è¾‘"""
        print(f"[IndexTTS2] ğŸ”„ å°†ä½¿ç”¨å¤‡ç”¨æƒ…æ„Ÿåˆ†ææ–¹æ³•")
        print(f"[IndexTTS2] ğŸ”„ Will use fallback emotion analysis method")

        # å°è¯•æ™ºèƒ½å¤‡ç”¨æ–¹æ¡ˆï¼šæ ¹æ®transformersç‰ˆæœ¬åŠ è½½å…¼å®¹çš„Qwenæ¨¡å‹
        print(f"[IndexTTS2] ğŸ”„ å°è¯•æ™ºèƒ½å¤‡ç”¨æ–¹æ¡ˆ...")
        print(f"[IndexTTS2] ğŸ”„ Trying intelligent fallback...")

        fallback_success = self._try_fallback_qwen_models()

        if not fallback_success:
            print(f"[IndexTTS2] ğŸ”„ æ‰€æœ‰Qwenæ¨¡å‹éƒ½æ— æ³•åŠ è½½ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…å¤‡ç”¨æ–¹æ¡ˆ")
            print(f"[IndexTTS2] ğŸ”„ All Qwen models failed to load, using keyword matching fallback")
            self.is_available = False
            self.model = None
            self.tokenizer = None

    def _get_compatible_qwen_models(self):
        """æ ¹æ®transformersç‰ˆæœ¬è·å–å…¼å®¹çš„Qwenæ¨¡å‹åˆ—è¡¨"""
        try:
            import transformers
            from packaging import version

            current_ver = version.parse(transformers.__version__)
            print(f"[IndexTTS2] æ£€æµ‹transformersç‰ˆæœ¬: {transformers.__version__}")
            print(f"[IndexTTS2] Detecting transformers version: {transformers.__version__}")

            # å®šä¹‰ä¸åŒQwenæ¨¡å‹çš„ç‰ˆæœ¬è¦æ±‚å’Œä¼˜å…ˆçº§
            qwen_models = []

            # Qwen3ç³»åˆ— (éœ€è¦transformers >= 4.51.0)
            if current_ver >= version.parse("4.51.0"):
                qwen_models.extend([
                    {
                        "name": "Qwen3-0.5B-Instruct",
                        "model_id": "Qwen/Qwen3-0.5B-Instruct",
                        "priority": 1,
                        "size": "0.5B",
                        "description": "æœ€æ–°Qwen3æ¨¡å‹ï¼Œå°å‹é«˜æ•ˆ"
                    },
                    {
                        "name": "Qwen3-1.8B-Instruct",
                        "model_id": "Qwen/Qwen3-1.8B-Instruct",
                        "priority": 2,
                        "size": "1.8B",
                        "description": "Qwen3ä¸­å‹æ¨¡å‹"
                    }
                ])

            # Qwen2.5ç³»åˆ— (éœ€è¦transformers >= 4.37.0)
            if current_ver >= version.parse("4.37.0"):
                qwen_models.extend([
                    {
                        "name": "Qwen2.5-0.5B-Instruct",
                        "model_id": "Qwen/Qwen2.5-0.5B-Instruct",
                        "priority": 3,
                        "size": "0.5B",
                        "description": "Qwen2.5å°å‹æ¨¡å‹"
                    },
                    {
                        "name": "Qwen2.5-1.5B-Instruct",
                        "model_id": "Qwen/Qwen2.5-1.5B-Instruct",
                        "priority": 4,
                        "size": "1.5B",
                        "description": "Qwen2.5ä¸­å‹æ¨¡å‹"
                    }
                ])

            # Qwen2ç³»åˆ— (éœ€è¦transformers >= 4.37.0)
            if current_ver >= version.parse("4.37.0"):
                qwen_models.extend([
                    {
                        "name": "Qwen2-0.5B-Instruct",
                        "model_id": "Qwen/Qwen2-0.5B-Instruct",
                        "priority": 5,
                        "size": "0.5B",
                        "description": "Qwen2å°å‹æ¨¡å‹"
                    },
                    {
                        "name": "Qwen2-1.5B-Instruct",
                        "model_id": "Qwen/Qwen2-1.5B-Instruct",
                        "priority": 6,
                        "size": "1.5B",
                        "description": "Qwen2ä¸­å‹æ¨¡å‹"
                    }
                ])

            # Qwen1.5ç³»åˆ— (éœ€è¦transformers >= 4.37.0)
            if current_ver >= version.parse("4.37.0"):
                qwen_models.extend([
                    {
                        "name": "Qwen1.5-0.5B-Chat",
                        "model_id": "Qwen/Qwen1.5-0.5B-Chat",
                        "priority": 7,
                        "size": "0.5B",
                        "description": "Qwen1.5å°å‹æ¨¡å‹"
                    },
                    {
                        "name": "Qwen1.5-1.8B-Chat",
                        "model_id": "Qwen/Qwen1.5-1.8B-Chat",
                        "priority": 8,
                        "size": "1.8B",
                        "description": "Qwen1.5ä¸­å‹æ¨¡å‹"
                    }
                ])

            # æŒ‰ä¼˜å…ˆçº§æ’åº
            qwen_models.sort(key=lambda x: x["priority"])

            print(f"[IndexTTS2] æ‰¾åˆ° {len(qwen_models)} ä¸ªå…¼å®¹çš„Qwenæ¨¡å‹")
            print(f"[IndexTTS2] Found {len(qwen_models)} compatible Qwen models")

            return qwen_models

        except Exception as e:
            print(f"[IndexTTS2] âš ï¸  è·å–å…¼å®¹æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            print(f"[IndexTTS2] âš ï¸  Failed to get compatible model list: {e}")
            return []

    def _try_fallback_qwen_models(self):
        """å°è¯•åŠ è½½å¤‡ç”¨Qwenæ¨¡å‹"""
        compatible_models = self._get_compatible_qwen_models()

        if not compatible_models:
            print(f"[IndexTTS2] âš ï¸  æ²¡æœ‰æ‰¾åˆ°å…¼å®¹çš„Qwenæ¨¡å‹")
            print(f"[IndexTTS2] âš ï¸  No compatible Qwen models found")
            return False

        for model_info in compatible_models:
            try:
                print(f"[IndexTTS2] ğŸ”„ å°è¯•åŠ è½½å¤‡ç”¨æ¨¡å‹: {model_info['name']} ({model_info['size']})")
                print(f"[IndexTTS2] ğŸ”„ Trying fallback model: {model_info['name']} ({model_info['size']})")
                print(f"[IndexTTS2] ğŸ“ æ¨¡å‹æè¿°: {model_info['description']}")
                print(f"[IndexTTS2] ğŸ“ Model description: {model_info['description']}")

                # å°è¯•åŠ è½½å¤‡ç”¨æ¨¡å‹
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_info['model_id'],
                    trust_remote_code=True
                )

                self.model = AutoModelForCausalLM.from_pretrained(
                    model_info['model_id'],
                    torch_dtype="float16",
                    device_map="auto",
                    trust_remote_code=True
                )

                self.is_available = True
                self.fallback_model_info = model_info

                print(f"[IndexTTS2] âœ… å¤‡ç”¨æ¨¡å‹åŠ è½½æˆåŠŸ: {model_info['name']}")
                print(f"[IndexTTS2] âœ… Fallback model loaded successfully: {model_info['name']}")
                print(f"[IndexTTS2] ğŸ’¡ ä½¿ç”¨ {model_info['size']} å‚æ•°çš„ {model_info['name']} è¿›è¡Œæƒ…æ„Ÿåˆ†æ")
                print(f"[IndexTTS2] ğŸ’¡ Using {model_info['size']} parameter {model_info['name']} for emotion analysis")

                return True

            except Exception as e:
                print(f"[IndexTTS2] âš ï¸  å¤‡ç”¨æ¨¡å‹ {model_info['name']} åŠ è½½å¤±è´¥: {e}")
                print(f"[IndexTTS2] âš ï¸  Fallback model {model_info['name']} failed to load: {e}")
                continue

        print(f"[IndexTTS2] âŒ æ‰€æœ‰å¤‡ç”¨Qwenæ¨¡å‹éƒ½åŠ è½½å¤±è´¥")
        print(f"[IndexTTS2] âŒ All fallback Qwen models failed to load")
        return False

    def convert(self, content):
        content = content.replace("\n", " ")
        content = content.replace(" ", "")
        content = content.replace("{", "")
        content = content.replace("}", "")
        content = content.replace('"', "")
        parts = content.strip().split(',')
        print(parts)
        parts_dict = {}
        desired_order = ["é«˜å…´", "æ„¤æ€’", "æ‚²ä¼¤", "ææƒ§", "åæ„Ÿ", "ä½è½", "æƒŠè®¶", "è‡ªç„¶"]
        for part in parts:
            key_value = part.strip().split(':')
            if len(key_value) == 2:
                parts_dict[key_value[0].strip()] = part
        # æŒ‰ç…§æœŸæœ›é¡ºåºé‡æ–°æ’åˆ—
        ordered_parts = [parts_dict[key] for key in desired_order if key in parts_dict]
        parts = ordered_parts
        if len(parts) != len(self.convert_dict):
            return self.backup_dict

        emotion_dict = {}
        for part in parts:
            key_value = part.strip().split(':')
            if len(key_value) == 2:
                try:
                    key = self.convert_dict[key_value[0].strip()]
                    value = float(key_value[1].strip())
                    value = max(self.min_score, min(self.max_score, value))
                    emotion_dict[key] = value
                except Exception:
                    continue

        for key in self.backup_dict:
            if key not in emotion_dict:
                emotion_dict[key] = 0.0

        if sum(emotion_dict.values()) <= 0:
            return self.backup_dict

        return emotion_dict

    def inference(self, text_input):
        """
        è¿›è¡Œæƒ…æ„Ÿæ¨ç†
        å¦‚æœæ¨¡å‹ä¸å¯ç”¨ï¼Œè¿”å›å¤‡ç”¨æƒ…æ„Ÿå­—å…¸
        """
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
        if not self.is_available or self.model is None or self.tokenizer is None:
            print(f"[IndexTTS2] âš ï¸  Qwen emotion model not available, using keyword-based fallback")
            print(f"[IndexTTS2] âš ï¸  Qwenæƒ…æ„Ÿæ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…å¤‡ç”¨æ–¹æ¡ˆ")

            # ä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
            fallback_emotion = self._fallback_emotion_analysis(text_input)
            return fallback_emotion, f"Keyword fallback for: {text_input[:50]}..."

        # æ˜¾ç¤ºä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯
        if hasattr(self, 'fallback_model_info'):
            model_info = self.fallback_model_info
            print(f"[IndexTTS2] ğŸ¤– ä½¿ç”¨å¤‡ç”¨æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†æ: {model_info['name']} ({model_info['size']})")
            print(f"[IndexTTS2] ğŸ¤– Using fallback model for emotion analysis: {model_info['name']} ({model_info['size']})")
        else:
            print(f"[IndexTTS2] ğŸ¤– ä½¿ç”¨åŸå§‹Qwenæ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†æ")
            print(f"[IndexTTS2] ğŸ¤– Using original Qwen model for emotion analysis")

        try:
            start = time.time()
            messages = [
                {"role": "system", "content": f"{self.prompt}"},
                {"role": "user", "content": f"{text_input}"}
            ]
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=False,
            )
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

            # conduct text completion
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=32768,
                pad_token_id=self.tokenizer.eos_token_id
            )
            output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()

            # parsing thinking content
            try:
                # rindex finding 151668 (</think>)
                index = len(output_ids) - output_ids[::-1].index(151668)
            except ValueError:
                index = 0

            content = self.tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")
            emotion_dict = self.convert(content)
            return emotion_dict, content

        except Exception as e:
            print(f"[IndexTTS2] âš ï¸  Qwen emotion inference failed: {e}")
            print(f"[IndexTTS2] âš ï¸  Qwenæƒ…æ„Ÿæ¨ç†å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨åˆ†æ")

            # å‘ç”Ÿé”™è¯¯æ—¶ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
            fallback_emotion = self._fallback_emotion_analysis(text_input)
            return fallback_emotion, f"Error fallback for: {text_input[:50]}..."

    def _fallback_emotion_analysis(self, text_input):
        """
        å¢å¼ºçš„å¤‡ç”¨æƒ…æ„Ÿåˆ†ææ–¹æ³•
        ä½¿ç”¨æ›´æ™ºèƒ½çš„å…³é”®è¯åŒ¹é…å’Œè¯­ä¹‰åˆ†ææ¥åˆ†ææƒ…æ„Ÿ
        Enhanced fallback emotion analysis method using smarter keyword matching and semantic analysis
        """
        print(f"[IndexTTS2] ğŸ” ä½¿ç”¨å¢å¼ºå…³é”®è¯åŒ¹é…è¿›è¡Œæƒ…æ„Ÿåˆ†æ")
        print(f"[IndexTTS2] ğŸ” Using enhanced keyword matching for emotion analysis")

        text_lower = text_input.lower()

        # å®šä¹‰æ›´å…¨é¢çš„æƒ…æ„Ÿå…³é”®è¯åº“ï¼ŒåŒ…å«æƒé‡
        emotion_keywords = {
            "happy": {
                "high": ["å¤ªå¥½äº†", "è¶…å¼€å¿ƒ", "éå¸¸é«˜å…´", "ç‰¹åˆ«å…´å¥‹", "ç‹‚æ¬¢", "æ¬£å–œè‹¥ç‹‚"],
                "medium": ["å¼€å¿ƒ", "é«˜å…´", "å¿«ä¹", "å…´å¥‹", "æ„‰å¿«", "æ¬¢ä¹", "å–œæ‚¦", "å¥½æ£’", "æ£’æäº†"],
                "low": ["å“ˆå“ˆ", "ç¬‘", "å‘µå‘µ", "å˜¿å˜¿", "ä¸é”™", "æŒºå¥½"]
            },
            "angry": {
                "high": ["æ°”æ­»äº†", "æ„¤æ€’è‡³æ", "ç«å†’ä¸‰ä¸ˆ", "æš´æ€’", "ç‹‚æ€’"],
                "medium": ["ç”Ÿæ°”", "æ„¤æ€’", "æ°”æ„¤", "æ¼ç«", "çƒ¦èº", "æ„¤æ…¨", "ç«å¤§"],
                "low": ["è®¨åŒ", "çƒ¦", "æ€’", "ä¸çˆ½", "éƒé—·"]
            },
            "sad": {
                "high": ["å¿ƒç—›", "ç—›ä¸æ¬²ç”Ÿ", "æ‚²ç—›æ¬²ç»", "ç»æœ›", "å´©æºƒ"],
                "medium": ["ä¼¤å¿ƒ", "éš¾è¿‡", "æ‚²ä¼¤", "æ²®ä¸§", "å¤±æœ›", "ç—›è‹¦", "éš¾å—"],
                "low": ["å“­", "çœ¼æ³ª", "å”‰", "å¯æƒœ", "é—æ†¾"]
            },
            "fear": {
                "high": ["ææ€–", "æƒŠæ…Œå¤±æª", "å“æ­»äº†", "ææƒ§è‡³æ"],
                "medium": ["å®³æ€•", "ææƒ§", "æ‹…å¿ƒ", "ç´§å¼ ", "ç„¦è™‘", "ä¸å®‰", "æƒŠæ…Œ"],
                "low": ["å¯æ€•", "å“", "æ‹…å¿§", "å¿§è™‘", "ä¸æ”¾å¿ƒ"]
            },
            "hate": {
                "high": ["æ†æ¨", "åŒæ¶è‡³æ", "æ·±æ¶ç—›ç»", "æ¨æ­»äº†"],
                "medium": ["è®¨åŒ", "åŒæ¶", "åæ„Ÿ", "æ¶å¿ƒ", "å«Œå¼ƒ", "å—ä¸äº†"],
                "low": ["çƒ¦äºº", "ä¸å–œæ¬¢", "åå¯¹", "æ‹’ç»"]
            },
            "low": {
                "high": ["æ¶ˆæ²‰", "é¢“åºŸ", "ç»æœ›", "æ— åŠ©", "ç©ºè™š"],
                "medium": ["ä½è½", "éƒé—·", "æ— èŠ", "ç–²æƒ«", "æ²¡åŠ²", "æ— åŠ›"],
                "low": ["ç´¯", "æ‡’", "å›°", "å€¦", "ä¹"]
            },
            "surprise": {
                "high": ["éœ‡æƒŠ", "æƒŠå‘†äº†", "ä¸æ•¢ç›¸ä¿¡", "å¤ªæ„å¤–äº†"],
                "medium": ["æƒŠè®¶", "æ„å¤–", "åƒæƒŠ", "æƒŠå¥‡", "æƒ³ä¸åˆ°"],
                "low": ["å¤©å“ª", "å“‡", "çœŸçš„å—", "æ˜¯å—", "å’¦"]
            },
            "neutral": {
                "high": ["æ˜ç™½äº†", "äº†è§£äº†", "çŸ¥é“äº†"],
                "medium": ["å¥½çš„", "æ˜ç™½", "äº†è§£", "æ˜¯çš„", "å¯¹"],
                "low": ["å—¯", "å“¦", "è¿™æ ·", "é‚£æ ·", "å¥½å§"]
            }
        }

        # æƒé‡è®¾ç½®
        weight_map = {"high": 3.0, "medium": 2.0, "low": 1.0}

        # è®¡ç®—æ¯ç§æƒ…æ„Ÿçš„åŠ æƒåŒ¹é…åˆ†æ•°
        emotion_scores = {}
        matched_keywords = {}

        for emotion, levels in emotion_keywords.items():
            score = 0
            matches = []
            for level, keywords in levels.items():
                weight = weight_map[level]
                for keyword in keywords:
                    if keyword in text_lower:
                        score += weight
                        matches.append(f"{keyword}({level})")
            emotion_scores[emotion] = score
            if matches:
                matched_keywords[emotion] = matches

        # æ˜¾ç¤ºåŒ¹é…çš„å…³é”®è¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if matched_keywords:
            print(f"[IndexTTS2] ğŸ” åŒ¹é…çš„æƒ…æ„Ÿå…³é”®è¯: {matched_keywords}")
            print(f"[IndexTTS2] ğŸ” Matched emotion keywords: {matched_keywords}")

        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•å…³é”®è¯ï¼Œè¿”å›ä¸­æ€§æƒ…æ„Ÿ
        if sum(emotion_scores.values()) == 0:
            return self.backup_dict.copy()

        # å½’ä¸€åŒ–åˆ†æ•°
        total_score = sum(emotion_scores.values())
        normalized_scores = {}
        for emotion, score in emotion_scores.items():
            if total_score > 0:
                normalized_scores[emotion] = min(self.max_score, (score / total_score) * 1.0)
            else:
                normalized_scores[emotion] = 0.0

        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæƒ…æ„Ÿæœ‰åˆ†æ•°
        if sum(normalized_scores.values()) == 0:
            normalized_scores["neutral"] = 0.8

        return normalized_scores


if __name__ == "__main__":
    prompt_wav = "examples/voice_01.wav"
    text = 'æ¬¢è¿å¤§å®¶æ¥ä½“éªŒindextts2ï¼Œå¹¶ç»™äºˆæˆ‘ä»¬æ„è§ä¸åé¦ˆï¼Œè°¢è°¢å¤§å®¶ã€‚'

    tts = IndexTTS2(cfg_path="checkpoints/config.yaml", model_dir="checkpoints", use_cuda_kernel=False)
    tts.infer(spk_audio_prompt=prompt_wav, text=text, output_path="gen.wav", verbose=True)

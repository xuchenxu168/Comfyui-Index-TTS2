# IndexTTS2 Emotion Voice Multi-Talk Node
# IndexTTS2 æƒ…ç»ªè¯­éŸ³å¤šäººå¯¹è¯èŠ‚ç‚¹

import os
import torch
import numpy as np
import tempfile
import torchaudio
from typing import Optional, Tuple, Any, List, Dict
import folder_paths

# å¯¼å…¥éŸ³é¢‘æ–‡ä»¶è·å–å‡½æ•°
from .basic_tts_node import get_audio_files

# å¯¼å…¥é«˜çº§éŸ³é¢‘æµè§ˆå™¨
try:
    from ..audio_browser import get_all_audio_files, clear_audio_cache
    AUDIO_BROWSER_AVAILABLE = True
except ImportError:
    AUDIO_BROWSER_AVAILABLE = False

# å¯¼å…¥ç›®å½•éŸ³é¢‘æµè§ˆå™¨
try:
    from ..directory_audio_browser import (
        get_audio_directory_choices,
        get_audio_file_choices,
        get_full_audio_path
    )
    DIRECTORY_BROWSER_AVAILABLE = True
except ImportError:
    DIRECTORY_BROWSER_AVAILABLE = False

class IndexTTS2EmotionVoiceMultiTalkNode:
    """
    IndexTTS2 æƒ…ç»ªè¯­éŸ³å¤šäººå¯¹è¯èŠ‚ç‚¹
    Emotion Voice Multi-Speaker Conversation Node for IndexTTS2
    
    Features:
    - Support 2-4 speakers conversation
    - Individual speaker voice cloning
    - Emotion control via emotion voice samples for each speaker
    - Multiple emotion modes (emotion_voice, emotion_vector, auto)
    - Automatic conversation flow with emotional expressions
    - Configurable silence intervals
    - High-quality multi-speaker synthesis with emotion voice control
    """

    def __init__(self):
        self.model = None
        self.model_config = None
        
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "num_speakers": (["1", "2", "3", "4"], {
                    "default": "2",
                    "tooltip": "å¯¹è¯äººæ•° / Number of speakers (1-4, 1=çº¯è¯­éŸ³å…‹éš†)"
                }),
                "conversation_text": ("STRING", {
                    "multiline": True,
                    "default": "Speaker1: [Happy] Hello, how are you today!\nSpeaker2: [Excited] I'm doing great, thank you for asking!",
                    "placeholder": "å•äººæ¨¡å¼ï¼šç›´æ¥è¾“å…¥æ–‡æœ¬\\nå¤šäººæ¨¡å¼ï¼šSpeaker1: [æƒ…ç»ª] æ–‡æœ¬\\nSpeaker2: [æƒ…ç»ª] æ–‡æœ¬..."
                }),
                "speaker1_voice": ("AUDIO", {
                    "tooltip": "è¯´è¯äºº1çš„éŸ³è‰²æ ·æœ¬ / Speaker 1 voice sample"
                }),
                "output_filename": ("STRING", {
                    "default": "emotion_voice_conversation.wav",
                    "placeholder": "è¾“å‡ºéŸ³é¢‘æ–‡ä»¶å / Output audio filename"
                }),
            },
            "optional": {
                "speaker2_voice": ("AUDIO", {
                    "tooltip": "è¯´è¯äºº2çš„éŸ³è‰²æ ·æœ¬ / Speaker 2 voice sample (å¤šäººæ¨¡å¼å¿…éœ€)"
                }),
                "speaker3_voice": ("AUDIO", {
                    "tooltip": "è¯´è¯äºº3çš„éŸ³è‰²æ ·æœ¬ / Speaker 3 voice sample"
                }),
                "speaker4_voice": ("AUDIO", {
                    "tooltip": "è¯´è¯äºº4çš„éŸ³è‰²æ ·æœ¬ / Speaker 4 voice sample"
                }),
                "model_manager": ("INDEXTTS2_MODEL",),
                "speaker1_emotion_voice": ("AUDIO", {
                    "tooltip": "è¯´è¯äºº1çš„æƒ…ç»ªè¯­éŸ³æ ·æœ¬ / Speaker 1 emotion voice sample"
                }),
                "speaker1_emotion_alpha": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "è¯´è¯äºº1æƒ…ç»ªå¼ºåº¦ / Speaker 1 emotion intensity"
                }),
                "speaker2_emotion_voice": ("AUDIO", {
                    "tooltip": "è¯´è¯äºº2çš„æƒ…ç»ªè¯­éŸ³æ ·æœ¬ / Speaker 2 emotion voice sample"
                }),
                "speaker2_emotion_alpha": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "è¯´è¯äºº2æƒ…ç»ªå¼ºåº¦ / Speaker 2 emotion intensity"
                }),
                "speaker3_emotion_voice": ("AUDIO", {
                    "tooltip": "è¯´è¯äºº3çš„æƒ…ç»ªè¯­éŸ³æ ·æœ¬ / Speaker 3 emotion voice sample"
                }),
                "speaker3_emotion_alpha": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "è¯´è¯äºº3æƒ…ç»ªå¼ºåº¦ / Speaker 3 emotion intensity"
                }),
                "speaker4_emotion_voice": ("AUDIO", {
                    "tooltip": "è¯´è¯äºº4çš„æƒ…ç»ªè¯­éŸ³æ ·æœ¬ / Speaker 4 emotion voice sample"
                }),
                "speaker4_emotion_alpha": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "è¯´è¯äºº4æƒ…ç»ªå¼ºåº¦ / Speaker 4 emotion intensity"
                }),
                "silence_duration": ("FLOAT", {
                    "default": 0.5,
                    "min": 0.0,
                    "max": 3.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "è¯´è¯é—´éš”æ—¶é—´(ç§’) / Silence duration between speakers (seconds)"
                }),
                "speed": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.5,
                    "max": 2.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "è¯­é€Ÿæ§åˆ¶ / Speech speed control"
                }),
                "use_fp16": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "ä½¿ç”¨FP16åŠ é€Ÿ / Use FP16 acceleration"
                }),
                "use_cuda_kernel": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "ä½¿ç”¨CUDAå†…æ ¸åŠ é€Ÿ / Use CUDA kernel acceleration"
                }),
            }
        }

    RETURN_TYPES = ("AUDIO", "STRING")
    RETURN_NAMES = ("audio", "conversation_info")
    FUNCTION = "synthesize_emotion_voice_conversation"
    CATEGORY = "IndexTTS2/Advanced"
    DESCRIPTION = "1-4 speaker conversation synthesis with emotion voice control (1=voice cloning, 2-4=conversation)"

    def synthesize_emotion_voice_conversation(
        self,
        num_speakers,
        conversation_text,
        speaker1_voice,
        output_filename,
        speaker2_voice=None,
        speaker3_voice=None,
        speaker4_voice=None,
        model_manager=None,
        speaker1_emotion_voice=None,
        speaker1_emotion_alpha=1.0,
        speaker2_emotion_voice=None,
        speaker2_emotion_alpha=1.0,
        speaker3_emotion_voice=None,
        speaker3_emotion_alpha=1.0,
        speaker4_emotion_voice=None,
        speaker4_emotion_alpha=1.0,
        silence_duration=0.5,
        speed=1.0,
        use_fp16=True,
        use_cuda_kernel=False,
    ):
        """
        åˆæˆå¸¦æƒ…ç»ªè¯­éŸ³æ§åˆ¶çš„å¤šäººå¯¹è¯
        Synthesize multi-speaker conversation with emotion voice control
        """
        try:
            # åŠ è½½æ¨¡å‹
            model = self._load_default_model(use_fp16, use_cuda_kernel)

            # æ£€æŸ¥æ˜¯å¦ä¸ºå•äººæ¨¡å¼
            if int(num_speakers) == 1:
                # å•äººæ¨¡å¼ï¼šçº¯è¯­éŸ³å…‹éš†
                return self._synthesize_single_speaker(
                    model, conversation_text, speaker1_voice, speaker1_emotion_voice,
                    speaker1_emotion_alpha, output_filename, speed
                )
            else:
                # å¤šäººæ¨¡å¼ï¼šåŸæœ‰é€»è¾‘
                # éªŒè¯å¤šäººæ¨¡å¼å¿…éœ€çš„è¯´è¯äººéŸ³é¢‘
                if speaker2_voice is None:
                    raise ValueError("Speaker 2 voice is required for 2+ speakers conversation")

                # è§£æå¯¹è¯æ–‡æœ¬
                conversation_lines = self._parse_conversation_text(conversation_text)

                # é…ç½®è¯´è¯äºº
                speakers_config = self._configure_speakers(
                    num_speakers,
                    speaker1_voice, speaker1_emotion_voice, speaker1_emotion_alpha,
                    speaker2_voice, speaker2_emotion_voice, speaker2_emotion_alpha,
                    speaker3_voice, speaker3_emotion_voice, speaker3_emotion_alpha,
                    speaker4_voice, speaker4_emotion_voice, speaker4_emotion_alpha
                )

                # åˆæˆå¯¹è¯
                conversation_audio = self._synthesize_with_emotion_voice(
                    model, conversation_lines, speakers_config, silence_duration, speed
                )

                # ä¿å­˜éŸ³é¢‘
                output_path = self._save_audio(conversation_audio, output_filename)

                # ç”Ÿæˆå¯¹è¯ä¿¡æ¯
                conversation_info = self._generate_conversation_info(conversation_lines, speakers_config)

                # ç¡®ä¿éŸ³é¢‘æ ¼å¼ç¬¦åˆ ComfyUI æ ‡å‡† [batch, channels, samples]
                if conversation_audio.dim() == 2:
                    # [channels, samples] -> [1, channels, samples]
                    conversation_audio = conversation_audio.unsqueeze(0)
                elif conversation_audio.dim() == 1:
                    # [samples] -> [1, 1, samples]
                    conversation_audio = conversation_audio.unsqueeze(0).unsqueeze(0)

                print(f"[EmotionVoiceMultiTalk] æœ€ç»ˆéŸ³é¢‘æ ¼å¼: {conversation_audio.shape}")
                print(f"[EmotionVoiceMultiTalk] ComfyUI AUDIOæ ¼å¼: batch={conversation_audio.shape[0]}, channels={conversation_audio.shape[1]}, samples={conversation_audio.shape[2]}")

                return ({"waveform": conversation_audio, "sample_rate": 24000}, conversation_info)
            
        except Exception as e:
            error_msg = f"IndexTTS2 emotion voice multi-talk synthesis failed: {str(e)}"
            print(f"[EmotionVoiceMultiTalk Error] {error_msg}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(error_msg)

    def _synthesize_single_speaker(self, model, text, voice_audio, emotion_voice_audio, emotion_alpha, output_filename, speed):
        """
        å•äººæ¨¡å¼åˆæˆ
        Single speaker mode synthesis
        """
        try:
            # ä½¿ç”¨ä¸å¤šäººæ¨¡å¼ç›¸åŒçš„åˆæˆæ–¹æ³•
            speaker_config = {
                'voice_audio': voice_audio,
                'emotion_mode': 'emotion_voice' if emotion_voice_audio is not None else 'none',
                'emotion_voice_audio': emotion_voice_audio,
                'emotion_alpha': emotion_alpha
            }

            # åˆæˆéŸ³é¢‘
            audio_tensor = self._synthesize_single_line(model, text, speaker_config, None, speed)

            if audio_tensor is None:
                raise RuntimeError("åˆæˆå¤±è´¥ï¼Œè¿”å›ç©ºéŸ³é¢‘")

            # ä¿å­˜éŸ³é¢‘
            output_path = self._save_audio(audio_tensor, output_filename)

            # ç¡®ä¿éŸ³é¢‘æ ¼å¼ç¬¦åˆ ComfyUI æ ‡å‡† [batch, channels, samples]
            if audio_tensor.dim() == 1:
                # [samples] -> [1, 1, samples]
                audio_tensor = audio_tensor.unsqueeze(0).unsqueeze(0)
            elif audio_tensor.dim() == 2:
                # [channels, samples] -> [1, channels, samples]
                audio_tensor = audio_tensor.unsqueeze(0)

            print(f"[SingleSpeaker] æœ€ç»ˆéŸ³é¢‘æ ¼å¼: {audio_tensor.shape}")

            # ç”Ÿæˆä¿¡æ¯
            info = f"å•äººè¯­éŸ³å…‹éš†å®Œæˆ\næ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦\néŸ³é¢‘é•¿åº¦: {audio_tensor.shape[-1]/24000:.2f} ç§’"
            if emotion_voice_audio is not None:
                info += f"\næƒ…ç»ªå¼ºåº¦: {emotion_alpha}"

            return ({"waveform": audio_tensor, "sample_rate": 24000}, info)

        except Exception as e:
            error_msg = f"å•äººæ¨¡å¼åˆæˆå¤±è´¥: {str(e)}"
            print(f"[SingleSpeaker Error] {error_msg}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(error_msg)

    def _load_default_model(self, use_fp16=True, use_cuda_kernel=False):
        """åŠ è½½é»˜è®¤æ¨¡å‹"""
        try:
            from ..indextts.infer_v2 import IndexTTS2

            if self.model is None:
                print("ğŸ”„ Loading IndexTTS2 model for emotion voice multi-talk...")

                from indextts.infer_v2 import IndexTTS2

                # ä½¿ç”¨é€šç”¨æ¨¡å‹è·¯å¾„å‡½æ•°
                from .model_utils import get_indextts2_model_path, validate_model_path

                model_dir, config_path = get_indextts2_model_path()

                print(f"[IndexTTS2] ä½¿ç”¨æ¨¡å‹è·¯å¾„: {model_dir}")
                print(f"[IndexTTS2] Using model path: {model_dir}")

                # éªŒè¯æ¨¡å‹è·¯å¾„
                validate_model_path(model_dir, config_path)

                # åˆå§‹åŒ–æ¨¡å‹
                self.model = IndexTTS2(
                    cfg_path=config_path,
                    model_dir=model_dir,
                    is_fp16=use_fp16,
                    use_cuda_kernel=use_cuda_kernel
                )

                print("âœ… IndexTTS2 model loaded successfully for emotion voice multi-talk")

            return self.model

        except Exception as e:
            raise RuntimeError(f"Failed to load IndexTTS2 model: {str(e)}")

    def _parse_conversation_text(self, conversation_text):
        """è§£æå¯¹è¯æ–‡æœ¬ï¼Œæå–è¯´è¯äººã€æƒ…ç»ªå’Œæ–‡æœ¬"""
        lines = []
        for line in conversation_text.strip().split('\n'):
            line = line.strip()
            if not line:
                continue

            # è§£ææ ¼å¼: Speaker1: [Emotion] Text æˆ– Speaker1: Text
            if ':' in line:
                speaker_part, text_part = line.split(':', 1)
                speaker = speaker_part.strip()
                text_part = text_part.strip()

                # æ£€æŸ¥æ˜¯å¦æœ‰æƒ…ç»ªæ ‡è®° [Emotion]
                emotion = None
                if text_part.startswith('[') and ']' in text_part:
                    end_bracket = text_part.find(']')
                    emotion = text_part[1:end_bracket].strip()
                    text = text_part[end_bracket + 1:].strip()
                else:
                    text = text_part

                lines.append({
                    'speaker': speaker,
                    'emotion': emotion,
                    'text': text
                })

        return lines

    def _configure_speakers(self, num_speakers, *speaker_configs):
        """é…ç½®è¯´è¯äººä¿¡æ¯"""
        speakers = {}

        # è§£æè¯´è¯äººé…ç½®å‚æ•° (voice, emotion_voice, emotion_alpha)
        config_groups = []
        for i in range(0, len(speaker_configs), 3):
            if i + 2 < len(speaker_configs):
                config_groups.append(speaker_configs[i:i+3])

        speaker_names = ["Speaker1", "Speaker2", "Speaker3", "Speaker4"]

        for i, (voice, emotion_voice, emotion_alpha) in enumerate(config_groups):
            if i >= int(num_speakers):
                break

            speaker_name = speaker_names[i]

            # åªæœ‰å½“voiceä¸ä¸ºNoneæ—¶æ‰é…ç½®è¯´è¯äºº
            if voice is not None:
                speakers[speaker_name] = {
                    'voice_audio': voice,
                    'emotion_mode': 'emotion_voice',  # é»˜è®¤ä½¿ç”¨æƒ…ç»ªè¯­éŸ³æ¨¡å¼
                    'emotion_voice_audio': emotion_voice,  # ç°åœ¨æ˜¯AUDIOå¯¹è±¡
                    'emotion_alpha': emotion_alpha
                }

        return speakers

    def _synthesize_with_emotion_voice(self, model, conversation_lines, speakers_config, silence_duration, speed):
        """ä½¿ç”¨æƒ…ç»ªè¯­éŸ³åˆæˆå¯¹è¯"""
        audio_segments = []

        for line in conversation_lines:
            speaker = line['speaker']
            text = line['text']
            emotion = line['emotion']

            if speaker not in speakers_config:
                print(f"âš ï¸ Speaker {speaker} not configured, skipping...")
                continue

            speaker_config = speakers_config[speaker]

            # åˆæˆå•å¥è¯
            audio_segment = self._synthesize_single_line(
                model, text, speaker_config, emotion, speed
            )

            if audio_segment is not None:
                audio_segments.append(audio_segment)

                # æ·»åŠ é™éŸ³é—´éš”
                if silence_duration > 0:
                    silence_samples = int(24000 * silence_duration)  # 24kHzé‡‡æ ·ç‡
                    silence = torch.zeros(silence_samples)
                    audio_segments.append(silence)

        # åˆå¹¶æ‰€æœ‰éŸ³é¢‘æ®µ
        if audio_segments:
            # ç§»é™¤æœ€åçš„é™éŸ³
            if len(audio_segments) > 1 and silence_duration > 0:
                audio_segments = audio_segments[:-1]

            # ç¡®ä¿æ‰€æœ‰éŸ³é¢‘æ®µéƒ½æ˜¯1Då¼ é‡
            normalized_segments = []
            for segment in audio_segments:
                if segment.dim() > 1:
                    segment = segment.squeeze()  # ç§»é™¤å¤šä½™ç»´åº¦
                if segment.dim() == 0:
                    segment = segment.unsqueeze(0)  # ç¡®ä¿è‡³å°‘æ˜¯1D
                normalized_segments.append(segment)

            conversation_audio = torch.cat(normalized_segments, dim=0)

            # ç¡®ä¿è¿”å›çš„æ˜¯2Då¼ é‡ [channels, samples]
            if conversation_audio.dim() == 1:
                conversation_audio = conversation_audio.unsqueeze(0)  # [samples] -> [1, samples]

            return conversation_audio
        else:
            # è¿”å›ç©ºéŸ³é¢‘ [1, samples]
            return torch.zeros(1, 1000)

    def _synthesize_single_line(self, model, text, speaker_config, emotion, speed):
        """åˆæˆå•å¥è¯"""
        try:
            # è·å–è¯´è¯äººéŸ³é¢‘
            voice_audio = speaker_config['voice_audio']
            emotion_mode = speaker_config['emotion_mode']
            emotion_voice_audio = speaker_config['emotion_voice_audio']
            emotion_alpha = speaker_config['emotion_alpha']

            # å‡†å¤‡è¯´è¯äººéŸ³é¢‘æ–‡ä»¶
            import tempfile
            import torchaudio

            # ä¿å­˜è¯´è¯äººéŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_speaker_file:
                speaker_audio_path = temp_speaker_file.name

                # ç¡®ä¿éŸ³é¢‘å¼ é‡æ˜¯2Dæ ¼å¼ [channels, samples]
                speaker_waveform = voice_audio['waveform']
                if speaker_waveform.dim() == 3:
                    speaker_waveform = speaker_waveform.squeeze(0)  # ç§»é™¤batchç»´åº¦
                elif speaker_waveform.dim() == 1:
                    speaker_waveform = speaker_waveform.unsqueeze(0)  # æ·»åŠ channelç»´åº¦

                torchaudio.save(
                    speaker_audio_path,
                    speaker_waveform,
                    voice_audio['sample_rate']
                )

            # åˆ›å»ºè¾“å‡ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_output_file:
                output_path = temp_output_file.name

            try:
                # æ ¹æ®æƒ…ç»ªæ¨¡å¼è°ƒç”¨ä¸åŒçš„åˆæˆæ–¹æ³•
                if emotion_mode == "emotion_voice" and emotion_voice_audio is not None:
                    # ä¿å­˜æƒ…ç»ªè¯­éŸ³åˆ°ä¸´æ—¶æ–‡ä»¶
                    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_emotion_file:
                        emotion_audio_path = temp_emotion_file.name

                        # ç¡®ä¿æƒ…ç»ªéŸ³é¢‘å¼ é‡æ˜¯2Dæ ¼å¼ [channels, samples]
                        emotion_waveform = emotion_voice_audio['waveform']
                        if emotion_waveform.dim() == 3:
                            emotion_waveform = emotion_waveform.squeeze(0)  # ç§»é™¤batchç»´åº¦
                        elif emotion_waveform.dim() == 1:
                            emotion_waveform = emotion_waveform.unsqueeze(0)  # æ·»åŠ channelç»´åº¦

                        torchaudio.save(
                            emotion_audio_path,
                            emotion_waveform,
                            emotion_voice_audio['sample_rate']
                        )

                    try:
                        # ä½¿ç”¨æƒ…ç»ªè¯­éŸ³æ§åˆ¶
                        model.infer(
                            spk_audio_prompt=speaker_audio_path,
                            text=text,
                            output_path=output_path,
                            emo_audio_prompt=emotion_audio_path,
                            emo_alpha=emotion_alpha,
                            verbose=False,
                            temperature=0.7,
                            top_p=0.9,
                            top_k=50,
                            max_text_tokens_per_sentence=120,
                            interval_silence=200
                        )
                    finally:
                        # æ¸…ç†æƒ…ç»ªéŸ³é¢‘ä¸´æ—¶æ–‡ä»¶
                        try:
                            os.unlink(emotion_audio_path)
                        except:
                            pass
                else:
                    # åŸºç¡€åˆæˆï¼ˆæ— æƒ…ç»ªæ§åˆ¶ï¼‰
                    model.infer(
                        spk_audio_prompt=speaker_audio_path,
                        text=text,
                        output_path=output_path,
                        verbose=False,
                        temperature=0.7,
                        top_p=0.9,
                        top_k=50,
                        max_text_tokens_per_sentence=120,
                        interval_silence=200
                    )

                # åŠ è½½ç”Ÿæˆçš„éŸ³é¢‘
                audio_data, sample_rate = torchaudio.load(output_path)

                # è¿”å›éŸ³é¢‘tensor
                return audio_data.squeeze()

            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.unlink(speaker_audio_path)
                    os.unlink(output_path)
                except:
                    pass

        except Exception as e:
            print(f"âŒ Failed to synthesize line: {text}, error: {str(e)}")
            import traceback
            traceback.print_exc()
            return None

    def _save_audio(self, audio_tensor, filename):
        """ä¿å­˜éŸ³é¢‘æ–‡ä»¶"""
        output_dir = folder_paths.get_output_directory()

        # ç¡®ä¿æ–‡ä»¶åæœ‰æ­£ç¡®çš„æ‰©å±•å
        if not filename.lower().endswith(('.wav', '.mp3', '.flac')):
            filename = filename + ".wav"

        output_path = os.path.join(output_dir, filename)

        # ç¡®ä¿éŸ³é¢‘æ˜¯æ­£ç¡®çš„å½¢çŠ¶
        if audio_tensor.dim() == 1:
            audio_tensor = audio_tensor.unsqueeze(0)

        # ä¿å­˜éŸ³é¢‘
        torchaudio.save(output_path, audio_tensor, 24000)
        print(f"ğŸ’¾ Emotion voice conversation saved to: {output_path}")

        return output_path

    def _generate_conversation_info(self, conversation_lines, speakers_config):
        """ç”Ÿæˆå¯¹è¯ä¿¡æ¯"""
        info_lines = []
        info_lines.append("ğŸ­ Emotion Voice Multi-Talk Conversation Info")
        info_lines.append("=" * 50)

        # è¯´è¯äººé…ç½®ä¿¡æ¯
        info_lines.append("\nğŸ‘¥ Speakers Configuration:")
        for speaker, config in speakers_config.items():
            info_lines.append(f"  {speaker}:")
            info_lines.append(f"    - Emotion Mode: {config['emotion_mode']}")
            info_lines.append(f"    - Emotion Alpha: {config['emotion_alpha']}")
            if config.get('emotion_voice_audio') is not None:
                info_lines.append(f"    - Emotion Voice: Connected")
            else:
                info_lines.append(f"    - Emotion Voice: None")

        # å¯¹è¯å†…å®¹ä¿¡æ¯
        info_lines.append(f"\nğŸ’¬ Conversation Lines: {len(conversation_lines)}")
        for i, line in enumerate(conversation_lines, 1):
            emotion_info = f" [{line['emotion']}]" if line['emotion'] else ""
            info_lines.append(f"  {i}. {line['speaker']}{emotion_info}: {line['text'][:50]}...")

        return "\n".join(info_lines)

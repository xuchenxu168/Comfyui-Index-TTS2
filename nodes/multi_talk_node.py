# IndexTTS2 Multi-Talk Node with Emotion Control
# IndexTTS2 å¤šäººå¯¹è¯è¯­éŸ³åˆæˆèŠ‚ç‚¹ï¼ˆå¸¦æƒ…æ„Ÿæ§åˆ¶ï¼‰

import os
import torch
import numpy as np
import tempfile
import torchaudio
from typing import Optional, Tuple, Any, List, Dict
import folder_paths

class IndexTTS2MultiTalkNode:
    """
    IndexTTS2 å¤šäººå¯¹è¯è¯­éŸ³åˆæˆèŠ‚ç‚¹ï¼ˆå¸¦æƒ…æ„Ÿæ§åˆ¶ï¼‰
    Multi-speaker conversation text-to-speech synthesis node for IndexTTS2 with emotion control

    Features:
    - Support 1-4 speakers: 1=voice cloning, 2-4=conversation
    - Individual speaker voice cloning
    - Individual emotion control for each speaker
    - Multiple emotion control modes (audio, vector, text, auto)
    - Automatic conversation flow
    - Configurable silence intervals
    - High-quality multi-speaker synthesis with emotions
    """

    def __init__(self):
        self.model = None
        self.model_config = None
        
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "num_speakers": (["1", "2", "3", "4"], {
                    "default": "2",
                    "tooltip": "å¯¹è¯äººæ•° / Number of speakers (1=çº¯è¯­éŸ³å…‹éš†)"
                }),
                "conversation_text": ("STRING", {
                    "multiline": True,
                    "default": "Speaker1: Hello, how are you today!\nSpeaker2: I'm doing great, thank you for asking!",
                    "placeholder": "å•äººæ¨¡å¼ï¼šç›´æ¥è¾“å…¥æ–‡æœ¬\\nå¤šäººæ¨¡å¼ï¼šSpeaker1: æ–‡æœ¬\\nSpeaker2: æ–‡æœ¬..."
                }),
                "speaker1_audio": ("AUDIO", {
                    "tooltip": "è¯´è¯äºº1çš„éŸ³é¢‘æ ·æœ¬ / Speaker 1 audio sample"
                }),
                "output_filename": ("STRING", {
                    "default": "multi_talk_emotion_output.wav",
                    "placeholder": "è¾“å‡ºéŸ³é¢‘æ–‡ä»¶å"
                }),
            },
            "optional": {
                "speaker2_audio": ("AUDIO", {
                    "tooltip": "è¯´è¯äºº2çš„éŸ³é¢‘æ ·æœ¬ / Speaker 2 audio sample (å¤šäººæ¨¡å¼å¿…éœ€)"
                }),
                "speaker3_audio": ("AUDIO", {
                    "tooltip": "è¯´è¯äºº3çš„éŸ³é¢‘æ ·æœ¬ï¼ˆ3-4äººå¯¹è¯æ—¶éœ€è¦ï¼‰/ Speaker 3 audio sample (required for 3-4 speakers)"
                }),
                "speaker4_audio": ("AUDIO", {
                    "tooltip": "è¯´è¯äºº4çš„éŸ³é¢‘æ ·æœ¬ï¼ˆ4äººå¯¹è¯æ—¶éœ€è¦ï¼‰/ Speaker 4 audio sample (required for 4 speakers)"
                }),
                "model_manager": ("INDEXTTS2_MODEL",),
                "language": (["auto", "zh", "en", "zh-en"], {
                    "default": "auto"
                }),
                "speed": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.5,
                    "max": 2.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "è¯­é€Ÿæ§åˆ¶ / Speed control"
                }),
                "silence_duration": ("FLOAT", {
                    "default": 0.5,
                    "min": 0.0,
                    "max": 3.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "è¯´è¯äººä¹‹é—´çš„é™éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰/ Silence duration between speakers (seconds)"
                }),
                "voice_consistency": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.1,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "å£°éŸ³ä¸€è‡´æ€§å¼ºåº¦ï¼ˆè¶Šé«˜è¶Šæ¥è¿‘å‚è€ƒéŸ³é¢‘ï¼‰/ Voice consistency strength (higher = closer to reference audio)"
                }),
                "reference_boost": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¯ç”¨å‚è€ƒéŸ³é¢‘å¢å¼ºï¼ˆæé«˜å£°éŸ³ç›¸ä¼¼åº¦ï¼‰/ Enable reference audio enhancement (improves voice similarity)"
                }),
                # æƒ…æ„Ÿæ§åˆ¶è¾“å…¥ç«¯å£
                "speaker1_emotion_config": ("SPEAKER_EMOTION_CONFIG", {
                    "tooltip": "è¯´è¯äºº1æƒ…æ„Ÿé…ç½® / Speaker 1 emotion config"
                }),
                "speaker2_emotion_config": ("SPEAKER_EMOTION_CONFIG", {
                    "tooltip": "è¯´è¯äºº2æƒ…æ„Ÿé…ç½® / Speaker 2 emotion config"
                }),
                "speaker3_emotion_config": ("SPEAKER_EMOTION_CONFIG", {
                    "tooltip": "è¯´è¯äºº3æƒ…æ„Ÿé…ç½®ï¼ˆ3-4äººå¯¹è¯æ—¶éœ€è¦ï¼‰/ Speaker 3 emotion config (required for 3-4 speakers)"
                }),
                "speaker4_emotion_config": ("SPEAKER_EMOTION_CONFIG", {
                    "tooltip": "è¯´è¯äºº4æƒ…æ„Ÿé…ç½®ï¼ˆ4äººå¯¹è¯æ—¶éœ€è¦ï¼‰/ Speaker 4 emotion config (required for 4 speakers)"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.1,
                    "max": 1.5,
                    "step": 0.1,
                    "display": "slider"
                }),
                "top_p": ("FLOAT", {
                    "default": 0.9,
                    "min": 0.1,
                    "max": 1.0,
                    "step": 0.05,
                    "display": "slider"
                }),
                "use_fp16": ("BOOLEAN", {
                    "default": False
                }),
                "use_cuda_kernel": ("BOOLEAN", {
                    "default": False
                }),
                "verbose": ("BOOLEAN", {
                    "default": True
                }),
            }
        }
    
    RETURN_TYPES = ("AUDIO", "STRING", "STRING", "STRING")
    RETURN_NAMES = ("audio", "output_path", "info", "emotion_analysis")
    FUNCTION = "synthesize_conversation"
    CATEGORY = "IndexTTS2/Advanced"
    DESCRIPTION = "1-4 speaker conversation synthesis with individual emotion control using IndexTTS2 (1=voice cloning, 2-4=conversation)"
    
    def synthesize_conversation(
        self,
        num_speakers: str,
        conversation_text: str,
        speaker1_audio: dict,
        output_filename: str,
        speaker2_audio: Optional[dict] = None,
        speaker3_audio: Optional[dict] = None,
        speaker4_audio: Optional[dict] = None,
        model_manager: Optional[Any] = None,
        language: str = "auto",
        speed: float = 1.0,
        silence_duration: float = 0.5,
        voice_consistency: float = 1.0,
        reference_boost: bool = True,
        speaker1_emotion_config: Optional[dict] = None,
        speaker2_emotion_config: Optional[dict] = None,
        speaker3_emotion_config: Optional[dict] = None,
        speaker4_emotion_config: Optional[dict] = None,
        temperature: float = 0.7,
        top_p: float = 0.9,
        use_fp16: bool = False,
        use_cuda_kernel: bool = False,
        verbose: bool = True
    ) -> Tuple[dict, str, str, str]:
        """
        æ‰§è¡Œå¤šäººå¯¹è¯è¯­éŸ³åˆæˆï¼ˆå¸¦æƒ…æ„Ÿæ§åˆ¶ï¼‰
        Perform multi-speaker conversation text-to-speech synthesis with emotion control
        """
        try:
            # éªŒè¯è¾“å…¥
            if not conversation_text.strip():
                raise ValueError("Conversation text cannot be empty")

            num_speakers_int = int(num_speakers)

            # æ£€æŸ¥æ˜¯å¦ä¸ºå•äººæ¨¡å¼
            if num_speakers_int == 1:
                # å•äººæ¨¡å¼ï¼šçº¯è¯­éŸ³å…‹éš†
                return self._synthesize_single_speaker(
                    conversation_text, speaker1_audio, speaker1_emotion_config,
                    output_filename, model_manager, language, speed, temperature,
                    top_p, use_fp16, use_cuda_kernel, verbose
                )

            # å¤šäººæ¨¡å¼ï¼šåŸæœ‰é€»è¾‘
            # éªŒè¯è¯´è¯äººéŸ³é¢‘
            if speaker2_audio is None:
                raise ValueError("Speaker 2 audio is required for 2+ speakers conversation")

            speaker_audios = [speaker1_audio, speaker2_audio]
            if num_speakers_int >= 3:
                if speaker3_audio is None:
                    raise ValueError("Speaker 3 audio is required for 3+ speakers conversation")
                speaker_audios.append(speaker3_audio)
            if num_speakers_int >= 4:
                if speaker4_audio is None:
                    raise ValueError("Speaker 4 audio is required for 4 speakers conversation")
                speaker_audios.append(speaker4_audio)

            # è§£æå¯¹è¯æ–‡æœ¬
            conversation_lines = self._parse_conversation(conversation_text, num_speakers_int, verbose)

            # å‡†å¤‡è¯´è¯äººéŸ³é¢‘æ–‡ä»¶ï¼ˆå¸¦ä¸€è‡´æ€§å¢å¼ºï¼‰
            speaker_audio_paths = self._prepare_speaker_audios(
                speaker_audios[:num_speakers_int], verbose, voice_consistency, reference_boost
            )

            # å‡†å¤‡æƒ…æ„Ÿæ§åˆ¶å‚æ•°
            emotion_configs = self._prepare_emotion_configs_from_inputs(
                num_speakers_int,
                [speaker1_emotion_config, speaker2_emotion_config, speaker3_emotion_config, speaker4_emotion_config],
                verbose
            )

            # è·å–æ¨¡å‹å®ä¾‹
            if model_manager is not None:
                model = model_manager
            else:
                model = self._load_default_model(use_fp16, use_cuda_kernel)
            
            # åˆæˆæ¯ä¸ªå¯¹è¯ç‰‡æ®µï¼ˆå¸¦æƒ…æ„Ÿæ§åˆ¶ï¼‰
            audio_segments = []
            emotion_analysis_list = []

            for line_info in conversation_lines:
                speaker_idx = line_info["speaker_idx"]
                text = line_info["text"]
                speaker_audio_path = speaker_audio_paths[speaker_idx]
                emotion_config = emotion_configs[speaker_idx]

                if verbose:
                    print(f"[MultiTalk] Synthesizing Speaker{speaker_idx + 1}: {text[:50]}...")
                    if emotion_config["mode"] != "none":
                        print(f"[MultiTalk] Emotion mode: {emotion_config['mode']}")

                # åˆ›å»ºä¸´æ—¶è¾“å‡ºæ–‡ä»¶
                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                    temp_output = tmp_file.name

                # å¤„ç†è¯­è¨€å‚æ•°
                processed_language = language if language != "auto" else "zh"

                # æ‰§è¡Œå•ä¸ªç‰‡æ®µçš„æƒ…æ„Ÿåˆæˆï¼ˆå¸¦ä¸€è‡´æ€§æ§åˆ¶ï¼‰
                emotion_analysis = self._synthesize_with_emotion(
                    model, text, speaker_audio_path, emotion_config,
                    temp_output, temperature, top_p, verbose, voice_consistency, processed_language
                )
                emotion_analysis_list.append(f"Speaker{speaker_idx + 1}: {emotion_analysis}")

                # åŠ è½½åˆæˆçš„éŸ³é¢‘
                segment_audio = self._load_audio(temp_output)
                audio_segments.append(segment_audio)

                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.unlink(temp_output)
                except:
                    pass
            
            # åˆå¹¶éŸ³é¢‘ç‰‡æ®µ
            final_audio = self._merge_audio_segments(audio_segments, silence_duration, verbose)
            
            # å‡†å¤‡è¾“å‡ºè·¯å¾„
            output_dir = folder_paths.get_output_directory()
            output_path = os.path.join(output_dir, output_filename)
            
            # ä¿å­˜æœ€ç»ˆéŸ³é¢‘
            torchaudio.save(output_path, final_audio["waveform"], final_audio["sample_rate"])
            
            # ç¡®ä¿éŸ³é¢‘æ ¼å¼å…¼å®¹ComfyUI
            waveform = final_audio["waveform"]
            sample_rate = final_audio["sample_rate"]
            
            # åº”ç”¨ComfyUIå…¼å®¹æ€§æ£€æŸ¥
            from .audio_utils import fix_comfyui_audio_compatibility
            waveform = fix_comfyui_audio_compatibility(waveform)
            
            # ComfyUI AUDIOæ ¼å¼éœ€è¦ [batch, channels, samples]
            if waveform.dim() == 1:
                # [samples] -> [1, 1, samples]
                waveform = waveform.unsqueeze(0).unsqueeze(0)
            elif waveform.dim() == 2:
                # [channels, samples] -> [1, channels, samples]
                waveform = waveform.unsqueeze(0)
            
            # åˆ›å»ºComfyUI AUDIOæ ¼å¼
            comfyui_audio = {
                "waveform": waveform,
                "sample_rate": sample_rate
            }
            
            if verbose:
                print(f"[MultiTalk] å¯¹è¯åˆæˆå®Œæˆ: {len(conversation_lines)} ä¸ªç‰‡æ®µ")
                print(f"[MultiTalk] æœ€ç»ˆéŸ³é¢‘æ ¼å¼: {waveform.shape}, é‡‡æ ·ç‡: {sample_rate}")

            # ç”Ÿæˆä¿¡æ¯å­—ç¬¦ä¸²
            info = self._generate_info_with_emotion(
                conversation_lines, num_speakers_int, output_path, language, speed,
                silence_duration, emotion_configs
            )

            # ç”Ÿæˆæƒ…æ„Ÿåˆ†æå­—ç¬¦ä¸²
            emotion_analysis = "\n".join(emotion_analysis_list)

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for path in speaker_audio_paths:
                try:
                    os.unlink(path)
                except:
                    pass

            return (comfyui_audio, output_path, info, emotion_analysis)
            
        except Exception as e:
            error_msg = f"IndexTTS2 multi-talk synthesis failed: {str(e)}"
            print(f"[MultiTalk Error] {error_msg}")
            raise RuntimeError(error_msg)

    def _synthesize_single_speaker(
        self,
        text: str,
        speaker_audio: dict,
        emotion_config: Optional[dict],
        output_filename: str,
        model_manager: Optional[Any],
        language: str,
        speed: float,
        temperature: float,
        top_p: float,
        use_fp16: bool,
        use_cuda_kernel: bool,
        verbose: bool
    ) -> Tuple[dict, str, str, str]:
        """
        å•äººæ¨¡å¼åˆæˆ
        Single speaker mode synthesis
        """
        try:
            if verbose:
                print(f"[MultiTalk] å•äººæ¨¡å¼åˆæˆ / Single speaker mode synthesis")
                print(f"[MultiTalk] æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦ / Text length: {len(text)} characters")

            # è·å–æ¨¡å‹å®ä¾‹
            if model_manager is not None:
                model = model_manager
            else:
                model = self._load_default_model(use_fp16, use_cuda_kernel)

            # å‡†å¤‡è¯´è¯äººéŸ³é¢‘
            speaker_audio_path = self._prepare_speaker_audios([speaker_audio], verbose, 1.0, True)[0]

            # å‡†å¤‡æƒ…æ„Ÿæ§åˆ¶å‚æ•°
            if emotion_config is None:
                emotion_config = {"mode": "none"}

            # åˆ›å»ºä¸´æ—¶è¾“å‡ºæ–‡ä»¶
            import tempfile
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
                temp_output_path = temp_file.name

            # å¤„ç†è¯­è¨€å‚æ•°
            processed_language = language if language != "auto" else "zh"

            # åˆæˆéŸ³é¢‘
            emotion_analysis = self._synthesize_with_emotion(
                model, text, speaker_audio_path, emotion_config, temp_output_path,
                temperature, top_p, verbose, language=processed_language
            )

            # ä»ä¸´æ—¶æ–‡ä»¶åŠ è½½éŸ³é¢‘
            if os.path.exists(temp_output_path):
                audio_tensor, sample_rate = torchaudio.load(temp_output_path)
                # ç¡®ä¿é‡‡æ ·ç‡æ­£ç¡®
                if sample_rate != 24000:
                    import torchaudio.transforms as T
                    resampler = T.Resample(sample_rate, 24000)
                    audio_tensor = resampler(audio_tensor)
            else:
                raise RuntimeError("åˆæˆå¤±è´¥ï¼Œä¸´æ—¶éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨")

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(temp_output_path)
            except:
                pass

            # ä¿å­˜éŸ³é¢‘
            output_path = self._save_audio(audio_tensor, output_filename)

            # ç”Ÿæˆä¿¡æ¯
            info = f"å•äººè¯­éŸ³åˆæˆå®Œæˆ\næ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦\néŸ³é¢‘é•¿åº¦: {len(audio_tensor[0])/24000:.2f} ç§’"
            if emotion_config["mode"] != "none":
                info += f"\næƒ…ç»ªæ¨¡å¼: {emotion_config['mode']}"
                info += f"\næƒ…æ„Ÿåˆ†æ: {emotion_analysis}"

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(speaker_audio_path)
            except:
                pass

            # ç¡®ä¿éŸ³é¢‘æ ¼å¼ç¬¦åˆ ComfyUI æ ‡å‡† [batch, channels, samples]
            if audio_tensor.dim() == 1:
                # [samples] -> [1, 1, samples]
                audio_tensor = audio_tensor.unsqueeze(0).unsqueeze(0)
            elif audio_tensor.dim() == 2:
                # [channels, samples] -> [1, channels, samples]
                audio_tensor = audio_tensor.unsqueeze(0)

            if verbose:
                print(f"[MultiTalk SingleSpeaker] æœ€ç»ˆéŸ³é¢‘æ ¼å¼: {audio_tensor.shape}")
                print(f"[MultiTalk SingleSpeaker] ComfyUI AUDIOæ ¼å¼: batch={audio_tensor.shape[0]}, channels={audio_tensor.shape[1]}, samples={audio_tensor.shape[2]}")

            # è¿”å›ComfyUIæ ¼å¼çš„éŸ³é¢‘
            comfyui_audio = {"waveform": audio_tensor, "sample_rate": 24000}

            return (comfyui_audio, output_path, info, emotion_analysis)

        except Exception as e:
            error_msg = f"å•äººæ¨¡å¼åˆæˆå¤±è´¥: {str(e)}"
            print(f"[SingleSpeaker Error] {error_msg}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(error_msg)

    def _parse_conversation(self, conversation_text: str, num_speakers: int, verbose: bool) -> List[Dict]:
        """è§£æå¯¹è¯æ–‡æœ¬ - æ”¯æŒè‡ªå®šä¹‰è¯´è¯äººåç§°"""
        lines = conversation_text.strip().split('\n')
        conversation_lines = []

        # é¦–å…ˆæ‰«ææ‰€æœ‰è¡Œï¼Œæå–æ‰€æœ‰è¯´è¯äººåç§°
        speaker_names = []
        for line in lines:
            line = line.strip()
            if ':' in line:
                potential_speaker = line.split(':', 1)[0].strip()
                if potential_speaker and potential_speaker not in speaker_names:
                    speaker_names.append(potential_speaker)

        # é™åˆ¶è¯´è¯äººæ•°é‡
        speaker_names = speaker_names[:num_speakers]

        if verbose:
            print(f"[MultiTalk] æ£€æµ‹åˆ°è¯´è¯äºº: {speaker_names}")

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # æŸ¥æ‰¾è¯´è¯äººæ ‡è¯†
            speaker_found = False

            # é¦–å…ˆå°è¯•åŒ¹é…æ£€æµ‹åˆ°çš„è¯´è¯äººåç§°
            for i, speaker_name in enumerate(speaker_names):
                if line.startswith(f"{speaker_name}:"):
                    text = line[len(f"{speaker_name}:"):].strip()
                    if text:
                        conversation_lines.append({
                            "speaker_idx": i,  # 0-based index
                            "speaker_name": speaker_name,
                            "text": text
                        })
                        speaker_found = True
                        break

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æ ‡å‡†æ ¼å¼
            if not speaker_found:
                for i in range(1, num_speakers + 1):
                    speaker_patterns = [f"Speaker{i}:", f"speaker{i}:", f"è¯´è¯äºº{i}:", f"S{i}:"]
                    for pattern in speaker_patterns:
                        if line.startswith(pattern):
                            text = line[len(pattern):].strip()
                            if text:
                                conversation_lines.append({
                                    "speaker_idx": i - 1,  # 0-based index
                                    "speaker_name": f"Speaker{i}",
                                    "text": text
                                })
                                speaker_found = True
                                break
                    if speaker_found:
                        break

            if not speaker_found and line:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯´è¯äººæ ‡è¯†ï¼Œé»˜è®¤åˆ†é…ç»™ç¬¬ä¸€ä¸ªè¯´è¯äºº
                conversation_lines.append({
                    "speaker_idx": 0,
                    "speaker_name": speaker_names[0] if speaker_names else "Speaker1",
                    "text": line
                })
                if verbose:
                    print(f"[MultiTalk] æœªè¯†åˆ«è¯´è¯äººï¼Œåˆ†é…ç»™{speaker_names[0] if speaker_names else 'Speaker1'}: {line[:30]}...")

        if not conversation_lines:
            raise ValueError("No valid conversation lines found. Please use format: 'Speaker1: text' or 'YourName: text'")

        if verbose:
            print(f"[MultiTalk] è§£æåˆ° {len(conversation_lines)} ä¸ªå¯¹è¯ç‰‡æ®µ")
            for i, line in enumerate(conversation_lines):
                print(f"  {i+1}. {line['speaker_name']}: {line['text'][:50]}...")

        return conversation_lines

    def _prepare_emotion_configs_from_inputs(self, num_speakers: int,
                                            emotion_config_inputs: List[Optional[Dict]],
                                            verbose: bool) -> List[Dict]:
        """ä»è¾“å…¥çš„æƒ…æ„Ÿé…ç½®å¯¹è±¡å‡†å¤‡æƒ…æ„Ÿæ§åˆ¶é…ç½®"""
        emotion_configs = []

        for i in range(num_speakers):
            emotion_input = emotion_config_inputs[i] if i < len(emotion_config_inputs) else None

            if emotion_input is None or not emotion_input.get("enabled", True):
                # å¦‚æœæ²¡æœ‰æä¾›æƒ…æ„Ÿé…ç½®æˆ–è¢«ç¦ç”¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
                emotion_config = {"mode": "none"}
                if verbose:
                    print(f"[MultiTalk] Speaker{i+1}: No emotion control (default)")
            else:
                # ä½¿ç”¨æä¾›çš„æƒ…æ„Ÿé…ç½®
                emotion_config = {
                    "mode": emotion_input.get("mode", "none"),
                    "audio": emotion_input.get("audio", ""),
                    "alpha": emotion_input.get("alpha", 1.0),
                    "vector": emotion_input.get("vector", [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]),
                    "text": emotion_input.get("text", "")
                }

                if verbose:
                    mode = emotion_config["mode"]
                    speaker_name = emotion_input.get("speaker_name", f"Speaker{i+1}")
                    print(f"[MultiTalk] {speaker_name} emotion mode: {mode}")

                    if mode == "emotion_vector":
                        vector = emotion_config["vector"]
                        emotion_names = ["Happy", "Angry", "Sad", "Fear", "Hate", "Low", "Surprise", "Neutral"]
                        active_emotions = [f"{name}: {val:.2f}" for name, val in zip(emotion_names, vector) if val > 0.05]
                        if active_emotions:
                            print(f"[MultiTalk] {speaker_name} emotions: {', '.join(active_emotions)}")

            emotion_configs.append(emotion_config)

        return emotion_configs

    def _save_audio(self, audio_tensor, filename):
        """ä¿å­˜éŸ³é¢‘æ–‡ä»¶"""
        output_dir = folder_paths.get_output_directory()

        # ç¡®ä¿æ–‡ä»¶åæœ‰æ­£ç¡®çš„æ‰©å±•å
        if not filename.lower().endswith(('.wav', '.mp3', '.flac')):
            filename = filename + ".wav"

        output_path = os.path.join(output_dir, filename)

        # ç¡®ä¿éŸ³é¢‘æ˜¯æ­£ç¡®çš„å½¢çŠ¶
        if audio_tensor.dim() == 1:
            audio_tensor = audio_tensor.unsqueeze(0)

        # ä¿å­˜éŸ³é¢‘
        torchaudio.save(output_path, audio_tensor, 24000)
        print(f"ğŸ’¾ Multi-talk conversation saved to: {output_path}")

        return output_path

    def _synthesize_with_emotion(self, model, text: str, speaker_audio_path: str,
                                emotion_config: Dict, output_path: str,
                                temperature: float, top_p: float, verbose: bool,
                                voice_consistency: float = 1.0, language: str = "zh") -> str:
        """æ‰§è¡Œå¸¦æƒ…æ„Ÿæ§åˆ¶çš„è¯­éŸ³åˆæˆ"""
        emotion_mode = emotion_config["mode"]

        # åº”ç”¨å£°éŸ³ä¸€è‡´æ€§å‚æ•°
        consistency_temp = max(0.1, temperature / voice_consistency)
        consistency_top_p = min(0.99, top_p * voice_consistency)

        if emotion_mode == "none":
            # æ— æƒ…æ„Ÿæ§åˆ¶ï¼Œä½¿ç”¨åŸºç¡€åˆæˆ
            model.infer(
                spk_audio_prompt=speaker_audio_path,
                text=text,
                output_path=output_path,
                verbose=False,
                temperature=consistency_temp,
                top_p=consistency_top_p,
                top_k=50,
                max_text_tokens_per_sentence=120,
                interval_silence=200
            )
            return "No emotion control"

        elif emotion_mode == "audio_prompt":
            emotion_audio_info = emotion_config["audio"]
            emotion_alpha = emotion_config["alpha"]

            if emotion_audio_info and isinstance(emotion_audio_info, dict) and "audio_object" in emotion_audio_info:
                # å°†AUDIOå¯¹è±¡ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
                emotion_audio_path = self._save_emotion_audio_to_temp(emotion_audio_info["audio_object"])
                if emotion_audio_path:
                    try:
                        model.infer(
                            spk_audio_prompt=speaker_audio_path,
                            text=text,
                            output_path=output_path,
                            emo_audio_prompt=emotion_audio_path,
                            emo_alpha=emotion_alpha,
                            verbose=False,
                            temperature=consistency_temp,
                            top_p=consistency_top_p,
                            top_k=50,
                            max_text_tokens_per_sentence=120,
                            interval_silence=200
                        )
                    finally:
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        try:
                            os.unlink(emotion_audio_path)
                        except:
                            pass
                else:
                    # å›é€€åˆ°æ— æƒ…æ„Ÿæ§åˆ¶
                    model.infer(
                        spk_audio_prompt=speaker_audio_path,
                        text=text,
                        output_path=output_path,
                        verbose=False,
                        temperature=consistency_temp,
                        top_p=consistency_top_p,
                        top_k=50,
                        max_text_tokens_per_sentence=120,
                        interval_silence=200
                    )
                return f"Audio emotion ({os.path.basename(emotion_audio)}, Î±={emotion_alpha})"
            else:
                # å›é€€åˆ°åŸºç¡€åˆæˆ
                model.infer(
                    spk_audio_prompt=speaker_audio_path,
                    text=text,
                    output_path=output_path,
                    verbose=False,
                    temperature=consistency_temp,
                    top_p=consistency_top_p,
                    top_k=50,
                    max_text_tokens_per_sentence=120,
                    interval_silence=200
                )
                return "No emotion audio provided"

        elif emotion_mode == "emotion_vector":
            emotion_vector = emotion_config["vector"]

            # æ£€æŸ¥æƒ…æ„Ÿå‘é‡æ˜¯å¦å…¨ä¸ºé›¶
            max_emotion_value = max(emotion_vector)
            if max_emotion_value == 0.0:
                # è®¾ç½®ä¸€ä¸ªå°çš„ä¸­æ€§æƒ…æ„Ÿå€¼
                emotion_vector = emotion_vector.copy()
                emotion_vector[7] = 0.1  # Neutral

            model.infer(
                spk_audio_prompt=speaker_audio_path,
                text=text,
                output_path=output_path,
                emo_vector=emotion_vector,
                verbose=False,
                temperature=consistency_temp,
                top_p=consistency_top_p,
                top_k=50,
                max_text_tokens_per_sentence=120,
                interval_silence=200
            )

            # åˆ†æä¸»è¦æƒ…æ„Ÿ
            emotion_names = ["Happy", "Angry", "Sad", "Fear", "Hate", "Low", "Surprise", "Neutral"]
            max_value = max(emotion_vector)
            if max_value > 0:
                max_idx = emotion_vector.index(max_value)
                dominant_emotion = emotion_names[max_idx]
                return f"Vector emotion ({dominant_emotion}: {max_value:.2f})"
            else:
                return "Vector emotion (Neutral)"

        elif emotion_mode == "text_description":
            emotion_text = emotion_config["text"]

            if emotion_text.strip():
                model.infer(
                    spk_audio_prompt=speaker_audio_path,
                    text=text,
                    output_path=output_path,
                    use_emo_text=True,
                    emo_text=emotion_text,
                    verbose=False,
                    temperature=consistency_temp,
                    top_p=consistency_top_p,
                    top_k=50,
                    max_text_tokens_per_sentence=120,
                    interval_silence=200
                )
                return f"Text emotion ({emotion_text[:30]}...)"
            else:
                # ä»åˆæˆæ–‡æœ¬æ¨æ–­æƒ…æ„Ÿ
                model.infer(
                    spk_audio_prompt=speaker_audio_path,
                    text=text,
                    output_path=output_path,
                    use_emo_text=True,
                    verbose=False,
                    temperature=consistency_temp,
                    top_p=consistency_top_p,
                    top_k=50,
                    max_text_tokens_per_sentence=120,
                    interval_silence=200
                )
                return "Text emotion (inferred)"

        else:  # auto mode
            model.infer(
                spk_audio_prompt=speaker_audio_path,
                text=text,
                output_path=output_path,
                verbose=False,
                temperature=consistency_temp,
                top_p=consistency_top_p,
                top_k=50,
                max_text_tokens_per_sentence=120,
                interval_silence=200
            )
            return "Auto emotion"

    def _prepare_speaker_audios(self, speaker_audios: List[dict], verbose: bool,
                               voice_consistency: float = 1.0, reference_boost: bool = True) -> List[str]:
        """å‡†å¤‡è¯´è¯äººéŸ³é¢‘æ–‡ä»¶ï¼ˆå¸¦ä¸€è‡´æ€§å¢å¼ºï¼‰"""
        speaker_audio_paths = []

        for i, speaker_audio in enumerate(speaker_audios):
            if not isinstance(speaker_audio, dict) or "waveform" not in speaker_audio or "sample_rate" not in speaker_audio:
                raise ValueError(f"Speaker {i+1} audio must be a ComfyUI AUDIO object")

            waveform = speaker_audio["waveform"]
            sample_rate = speaker_audio["sample_rate"]

            # ç§»é™¤batchç»´åº¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if waveform.dim() == 3:
                waveform = waveform.squeeze(0)

            # åº”ç”¨å‚è€ƒéŸ³é¢‘å¢å¼º
            if reference_boost and voice_consistency > 1.0:
                waveform = self._enhance_reference_audio(waveform, voice_consistency)

            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix=f"_speaker{i+1}.wav", delete=False) as tmp_file:
                speaker_audio_path = tmp_file.name

            # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
            torchaudio.save(speaker_audio_path, waveform, sample_rate)
            speaker_audio_paths.append(speaker_audio_path)

            if verbose:
                print(f"[MultiTalk] Speaker{i+1} éŸ³é¢‘: é‡‡æ ·ç‡={sample_rate}, å½¢çŠ¶={waveform.shape}")
                if reference_boost and voice_consistency > 1.0:
                    print(f"[MultiTalk] Speaker{i+1} åº”ç”¨äº†å‚è€ƒéŸ³é¢‘å¢å¼º (ä¸€è‡´æ€§={voice_consistency})")

        return speaker_audio_paths

    def _enhance_reference_audio(self, waveform: torch.Tensor, voice_consistency: float) -> torch.Tensor:
        """å¢å¼ºå‚è€ƒéŸ³é¢‘ä»¥æé«˜å£°éŸ³ä¸€è‡´æ€§"""
        try:
            import torch.nn.functional as F

            # ç¡®ä¿éŸ³é¢‘é•¿åº¦è¶³å¤Ÿ
            if waveform.shape[-1] < 1000:
                # å¦‚æœéŸ³é¢‘å¤ªçŸ­ï¼Œè¿›è¡Œé‡å¤
                repeat_times = int(1000 / waveform.shape[-1]) + 1
                waveform = waveform.repeat(1, repeat_times)[:, :1000]

            # åº”ç”¨è½»å¾®çš„éŸ³é¢‘å¢å¼º
            if voice_consistency > 1.0:
                # å¢å¼ºéŸ³é¢‘çš„æ¸…æ™°åº¦
                enhancement_factor = min(voice_consistency, 2.0)

                # è½»å¾®çš„é«˜é¢‘å¢å¼ºï¼ˆæé«˜æ¸…æ™°åº¦ï¼‰
                if waveform.shape[-1] > 512:
                    # ç®€å•çš„é«˜é€šæ»¤æ³¢æ•ˆæœ
                    kernel = torch.tensor([[-0.1, -0.1, 0.8, -0.1, -0.1]], dtype=waveform.dtype)
                    kernel = kernel.unsqueeze(0)  # [1, 1, 5]

                    # å¯¹æ¯ä¸ªå£°é“åˆ†åˆ«å¤„ç†
                    enhanced_waveform = []
                    for ch in range(waveform.shape[0]):
                        ch_data = waveform[ch:ch+1].unsqueeze(0)  # [1, 1, length]
                        # åº”ç”¨å·ç§¯
                        enhanced = F.conv1d(ch_data, kernel, padding=2)
                        # æ··åˆåŸå§‹å’Œå¢å¼ºçš„ä¿¡å·
                        mix_ratio = (enhancement_factor - 1.0) * 0.3  # é™åˆ¶å¢å¼ºå¼ºåº¦
                        enhanced = ch_data * (1 - mix_ratio) + enhanced * mix_ratio
                        enhanced_waveform.append(enhanced.squeeze(0))

                    waveform = torch.cat(enhanced_waveform, dim=0)

                # è½»å¾®çš„éŸ³é‡æ ‡å‡†åŒ–
                max_val = torch.max(torch.abs(waveform))
                if max_val > 0:
                    target_level = 0.7  # ç›®æ ‡éŸ³é‡çº§åˆ«
                    waveform = waveform * (target_level / max_val)

            return waveform

        except Exception as e:
            print(f"[MultiTalk] éŸ³é¢‘å¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹éŸ³é¢‘: {e}")
            return waveform

    def _merge_audio_segments(self, audio_segments: List[dict], silence_duration: float, verbose: bool) -> dict:
        """åˆå¹¶éŸ³é¢‘ç‰‡æ®µ"""
        if not audio_segments:
            raise ValueError("No audio segments to merge")

        # è·å–ç¬¬ä¸€ä¸ªç‰‡æ®µçš„é‡‡æ ·ç‡
        sample_rate = audio_segments[0]["sample_rate"]

        # ç¡®ä¿æ‰€æœ‰ç‰‡æ®µçš„é‡‡æ ·ç‡ä¸€è‡´
        for i, segment in enumerate(audio_segments):
            if segment["sample_rate"] != sample_rate:
                if verbose:
                    print(f"[MultiTalk] é‡é‡‡æ ·ç‰‡æ®µ {i+1}: {segment['sample_rate']} -> {sample_rate}")
                # é‡é‡‡æ ·åˆ°ç»Ÿä¸€é‡‡æ ·ç‡
                resampler = torchaudio.transforms.Resample(segment["sample_rate"], sample_rate)
                segment["waveform"] = resampler(segment["waveform"])
                segment["sample_rate"] = sample_rate

        # è®¡ç®—é™éŸ³ç‰‡æ®µ
        silence_samples = int(silence_duration * sample_rate)
        silence_waveform = torch.zeros(audio_segments[0]["waveform"].shape[0], silence_samples)

        # åˆå¹¶æ‰€æœ‰ç‰‡æ®µ
        merged_waveforms = []
        for i, segment in enumerate(audio_segments):
            waveform = segment["waveform"]

            # ç¡®ä¿æ˜¯2Då¼ é‡ [channels, samples]
            if waveform.dim() == 3:
                waveform = waveform.squeeze(0)
            elif waveform.dim() == 1:
                waveform = waveform.unsqueeze(0)

            merged_waveforms.append(waveform)

            # åœ¨ç‰‡æ®µä¹‹é—´æ·»åŠ é™éŸ³ï¼ˆé™¤äº†æœ€åä¸€ä¸ªç‰‡æ®µï¼‰
            if i < len(audio_segments) - 1 and silence_duration > 0:
                merged_waveforms.append(silence_waveform)

        # è¿æ¥æ‰€æœ‰æ³¢å½¢
        final_waveform = torch.cat(merged_waveforms, dim=1)

        if verbose:
            total_duration = final_waveform.shape[1] / sample_rate
            print(f"[MultiTalk] åˆå¹¶å®Œæˆ: {len(audio_segments)} ä¸ªç‰‡æ®µ, æ€»æ—¶é•¿: {total_duration:.2f}ç§’")

        return {
            "waveform": final_waveform,
            "sample_rate": sample_rate
        }

    def _load_default_model(self, use_fp16: bool, use_cuda_kernel: bool):
        """åŠ è½½é»˜è®¤æ¨¡å‹"""
        try:
            # ç»Ÿä¸€ä½¿ç”¨æ ‡å‡†å¯¼å…¥è·¯å¾„
            from indextts.infer_v2 import IndexTTS2

            # ä½¿ç”¨é€šç”¨æ¨¡å‹è·¯å¾„å‡½æ•°
            from .model_utils import get_indextts2_model_path, validate_model_path

            model_dir, config_path = get_indextts2_model_path()

            print(f"[MultiTalk] ä½¿ç”¨æ¨¡å‹è·¯å¾„: {model_dir}")

            # éªŒè¯æ¨¡å‹è·¯å¾„
            validate_model_path(model_dir, config_path)

            model = IndexTTS2(
                cfg_path=config_path,
                model_dir=model_dir,
                is_fp16=use_fp16,
                use_cuda_kernel=use_cuda_kernel
            )

            return model

        except Exception as e:
            error_msg = f"Failed to load IndexTTS2 model: {str(e)}"
            # ç‰¹åˆ«å¤„ç†DeepSpeedç›¸å…³é”™è¯¯
            if "deepspeed" in str(e).lower():
                error_msg += "\n[MultiTalk] DeepSpeedç›¸å…³é”™è¯¯ï¼Œä½†åŸºæœ¬åŠŸèƒ½åº”è¯¥ä»ç„¶å¯ç”¨"
                error_msg += "\n[MultiTalk] DeepSpeed-related error, but basic functionality should still work"
            raise RuntimeError(error_msg)

    def _load_audio(self, audio_path: str) -> dict:
        """åŠ è½½éŸ³é¢‘æ–‡ä»¶"""
        from .audio_utils import load_audio_for_comfyui
        return load_audio_for_comfyui(audio_path)

    def _generate_info_with_emotion(self, conversation_lines: List[Dict], num_speakers: int,
                                   output_path: str, language: str, speed: float, silence_duration: float,
                                   emotion_configs: List[Dict]) -> str:
        """ç”ŸæˆåŒ…å«æƒ…æ„Ÿä¿¡æ¯çš„ä¿¡æ¯å­—ç¬¦ä¸²"""
        info_lines = [
            "=== IndexTTS2 Multi-Talk Synthesis with Emotion Control ===",
            f"Speakers: {num_speakers}",
            f"Conversation Lines: {len(conversation_lines)}",
            f"Language: {language}",
            f"Speed: {speed}x",
            f"Silence Duration: {silence_duration}s",
            f"Output: {os.path.basename(output_path)}",
            "",
            "=== Speaker Emotion Settings ===",
        ]

        # æ·»åŠ æ¯ä¸ªè¯´è¯äººçš„æƒ…æ„Ÿè®¾ç½®
        for i, emotion_config in enumerate(emotion_configs):
            if i < num_speakers:
                mode = emotion_config.get("mode", "none")
                info_lines.append(f"Speaker{i+1}: {mode}")

                if mode == "emotion_vector":
                    vector = emotion_config.get("vector", [])
                    emotion_names = ["Happy", "Angry", "Sad", "Fear", "Hate", "Low", "Surprise", "Neutral"]
                    active_emotions = [f"{name}: {val:.2f}" for name, val in zip(emotion_names, vector) if val > 0.1]
                    if active_emotions:
                        info_lines.append(f"  Emotions: {', '.join(active_emotions)}")
                elif mode == "audio_prompt":
                    audio_info = emotion_config.get("audio", None)
                    alpha = emotion_config.get("alpha", 1.0)
                    if audio_info and isinstance(audio_info, dict):
                        duration = audio_info.get("duration", 0)
                        info_lines.append(f"  Audio: {duration:.2f}s (Î±={alpha})")
                elif mode == "text_description":
                    text = emotion_config.get("text", "")
                    if text:
                        info_lines.append(f"  Description: {text[:50]}...")

        info_lines.extend([
            "",
            "=== Conversation Preview ===",
        ])

        # æ·»åŠ å¯¹è¯é¢„è§ˆï¼ˆæœ€å¤šæ˜¾ç¤ºå‰5è¡Œï¼‰
        for i, line in enumerate(conversation_lines[:5]):
            preview_text = line["text"][:60] + "..." if len(line["text"]) > 60 else line["text"]
            info_lines.append(f"{line['speaker_name']}: {preview_text}")

        if len(conversation_lines) > 5:
            info_lines.append(f"... and {len(conversation_lines) - 5} more lines")

        # æ·»åŠ Qwenæ¨¡å‹ä¿¡æ¯
        info_lines.extend([
            "",
            "=== Qwen Emotion Model Status ===",
        ])

        qwen_info = self._get_qwen_model_info()
        info_lines.extend(qwen_info)

        return "\n".join(info_lines)

    def _get_qwen_model_info(self) -> list:
        """è·å–å½“å‰Qwenæ¨¡å‹ä¿¡æ¯"""
        try:
            # æ£€æŸ¥transformersç‰ˆæœ¬
            import transformers
            from packaging import version

            current_version = transformers.__version__
            info_lines = [f"ğŸ”§ Transformersç‰ˆæœ¬: {current_version}"]

            # ç›´æ¥æ£€æŸ¥å…¼å®¹æ€§ï¼Œä¸åˆ›å»ºQwenEmotionå®ä¾‹
            compatible_models = self._get_compatible_qwen_models_direct()

            # æ˜¾ç¤ºå…¼å®¹æ¨¡å‹ä¿¡æ¯
            if compatible_models:
                best_model = compatible_models[0]  # ç¬¬ä¸€ä¸ªæ˜¯ä¼˜å…ˆçº§æœ€é«˜çš„
                info_lines.extend([
                    f"ğŸ¤– æ¨èæ¨¡å‹: {best_model['name']}",
                    f"ğŸ“Š æ¨¡å‹å¤§å°: {best_model['size']}",
                    f"ğŸ“ æ¨¡å‹ç±»å‹: æ™ºèƒ½é€‰æ‹©",
                    f"âœ… çŠ¶æ€: é«˜ç²¾åº¦æƒ…æ„Ÿåˆ†æå¯ç”¨"
                ])
            else:
                info_lines.extend([
                    f"ğŸ¤– æƒ…æ„Ÿæ¨¡å‹: å…³é”®è¯åŒ¹é…",
                    f"ğŸ“ æ¨¡å‹ç±»å‹: å¤‡ç”¨æ–¹æ¡ˆ",
                    f"âš ï¸  çŠ¶æ€: åŸºç¡€æƒ…æ„Ÿåˆ†æå¯ç”¨"
                ])

            # æ˜¾ç¤ºå…¼å®¹æ¨¡å‹æ•°é‡
            info_lines.append(f"ğŸ” å…¼å®¹Qwenæ¨¡å‹: {len(compatible_models)}ä¸ª")

            return info_lines

        except Exception as e:
            return [
                f"âŒ Qwenæ¨¡å‹ä¿¡æ¯è·å–å¤±è´¥: {str(e)[:50]}...",
                f"â„¹ï¸  åŸºæœ¬TTSåŠŸèƒ½ä¸å—å½±å“"
            ]

    def _get_compatible_qwen_models_direct(self):
        """ç›´æ¥è·å–å…¼å®¹çš„Qwenæ¨¡å‹åˆ—è¡¨ï¼Œä¸åˆ›å»ºQwenEmotionå®ä¾‹"""
        try:
            import transformers
            from packaging import version

            current_ver = version.parse(transformers.__version__)

            # å®šä¹‰ä¸åŒQwenæ¨¡å‹çš„ç‰ˆæœ¬è¦æ±‚å’Œä¼˜å…ˆçº§
            qwen_models = []

            # Qwen3ç³»åˆ— (éœ€è¦transformers >= 4.51.0)
            if current_ver >= version.parse("4.51.0"):
                qwen_models.extend([
                    {
                        "name": "Qwen3-0.5B-Instruct",
                        "model_id": "Qwen/Qwen3-0.5B-Instruct",
                        "priority": 1,
                        "size": "0.5B",
                        "description": "æœ€æ–°Qwen3æ¨¡å‹ï¼Œå°å‹é«˜æ•ˆ"
                    },
                    {
                        "name": "Qwen3-1.8B-Instruct",
                        "model_id": "Qwen/Qwen3-1.8B-Instruct",
                        "priority": 2,
                        "size": "1.8B",
                        "description": "Qwen3ä¸­å‹æ¨¡å‹"
                    }
                ])

            # Qwen2.5ç³»åˆ— (éœ€è¦transformers >= 4.37.0)
            if current_ver >= version.parse("4.37.0"):
                qwen_models.extend([
                    {
                        "name": "Qwen2.5-0.5B-Instruct",
                        "model_id": "Qwen/Qwen2.5-0.5B-Instruct",
                        "priority": 3,
                        "size": "0.5B",
                        "description": "Qwen2.5å°å‹æ¨¡å‹"
                    },
                    {
                        "name": "Qwen2.5-1.5B-Instruct",
                        "model_id": "Qwen/Qwen2.5-1.5B-Instruct",
                        "priority": 4,
                        "size": "1.5B",
                        "description": "Qwen2.5ä¸­å‹æ¨¡å‹"
                    }
                ])

            # Qwen2ç³»åˆ— (éœ€è¦transformers >= 4.37.0)
            if current_ver >= version.parse("4.37.0"):
                qwen_models.extend([
                    {
                        "name": "Qwen2-0.5B-Instruct",
                        "model_id": "Qwen/Qwen2-0.5B-Instruct",
                        "priority": 5,
                        "size": "0.5B",
                        "description": "Qwen2å°å‹æ¨¡å‹"
                    },
                    {
                        "name": "Qwen2-1.5B-Instruct",
                        "model_id": "Qwen/Qwen2-1.5B-Instruct",
                        "priority": 6,
                        "size": "1.5B",
                        "description": "Qwen2ä¸­å‹æ¨¡å‹"
                    }
                ])

            # Qwen1.5ç³»åˆ— (éœ€è¦transformers >= 4.37.0)
            if current_ver >= version.parse("4.37.0"):
                qwen_models.extend([
                    {
                        "name": "Qwen1.5-0.5B-Chat",
                        "model_id": "Qwen/Qwen1.5-0.5B-Chat",
                        "priority": 7,
                        "size": "0.5B",
                        "description": "Qwen1.5å°å‹æ¨¡å‹"
                    },
                    {
                        "name": "Qwen1.5-1.8B-Chat",
                        "model_id": "Qwen/Qwen1.5-1.8B-Chat",
                        "priority": 8,
                        "size": "1.8B",
                        "description": "Qwen1.5ä¸­å‹æ¨¡å‹"
                    }
                ])

            # æŒ‰ä¼˜å…ˆçº§æ’åº
            qwen_models.sort(key=lambda x: x["priority"])

            return qwen_models

        except Exception as e:
            print(f"[IndexTTS2] âš ï¸  è·å–å…¼å®¹æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def _save_emotion_audio_to_temp(self, emotion_audio: dict) -> Optional[str]:
        """å°†ComfyUI AUDIOå¯¹è±¡ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶ä¾›IndexTTS2ä½¿ç”¨"""
        try:
            import tempfile
            import torchaudio

            if not isinstance(emotion_audio, dict) or "waveform" not in emotion_audio or "sample_rate" not in emotion_audio:
                print("[MultiTalk] Invalid emotion audio object")
                return None

            waveform = emotion_audio["waveform"]
            sample_rate = emotion_audio["sample_rate"]

            # ç§»é™¤batchç»´åº¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if waveform.dim() == 3:
                waveform = waveform.squeeze(0)

            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                emotion_audio_path = tmp_file.name

            # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
            torchaudio.save(emotion_audio_path, waveform, sample_rate)

            return emotion_audio_path

        except Exception as e:
            print(f"[MultiTalk] Failed to save emotion audio: {str(e)}")
            return None

    @classmethod
    def IS_CHANGED(cls, **kwargs):
        """æ£€æŸ¥è¾“å…¥æ˜¯å¦æ”¹å˜"""
        return float("nan")  # æ€»æ˜¯é‡æ–°æ‰§è¡Œ
